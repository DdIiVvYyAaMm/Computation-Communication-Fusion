==PROF== Connected to process 6638 (/usr/bin/python3.12)
==LOG== Target process 6687 terminated before first instrumented API call.
==LOG== Target process 6709 terminated before first instrumented API call.
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
running l2 cache tests for ../original_gemm.cubin
512
766
1020
1274
1528
1782
2036
==PROF== Disconnected from process 6638
[6638] python3.12@127.0.0.1
  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         2.17
    Elapsed Cycles                cycle       780013
    Memory Throughput                 %        89.90
    DRAM Throughput                   %         2.62
    Duration                         us       359.07
    L1/TEX Cache Throughput           %        91.01
    L2 Cache Throughput               %        11.58
    SM Active Cycles              cycle    770355.54
    Compute (SM) Throughput           %        89.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.65
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.01
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.30
    Issued Ipc Active     inst/cycle         2.01
    SM Busy                        %        57.50
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         5.85
    Mem Busy                              %        67.49
    Max Bandwidth                         %        89.90
    L1/TEX Hit Rate                       %        89.17
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.46
    Mem Pipes Busy                        %        89.90
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.33%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.39
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.61
    Active Warps Per Scheduler          warp        11.36
    Eligible Warps Per Scheduler        warp         2.21
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.1%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.36 active warps per scheduler, but only an average of 2.21 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.54
    Warp Cycles Per Executed Instruction           cycle        22.55
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.25
    Issued Instructions                             inst     37200408
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.59
    Achieved Active Warps Per SM           warp        45.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle     10044416
    Average L1 Active Cycles         cycle    770355.54
    Total L1 Elapsed Cycles          cycle     18716496
    Average L2 Active Cycles         cycle    656214.19
    Total L2 Elapsed Cycles          cycle     11273696
    Average SM Active Cycles         cycle    770355.54
    Total SM Elapsed Cycles          cycle     18716496
    Average SMSP Active Cycles       cycle    769061.73
    Total SMSP Elapsed Cycles        cycle     74865984
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       779377
    Memory Throughput                 %        89.98
    DRAM Throughput                   %         2.76
    Duration                         us       340.26
    L1/TEX Cache Throughput           %        91.30
    L2 Cache Throughput               %        11.56
    SM Active Cycles              cycle    767903.46
    Compute (SM) Throughput           %        89.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.71
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.46
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.68
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.18
    Mem Busy                              %        67.54
    Max Bandwidth                         %        89.98
    L1/TEX Hit Rate                       %        89.28
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.40
    Mem Pipes Busy                        %        89.98
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.36%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.28
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.72
    Active Warps Per Scheduler          warp        11.38
    Eligible Warps Per Scheduler        warp         2.18
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.02%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.38 active warps per scheduler, but only an average of 2.18 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.63
    Warp Cycles Per Executed Instruction           cycle        22.63
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.02%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 31.1% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.17
    Issued Instructions                             inst     37200400
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.05
    Achieved Active Warps Per SM           warp        45.62
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      9517056
    Average L1 Active Cycles         cycle    767903.46
    Total L1 Elapsed Cycles          cycle     18701088
    Average L2 Active Cycles         cycle       656726
    Total L2 Elapsed Cycles          cycle     11160448
    Average SM Active Cycles         cycle    767903.46
    Total SM Elapsed Cycles          cycle     18701088
    Average SMSP Active Cycles       cycle    770665.60
    Total SMSP Elapsed Cycles        cycle     74804352
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       778083
    Memory Throughput                 %        90.11
    DRAM Throughput                   %         2.42
    Duration                         us       339.33
    L1/TEX Cache Throughput           %        91.32
    L2 Cache Throughput               %        11.62
    SM Active Cycles              cycle    767696.58
    Compute (SM) Throughput           %        90.11
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.65
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.48
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.70
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.19
    Mem Busy                              %        67.65
    Max Bandwidth                         %        90.11
    L1/TEX Hit Rate                       %        89.22
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.92
    Mem Pipes Busy                        %        90.11
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.42%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.49
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.51
    Active Warps Per Scheduler          warp        11.40
    Eligible Warps Per Scheduler        warp         2.21
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.887%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.40 active warps per scheduler, but only an average of 2.21 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.58
    Warp Cycles Per Executed Instruction           cycle        22.59
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.26
    Issued Instructions                             inst     37200409
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.00
    Achieved Active Warps Per SM           warp        45.60
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle     10846208
    Average L1 Active Cycles         cycle    767696.58
    Total L1 Elapsed Cycles          cycle     18672448
    Average L2 Active Cycles         cycle    655247.88
    Total L2 Elapsed Cycles          cycle     11143616
    Average SM Active Cycles         cycle    767696.58
    Total SM Elapsed Cycles          cycle     18672448
    Average SMSP Active Cycles       cycle    767504.52
    Total SMSP Elapsed Cycles        cycle     74689792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       778743
    Memory Throughput                 %        90.04
    DRAM Throughput                   %         2.42
    Duration                         us       339.58
    L1/TEX Cache Throughput           %        91.19
    L2 Cache Throughput               %        11.67
    SM Active Cycles              cycle    768805.79
    Compute (SM) Throughput           %        90.04
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.40
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.62
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.19
    Mem Busy                              %        67.59
    Max Bandwidth                         %        90.04
    L1/TEX Hit Rate                       %        89.30
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.68
    Mem Pipes Busy                        %        90.04
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.39%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.35
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.65
    Active Warps Per Scheduler          warp        11.36
    Eligible Warps Per Scheduler        warp         2.22
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.961%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.36 active warps per scheduler, but only an average of 2.22 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.56
    Warp Cycles Per Executed Instruction           cycle        22.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.27
    Issued Instructions                             inst     37200410
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.81
    Achieved Active Warps Per SM           warp        45.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle     10855424
    Average L1 Active Cycles         cycle    768805.79
    Total L1 Elapsed Cycles          cycle     18687776
    Average L2 Active Cycles         cycle    657429.62
    Total L2 Elapsed Cycles          cycle     11152800
    Average SM Active Cycles         cycle    768805.79
    Total SM Elapsed Cycles          cycle     18687776
    Average SMSP Active Cycles       cycle    769556.67
    Total SMSP Elapsed Cycles        cycle     74751104
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         7.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       777686
    Memory Throughput                 %        90.16
    DRAM Throughput                   %         2.42
    Duration                         us       339.10
    L1/TEX Cache Throughput           %        91.27
    L2 Cache Throughput               %        11.67
    SM Active Cycles              cycle    768200.50
    Compute (SM) Throughput           %        90.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.85
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.44
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.67
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.20
    Mem Busy                              %        67.69
    Max Bandwidth                         %        90.16
    L1/TEX Hit Rate                       %        89.26
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.06
    Mem Pipes Busy                        %        90.16
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.45%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.43
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.57
    Active Warps Per Scheduler          warp        11.38
    Eligible Warps Per Scheduler        warp         2.20
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.836%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.38 active warps per scheduler, but only an average of 2.20 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.56
    Warp Cycles Per Executed Instruction           cycle        22.56
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.30
    Issued Instructions                             inst     37200413
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.86
    Achieved Active Warps Per SM           warp        45.53
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle     10840064
    Average L1 Active Cycles         cycle    768200.50
    Total L1 Elapsed Cycles          cycle     18661912
    Average L2 Active Cycles         cycle       658066
    Total L2 Elapsed Cycles          cycle     11137264
    Average SM Active Cycles         cycle    768200.50
    Total SM Elapsed Cycles          cycle     18661912
    Average SMSP Active Cycles       cycle    768395.34
    Total SMSP Elapsed Cycles        cycle     74647648
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       783996
    Memory Throughput                 %        89.44
    DRAM Throughput                   %         3.21
    Duration                         us       341.82
    L1/TEX Cache Throughput           %        90.32
    L2 Cache Throughput               %        11.36
    SM Active Cycles              cycle    776202.38
    Compute (SM) Throughput           %        89.44
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.71
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.00
    Executed Ipc Elapsed  inst/cycle         1.98
    Issue Slots Busy               %        49.92
    Issued Ipc Active     inst/cycle         2.00
    SM Busy                        %        57.05
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (33.9%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.15
    Mem Busy                              %        67.14
    Max Bandwidth                         %        89.44
    L1/TEX Hit Rate                       %        89.33
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.59
    Mem Pipes Busy                        %        89.44
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.13%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.81
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        50.19
    Active Warps Per Scheduler          warp        11.37
    Eligible Warps Per Scheduler        warp         2.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.37 active warps per scheduler, but only an average of 2.09 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.83
    Warp Cycles Per Executed Instruction           cycle        22.84
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.56%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.2% of the total average of 22.8 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.31
    Issued Instructions                             inst     37200414
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.94
    Achieved Active Warps Per SM           warp        45.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8194048
    Average L1 Active Cycles         cycle    776202.38
    Total L1 Elapsed Cycles          cycle     18813512
    Average L2 Active Cycles         cycle    663667.31
    Total L2 Elapsed Cycles          cycle     11226640
    Average SM Active Cycles         cycle    776202.38
    Total SM Elapsed Cycles          cycle     18813512
    Average SMSP Active Cycles       cycle    777979.30
    Total SMSP Elapsed Cycles        cycle     75254048
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       784843
    Memory Throughput                 %        89.35
    DRAM Throughput                   %         3.20
    Duration                         us       342.43
    L1/TEX Cache Throughput           %        90.34
    L2 Cache Throughput               %        11.38
    SM Active Cycles              cycle    776038.08
    Compute (SM) Throughput           %        89.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.58
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.00
    Executed Ipc Elapsed  inst/cycle         1.98
    Issue Slots Busy               %        49.93
    Issued Ipc Active     inst/cycle         2.00
    SM Busy                        %        57.06
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (33.9%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.14
    Mem Busy                              %        67.08
    Max Bandwidth                         %        89.35
    L1/TEX Hit Rate                       %        89.31
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.70
    Mem Pipes Busy                        %        89.35
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.09%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.02
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.98
    Active Warps Per Scheduler          warp        11.41
    Eligible Warps Per Scheduler        warp         2.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.65%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.41 active warps per scheduler, but only an average of 2.10 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.81
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.65%                                                                                          
          On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 34.0% of the total average of 22.8 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.45
    Issued Instructions                             inst     37200427
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.01
    Achieved Active Warps Per SM           warp        45.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8210432
    Average L1 Active Cycles         cycle    776038.08
    Total L1 Elapsed Cycles          cycle     18831040
    Average L2 Active Cycles         cycle    662135.62
    Total L2 Elapsed Cycles          cycle     11220368
    Average SM Active Cycles         cycle    776038.08
    Total SM Elapsed Cycles          cycle     18831040
    Average SMSP Active Cycles       cycle    774730.85
    Total SMSP Elapsed Cycles        cycle     75324160
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       786847
    Memory Throughput                 %        89.12
    DRAM Throughput                   %         3.19
    Duration                         us       343.33
    L1/TEX Cache Throughput           %        90.23
    L2 Cache Throughput               %        11.41
    SM Active Cycles              cycle    776981.25
    Compute (SM) Throughput           %        89.12
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.10
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.99
    Executed Ipc Elapsed  inst/cycle         1.97
    Issue Slots Busy               %        49.87
    Issued Ipc Active     inst/cycle         1.99
    SM Busy                        %        56.99
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (33.9%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.12
    Mem Busy                              %        66.90
    Max Bandwidth                         %        89.12
    L1/TEX Hit Rate                       %        89.39
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.34
    Mem Pipes Busy                        %        89.12
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 38.99%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.93
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        50.07
    Active Warps Per Scheduler          warp        11.39
    Eligible Warps Per Scheduler        warp         2.11
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.88%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.39 active warps per scheduler, but only an average of 2.11 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.82
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.88%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 32.6% of the total average of 22.8 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.19
    Issued Instructions                             inst     37200402
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.68
    Achieved Active Warps Per SM           warp        45.45
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8230912
    Average L1 Active Cycles         cycle    776981.25
    Total L1 Elapsed Cycles          cycle     18880152
    Average L2 Active Cycles         cycle    664533.56
    Total L2 Elapsed Cycles          cycle     11250800
    Average SM Active Cycles         cycle    776981.25
    Total SM Elapsed Cycles          cycle     18880152
    Average SMSP Active Cycles       cycle    776169.17
    Total SMSP Elapsed Cycles        cycle     75520608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.00
    SM Frequency                    Ghz         2.20
    Elapsed Cycles                cycle       781394
    Memory Throughput                 %        89.73
    DRAM Throughput                   %         3.09
    Duration                         us       354.59
    L1/TEX Cache Throughput           %        90.75
    L2 Cache Throughput               %        11.34
    SM Active Cycles              cycle    772551.38
    Compute (SM) Throughput           %        89.73
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.58
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.01
    Executed Ipc Elapsed  inst/cycle         1.98
    Issue Slots Busy               %        50.16
    Issued Ipc Active     inst/cycle         2.01
    SM Busy                        %        57.33
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         5.93
    Mem Busy                              %        67.36
    Max Bandwidth                         %        89.73
    L1/TEX Hit Rate                       %        89.39
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.78
    Mem Pipes Busy                        %        89.73
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.26%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.97
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        50.03
    Active Warps Per Scheduler          warp        11.40
    Eligible Warps Per Scheduler        warp         2.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.27%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.40 active warps per scheduler, but only an average of 2.12 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.82
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.27%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.5% of the total average of 22.8 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387504.09
    Issued Instructions                             inst     37200393
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.34
    Achieved Active Warps Per SM           warp        45.76
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8503296
    Average L1 Active Cycles         cycle    772551.38
    Total L1 Elapsed Cycles          cycle     18751448
    Average L2 Active Cycles         cycle    665942.94
    Total L2 Elapsed Cycles          cycle     11307264
    Average SM Active Cycles         cycle    772551.38
    Total SM Elapsed Cycles          cycle     18751448
    Average SMSP Active Cycles       cycle    775430.60
    Total SMSP Elapsed Cycles        cycle     75005792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       784655
    Memory Throughput                 %        89.36
    DRAM Throughput                   %         3.20
    Duration                         us       342.14
    L1/TEX Cache Throughput           %        90.08
    L2 Cache Throughput               %        11.38
    SM Active Cycles              cycle    778316.83
    Compute (SM) Throughput           %        89.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.58
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.99
    Executed Ipc Elapsed  inst/cycle         1.98
    Issue Slots Busy               %        49.79
    Issued Ipc Active     inst/cycle         1.99
    SM Busy                        %        56.89
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (33.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.14
    Mem Busy                              %        67.08
    Max Bandwidth                         %        89.36
    L1/TEX Hit Rate                       %        89.32
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.75
    Mem Pipes Busy                        %        89.36
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.1%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.96
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        50.04
    Active Warps Per Scheduler          warp        11.40
    Eligible Warps Per Scheduler        warp         2.10
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.64%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.40 active warps per scheduler, but only an average of 2.10 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.82
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.96
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.64%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 32.4% of the total average of 22.8 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387413.33
    Executed Instructions                           inst     37191680
    Avg. Issued Instructions Per Scheduler          inst    387503.97
    Issued Instructions                             inst     37200381
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.68
    Achieved Active Warps Per SM           warp        45.45
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8202240
    Average L1 Active Cycles         cycle    778316.83
    Total L1 Elapsed Cycles          cycle     18829712
    Average L2 Active Cycles         cycle    663581.75
    Total L2 Elapsed Cycles          cycle     11235872
    Average SM Active Cycles         cycle    778316.83
    Total SM Elapsed Cycles          cycle     18829712
    Average SMSP Active Cycles       cycle    775611.42
    Total SMSP Elapsed Cycles        cycle     75318848
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1204224
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.20
    Elapsed Cycles                cycle      2540208
    Memory Throughput                 %        92.58
    DRAM Throughput                   %         2.13
    Duration                         ms         1.15
    L1/TEX Cache Throughput           %        92.78
    L2 Cache Throughput               %        15.14
    SM Active Cycles              cycle   2534633.75
    Compute (SM) Throughput           %        92.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.04
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.10
    Issued Ipc Active     inst/cycle         2.04
    SM Busy                        %        58.61
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.08
    Mem Busy                              %        53.88
    Max Bandwidth                         %        92.58
    L1/TEX Hit Rate                       %        88.23
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.75
    Mem Pipes Busy                        %        92.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.77%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.28%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.00
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        49.00
    Active Warps Per Scheduler          warp        11.70
    Eligible Warps Per Scheduler        warp         2.31
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.421%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.70 active warps per scheduler, but only an average of 2.31 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.95
    Warp Cycles Per Executed Instruction           cycle        22.95
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.421%                                                                                          
          On average, each warp of this kernel spends 7.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.7% of the total average of 22.9 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295221.97
    Issued Instructions                             inst    124341309
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.66
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     27631616
    Average L1 Active Cycles         cycle   2534633.75
    Total L1 Elapsed Cycles          cycle     60962856
    Average L2 Active Cycles         cycle   2236816.19
    Total L2 Elapsed Cycles          cycle     36768112
    Average SM Active Cycles         cycle   2534633.75
    Total SM Elapsed Cycles          cycle     60962856
    Average SMSP Active Cycles       cycle   2539484.90
    Total SMSP Elapsed Cycles        cycle    243851424
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0398%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      2543767
    Memory Throughput                 %        92.45
    DRAM Throughput                   %         2.21
    Duration                         ms         1.11
    L1/TEX Cache Throughput           %        92.61
    L2 Cache Throughput               %        15.20
    SM Active Cycles              cycle   2539347.96
    Compute (SM) Throughput           %        92.45
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.01
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.04
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.01
    Issued Ipc Active     inst/cycle         2.04
    SM Busy                        %        58.50
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.24
    Mem Busy                              %        53.79
    Max Bandwidth                         %        92.45
    L1/TEX Hit Rate                       %        88.22
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.77
    Mem Pipes Busy                        %        92.45
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.71%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.25%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.03
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.97
    Active Warps Per Scheduler          warp        11.71
    Eligible Warps Per Scheduler        warp         2.31
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.548%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.71 active warps per scheduler, but only an average of 2.31 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.94
    Warp Cycles Per Executed Instruction           cycle        22.94
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.548%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.6% of the total average of 22.9 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.17
    Issued Instructions                             inst    124341328
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.55
    Achieved Active Warps Per SM           warp        46.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     26585088
    Average L1 Active Cycles         cycle   2539347.96
    Total L1 Elapsed Cycles          cycle     61046920
    Average L2 Active Cycles         cycle      2229043
    Total L2 Elapsed Cycles          cycle     36436336
    Average SM Active Cycles         cycle   2539347.96
    Total SM Elapsed Cycles          cycle     61046920
    Average SMSP Active Cycles       cycle   2537949.99
    Total SMSP Elapsed Cycles        cycle    244187680
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.04003%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      2542725
    Memory Throughput                 %        92.49
    DRAM Throughput                   %         2.21
    Duration                         ms         1.11
    L1/TEX Cache Throughput           %        92.63
    L2 Cache Throughput               %        15.09
    SM Active Cycles              cycle   2538857.96
    Compute (SM) Throughput           %        92.49
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.14
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.04
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.02
    Issued Ipc Active     inst/cycle         2.04
    SM Busy                        %        58.51
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.24
    Mem Busy                              %        53.83
    Max Bandwidth                         %        92.49
    L1/TEX Hit Rate                       %        88.22
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.98
    Mem Pipes Busy                        %        92.49
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.72%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.26%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.04
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.96
    Active Warps Per Scheduler          warp        11.71
    Eligible Warps Per Scheduler        warp         2.30
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.514%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.71 active warps per scheduler, but only an average of 2.30 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.95
    Warp Cycles Per Executed Instruction           cycle        22.95
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.514%                                                                                          
          On average, each warp of this kernel spends 7.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.8% of the total average of 22.9 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.25
    Issued Instructions                             inst    124341336
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.57
    Achieved Active Warps Per SM           warp        46.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     26570752
    Average L1 Active Cycles         cycle   2538857.96
    Total L1 Elapsed Cycles          cycle     61024360
    Average L2 Active Cycles         cycle   2227205.06
    Total L2 Elapsed Cycles          cycle     36426912
    Average SM Active Cycles         cycle   2538857.96
    Total SM Elapsed Cycles          cycle     61024360
    Average SMSP Active Cycles       cycle   2537908.24
    Total SMSP Elapsed Cycles        cycle    244097440
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.04%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.00
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      2544523
    Memory Throughput                 %        92.42
    DRAM Throughput                   %         2.21
    Duration                         ms         1.11
    L1/TEX Cache Throughput           %        92.63
    L2 Cache Throughput               %        15.17
    SM Active Cycles              cycle   2538707.58
    Compute (SM) Throughput           %        92.42
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.01
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.04
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.02
    Issued Ipc Active     inst/cycle         2.04
    SM Busy                        %        58.52
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.24
    Mem Busy                              %        53.82
    Max Bandwidth                         %        92.42
    L1/TEX Hit Rate                       %        88.22
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.29
    Mem Pipes Busy                        %        92.42
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.69%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.24%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.04
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.96
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.31
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.577%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.31 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.95
    Warp Cycles Per Executed Instruction           cycle        22.95
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.577%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.6% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.08
    Issued Instructions                             inst    124341320
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.57
    Achieved Active Warps Per SM           warp        46.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     26593280
    Average L1 Active Cycles         cycle   2538707.58
    Total L1 Elapsed Cycles          cycle     61065856
    Average L2 Active Cycles         cycle   2223542.81
    Total L2 Elapsed Cycles          cycle     36448448
    Average SM Active Cycles         cycle   2538707.58
    Total SM Elapsed Cycles          cycle     61065856
    Average SMSP Active Cycles       cycle   2537434.66
    Total SMSP Elapsed Cycles        cycle    244263424
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03991%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      2544754
    Memory Throughput                 %        92.42
    DRAM Throughput                   %         2.21
    Duration                         ms         1.11
    L1/TEX Cache Throughput           %        92.64
    L2 Cache Throughput               %        15.13
    SM Active Cycles              cycle   2538566.08
    Compute (SM) Throughput           %        92.42
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.01
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.04
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.02
    Issued Ipc Active     inst/cycle         2.04
    SM Busy                        %        58.52
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.24
    Mem Busy                              %        53.90
    Max Bandwidth                         %        92.42
    L1/TEX Hit Rate                       %        88.29
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.78
    Mem Pipes Busy                        %        92.42
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.69%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.24%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.03
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.97
    Active Warps Per Scheduler          warp        11.71
    Eligible Warps Per Scheduler        warp         2.31
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.584%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.71 active warps per scheduler, but only an average of 2.31 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.95
    Warp Cycles Per Executed Instruction           cycle        22.96
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.584%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.6% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.09
    Issued Instructions                             inst    124341321
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.58
    Achieved Active Warps Per SM           warp        46.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146828
    Total DRAM Elapsed Cycles        cycle     26593280
    Average L1 Active Cycles         cycle   2538566.08
    Total L1 Elapsed Cycles          cycle     61070792
    Average L2 Active Cycles         cycle   2218134.56
    Total L2 Elapsed Cycles          cycle     36451552
    Average SM Active Cycles         cycle   2538566.08
    Total SM Elapsed Cycles          cycle     61070792
    Average SMSP Active Cycles       cycle   2537990.42
    Total SMSP Elapsed Cycles        cycle    244283168
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03981%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.17
    Elapsed Cycles                cycle      2542841
    Memory Throughput                 %        92.49
    DRAM Throughput                   %         2.10
    Duration                         ms         1.17
    L1/TEX Cache Throughput           %        92.81
    L2 Cache Throughput               %        15.11
    SM Active Cycles              cycle   2533744.12
    Compute (SM) Throughput           %        92.49
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.04
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.12
    Issued Ipc Active     inst/cycle         2.04
    SM Busy                        %        58.64
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.03
    Mem Busy                              %        54.14
    Max Bandwidth                         %        92.49
    L1/TEX Hit Rate                       %        87.82
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.09
    Mem Pipes Busy                        %        92.49
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.73%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.26%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.42
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.513%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.42 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.82
    Warp Cycles Per Executed Instruction           cycle        22.82
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.513%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.4% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.05
    Issued Instructions                             inst    124341317
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.23
    Achieved Active Warps Per SM           warp        46.67
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147148
    Total DRAM Elapsed Cycles        cycle     28043264
    Average L1 Active Cycles         cycle   2533744.12
    Total L1 Elapsed Cycles          cycle     61023648
    Average L2 Active Cycles         cycle   2368599.75
    Total L2 Elapsed Cycles          cycle     36979616
    Average SM Active Cycles         cycle   2533744.12
    Total SM Elapsed Cycles          cycle     61023648
    Average SMSP Active Cycles       cycle   2521088.61
    Total SMSP Elapsed Cycles        cycle    244094592
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.04191%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.56
    Elapsed Cycles                cycle      2530800
    Memory Throughput                 %        92.92
    DRAM Throughput                   %         1.51
    Duration                         ms         1.62
    L1/TEX Cache Throughput           %        93.23
    L2 Cache Throughput               %        14.27
    SM Active Cycles              cycle   2522317.29
    Compute (SM) Throughput           %        92.92
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.91
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.90
    Mem Busy                              %        54.49
    Max Bandwidth                         %        92.92
    L1/TEX Hit Rate                       %        87.72
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.53
    Mem Pipes Busy                        %        92.92
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.95%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.37%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.39
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.61
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.48
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.078%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.48 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.81
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.078%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.8% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.14
    Issued Instructions                             inst    124341325
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.63
    Achieved Active Warps Per SM           warp        46.86
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146828
    Total DRAM Elapsed Cycles        cycle     38903808
    Average L1 Active Cycles         cycle   2522317.29
    Total L1 Elapsed Cycles          cycle     60738280
    Average L2 Active Cycles         cycle   2468458.50
    Total L2 Elapsed Cycles          cycle     40878864
    Average SM Active Cycles         cycle   2522317.29
    Total SM Elapsed Cycles          cycle     60738280
    Average SMSP Active Cycles       cycle   2520527.47
    Total SMSP Elapsed Cycles        cycle    242953120
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03951%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.00
    SM Frequency                    Ghz         1.77
    Elapsed Cycles                cycle      2529327
    Memory Throughput                 %        92.98
    DRAM Throughput                   %         1.71
    Duration                         ms         1.43
    L1/TEX Cache Throughput           %        93.25
    L2 Cache Throughput               %        14.52
    SM Active Cycles              cycle   2521756.17
    Compute (SM) Throughput           %        92.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.36
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.92
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         3.29
    Mem Busy                              %        54.40
    Max Bandwidth                         %        92.98
    L1/TEX Hit Rate                       %        87.91
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.23
    Mem Pipes Busy                        %        92.98
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.98%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.39%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.33
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.67
    Active Warps Per Scheduler          warp        11.71
    Eligible Warps Per Scheduler        warp         2.42
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.024%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.71 active warps per scheduler, but only an average of 2.42 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.82
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.024%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.8% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.23
    Issued Instructions                             inst    124341334
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.67
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     34271232
    Average L1 Active Cycles         cycle   2521756.17
    Total L1 Elapsed Cycles          cycle     60702664
    Average L2 Active Cycles         cycle   2383814.06
    Total L2 Elapsed Cycles          cycle     39435952
    Average SM Active Cycles         cycle   2521756.17
    Total SM Elapsed Cycles          cycle     60702664
    Average SMSP Active Cycles       cycle   2523148.80
    Total SMSP Elapsed Cycles        cycle    242810656
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03955%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.87
    Elapsed Cycles                cycle      2529658
    Memory Throughput                 %        92.97
    DRAM Throughput                   %         1.81
    Duration                         ms         1.35
    L1/TEX Cache Throughput           %        93.19
    L2 Cache Throughput               %        14.74
    SM Active Cycles              cycle   2523459.54
    Compute (SM) Throughput           %        92.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.33
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.88
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         3.48
    Mem Busy                              %        54.37
    Max Bandwidth                         %        92.97
    L1/TEX Hit Rate                       %        87.98
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.36
    Mem Pipes Busy                        %        92.97
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.98%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.39%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.73
    Eligible Warps Per Scheduler        warp         2.39
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.028%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.73 active warps per scheduler, but only an average of 2.39 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.83
    Warp Cycles Per Executed Instruction           cycle        22.83
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.028%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.3% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.17
    Issued Instructions                             inst    124341328
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.67
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146828
    Total DRAM Elapsed Cycles        cycle     32364544
    Average L1 Active Cycles         cycle   2523459.54
    Total L1 Elapsed Cycles          cycle     60705408
    Average L2 Active Cycles         cycle   2328460.25
    Total L2 Elapsed Cycles          cycle     38469520
    Average SM Active Cycles         cycle   2523459.54
    Total SM Elapsed Cycles          cycle     60705408
    Average SMSP Active Cycles       cycle   2521432.73
    Total SMSP Elapsed Cycles        cycle    242821632
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0396%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.00
    SM Frequency                    Ghz         1.87
    Elapsed Cycles                cycle      2533660
    Memory Throughput                 %        92.83
    DRAM Throughput                   %         1.82
    Duration                         ms         1.35
    L1/TEX Cache Throughput           %        93.15
    L2 Cache Throughput               %        14.76
    SM Active Cycles              cycle   2524610.92
    Compute (SM) Throughput           %        92.83
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.30
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.85
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         3.49
    Mem Busy                              %        54.25
    Max Bandwidth                         %        92.83
    L1/TEX Hit Rate                       %        87.98
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.27
    Mem Pipes Busy                        %        92.83
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 48.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.35%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.33
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.67
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.39
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.17%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.39 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.84
    Warp Cycles Per Executed Instruction           cycle        22.84
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.89
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.17%                                                                                           
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.4% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1295131.50
    Executed Instructions                           inst    124332624
    Avg. Issued Instructions Per Scheduler          inst   1295222.25
    Issued Instructions                             inst    124341336
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.62
    Achieved Active Warps Per SM           warp        46.86
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147344
    Total DRAM Elapsed Cycles        cycle     32415744
    Average L1 Active Cycles         cycle   2524610.92
    Total L1 Elapsed Cycles          cycle     60798568
    Average L2 Active Cycles         cycle   2327700.94
    Total L2 Elapsed Cycles          cycle     38539392
    Average SM Active Cycles         cycle   2524610.92
    Total SM Elapsed Cycles          cycle     60798568
    Average SMSP Active Cycles       cycle   2523294.88
    Total SMSP Elapsed Cycles        cycle    243194272
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3861264
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03952%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      5996579
    Memory Throughput                 %        92.67
    DRAM Throughput                   %         1.66
    Duration                         ms         2.61
    L1/TEX Cache Throughput           %        92.70
    L2 Cache Throughput               %        14.98
    SM Active Cycles              cycle   5994322.96
    Compute (SM) Throughput           %        92.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.11
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.04
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        50.97
    Issued Ipc Active     inst/cycle         2.04
    SM Busy                        %        58.48
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         3.19
    Mem Busy                              %        64.70
    Max Bandwidth                         %        92.67
    L1/TEX Hit Rate                       %        88.19
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.27
    Mem Pipes Busy                        %        92.67
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.42%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.99
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        49.01
    Active Warps Per Scheduler          warp        11.74
    Eligible Warps Per Scheduler        warp         2.19
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.329%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.74 active warps per scheduler, but only an average of 2.19 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.03
    Warp Cycles Per Executed Instruction           cycle        23.03
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.329%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.5% of the total average of 23.0 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst      3055036
    Issued Instructions                             inst    293283456
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.83
    Achieved Active Warps Per SM           warp        46.96
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260384
    Total DRAM Elapsed Cycles        cycle     62660608
    Average L1 Active Cycles         cycle   5994322.96
    Total L1 Elapsed Cycles          cycle    143914864
    Average L2 Active Cycles         cycle   5267920.75
    Total L2 Elapsed Cycles          cycle     85902688
    Average SM Active Cycles         cycle   5994322.96
    Total SM Elapsed Cycles          cycle    143914864
    Average SMSP Active Cycles       cycle   5991776.28
    Total SMSP Elapsed Cycles        cycle    575659456
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02139%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.03
    Elapsed Cycles                cycle      5965585
    Memory Throughput                 %        93.15
    DRAM Throughput                   %         0.75
    Duration                         ms         5.76
    L1/TEX Cache Throughput           %        93.35
    L2 Cache Throughput               %        15.12
    SM Active Cycles              cycle   5953085.79
    Compute (SM) Throughput           %        93.15
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.32
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.87
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.44
    Mem Busy                              %        66.72
    Max Bandwidth                         %        93.15
    L1/TEX Hit Rate                       %        86.84
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.30
    Mem Pipes Busy                        %        93.15
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.66%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.69%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.47
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.849%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.47 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.98
    Warp Cycles Per Executed Instruction           cycle        22.98
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.849%                                                                                          
          On average, each warp of this kernel spends 8.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.3% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.17
    Issued Instructions                             inst    293283472
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.34
    Achieved Active Warps Per SM           warp        47.21
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle    138215424
    Average L1 Active Cycles         cycle   5953085.79
    Total L1 Elapsed Cycles          cycle    143174016
    Average L2 Active Cycles         cycle   5884836.75
    Total L2 Elapsed Cycles          cycle     96832688
    Average SM Active Cycles         cycle   5953085.79
    Total SM Elapsed Cycles          cycle    143174016
    Average SMSP Active Cycles       cycle   5947009.32
    Total SMSP Elapsed Cycles        cycle    572696064
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0212%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.68
    Elapsed Cycles                cycle      5969362
    Memory Throughput                 %        93.09
    DRAM Throughput                   %         1.22
    Duration                         ms         3.55
    L1/TEX Cache Throughput           %        93.25
    L2 Cache Throughput               %        14.18
    SM Active Cycles              cycle   5959444.50
    Compute (SM) Throughput           %        93.09
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         5.51
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.26
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.82
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.34
    Mem Busy                              %        65.00
    Max Bandwidth                         %        93.09
    L1/TEX Hit Rate                       %        87.85
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.39
    Mem Pipes Busy                        %        93.09
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.63%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.68%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.21
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.79
    Active Warps Per Scheduler          warp        11.75
    Eligible Warps Per Scheduler        warp         2.30
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.905%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.75 active warps per scheduler, but only an average of 2.30 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.95
    Warp Cycles Per Executed Instruction           cycle        22.95
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.905%                                                                                          
          On average, each warp of this kernel spends 7.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.3% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.28
    Issued Instructions                             inst    293283483
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.03
    Achieved Active Warps Per SM           warp        47.06
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle     85213184
    Average L1 Active Cycles         cycle   5959444.50
    Total L1 Elapsed Cycles          cycle    143260208
    Average L2 Active Cycles         cycle   5697231.50
    Total L2 Elapsed Cycles          cycle     94285616
    Average SM Active Cycles         cycle   5959444.50
    Total SM Elapsed Cycles          cycle    143260208
    Average SMSP Active Cycles       cycle   5965811.32
    Total SMSP Elapsed Cycles        cycle    573040832
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02108%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.98
    Elapsed Cycles                cycle      5975701
    Memory Throughput                 %        93.00
    DRAM Throughput                   %         1.44
    Duration                         ms         3.02
    L1/TEX Cache Throughput           %        93.12
    L2 Cache Throughput               %        14.81
    SM Active Cycles              cycle   5967734.58
    Compute (SM) Throughput           %        93.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.42
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.19
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.74
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.9%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.76
    Mem Busy                              %        65.07
    Max Bandwidth                         %        93.00
    L1/TEX Hit Rate                       %        87.80
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.88
    Mem Pipes Busy                        %        93.00
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.58%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.66%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.19
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.81
    Active Warps Per Scheduler          warp        11.76
    Eligible Warps Per Scheduler        warp         2.26
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.002%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.76 active warps per scheduler, but only an average of 2.26 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.97
    Warp Cycles Per Executed Instruction           cycle        22.97
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.002%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.7% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.01
    Issued Instructions                             inst    293283457
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.98
    Achieved Active Warps Per SM           warp        47.03
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260236
    Total DRAM Elapsed Cycles        cycle     72380416
    Average L1 Active Cycles         cycle   5967734.58
    Total L1 Elapsed Cycles          cycle    143409656
    Average L2 Active Cycles         cycle   5457403.56
    Total L2 Elapsed Cycles          cycle     88996512
    Average SM Active Cycles         cycle   5967734.58
    Total SM Elapsed Cycles          cycle    143409656
    Average SMSP Active Cycles       cycle   5968030.39
    Total SMSP Elapsed Cycles        cycle    573638624
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02139%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       772.44
    Elapsed Cycles                cycle      6015192
    Memory Throughput                 %        93.11
    DRAM Throughput                   %         0.56
    Duration                         ms         7.73
    L1/TEX Cache Throughput           %        93.35
    L2 Cache Throughput               %        14.67
    SM Active Cycles              cycle   5953070.46
    Compute (SM) Throughput           %        93.11
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.06
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.32
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.87
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.08
    Mem Busy                              %        66.99
    Max Bandwidth                         %        93.11
    L1/TEX Hit Rate                       %        86.62
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.02
    Mem Pipes Busy                        %        93.11
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.64%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.68%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.51
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.892%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.51 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.99
    Warp Cycles Per Executed Instruction           cycle        22.99
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.892%                                                                                          
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.5% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.07
    Issued Instructions                             inst    293283463
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.32
    Achieved Active Warps Per SM           warp        47.20
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle    185282560
    Average L1 Active Cycles         cycle   5953070.46
    Total L1 Elapsed Cycles          cycle    143239448
    Average L2 Active Cycles         cycle   6206247.50
    Total L2 Elapsed Cycles          cycle    101990000
    Average SM Active Cycles         cycle   5953070.46
    Total SM Elapsed Cycles          cycle    143239448
    Average SMSP Active Cycles       cycle   5945388.04
    Total SMSP Elapsed Cycles        cycle    572957792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02123%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.56
    Elapsed Cycles                cycle      5968113
    Memory Throughput                 %        93.11
    DRAM Throughput                   %         1.13
    Duration                         ms         3.83
    L1/TEX Cache Throughput           %        93.28
    L2 Cache Throughput               %        14.24
    SM Active Cycles              cycle   5957415.54
    Compute (SM) Throughput           %        93.11
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         5.83
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.28
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.84
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.18
    Mem Busy                              %        65.08
    Max Bandwidth                         %        93.11
    L1/TEX Hit Rate                       %        87.74
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.37
    Mem Pipes Busy                        %        93.11
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.64%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.68%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.30
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.70
    Active Warps Per Scheduler          warp        11.79
    Eligible Warps Per Scheduler        warp         2.39
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.888%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.79 active warps per scheduler, but only an average of 2.39 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.99
    Warp Cycles Per Executed Instruction           cycle        22.99
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.888%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.9% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.08
    Issued Instructions                             inst    293283464
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.26
    Achieved Active Warps Per SM           warp        47.16
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle     91742208
    Average L1 Active Cycles         cycle   5957415.54
    Total L1 Elapsed Cycles          cycle    143233872
    Average L2 Active Cycles         cycle   5910407.56
    Total L2 Elapsed Cycles          cycle     96405040
    Average SM Active Cycles         cycle   5957415.54
    Total SM Elapsed Cycles          cycle    143233872
    Average SMSP Active Cycles       cycle   5954961.28
    Total SMSP Elapsed Cycles        cycle    572935488
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02138%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.78
    Elapsed Cycles                cycle      5966696
    Memory Throughput                 %        93.13
    DRAM Throughput                   %         1.30
    Duration                         ms         3.34
    L1/TEX Cache Throughput           %        93.20
    L2 Cache Throughput               %        14.41
    SM Active Cycles              cycle   5962493.83
    Compute (SM) Throughput           %        93.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         6.49
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.24
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.79
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.49
    Mem Busy                              %        64.99
    Max Bandwidth                         %        93.13
    L1/TEX Hit Rate                       %        87.82
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.25
    Mem Pipes Busy                        %        93.13
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.65%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.69%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.27
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.73
    Active Warps Per Scheduler          warp        11.77
    Eligible Warps Per Scheduler        warp         2.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.865%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.77 active warps per scheduler, but only an average of 2.34 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.96
    Warp Cycles Per Executed Instruction           cycle        22.96
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.865%                                                                                          
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.0% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.18
    Issued Instructions                             inst    293283473
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.92
    Achieved Active Warps Per SM           warp        47.00
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260316
    Total DRAM Elapsed Cycles        cycle     80161792
    Average L1 Active Cycles         cycle   5962493.83
    Total L1 Elapsed Cycles          cycle    143198688
    Average L2 Active Cycles         cycle   5653387.62
    Total L2 Elapsed Cycles          cycle     92255600
    Average SM Active Cycles         cycle   5962493.83
    Total SM Elapsed Cycles          cycle    143198688
    Average SMSP Active Cycles       cycle   5958955.35
    Total SMSP Elapsed Cycles        cycle    572794752
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02137%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.78
    Elapsed Cycles                cycle      5967569
    Memory Throughput                 %        93.12
    DRAM Throughput                   %         1.30
    Duration                         ms         3.34
    L1/TEX Cache Throughput           %        93.21
    L2 Cache Throughput               %        14.45
    SM Active Cycles              cycle   5962050.38
    Compute (SM) Throughput           %        93.12
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         5.70
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.24
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.80
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.49
    Mem Busy                              %        65.08
    Max Bandwidth                         %        93.12
    L1/TEX Hit Rate                       %        87.85
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.94
    Mem Pipes Busy                        %        93.12
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.64%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.68%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.24
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.76
    Active Warps Per Scheduler          warp        11.75
    Eligible Warps Per Scheduler        warp         2.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.879%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.75 active warps per scheduler, but only an average of 2.33 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.94
    Warp Cycles Per Executed Instruction           cycle        22.94
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.879%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.1% of the total average of 22.9 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055035.81
    Issued Instructions                             inst    293283438
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.03
    Achieved Active Warps Per SM           warp        47.06
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260244
    Total DRAM Elapsed Cycles        cycle     80173056
    Average L1 Active Cycles         cycle   5962050.38
    Total L1 Elapsed Cycles          cycle    143219344
    Average L2 Active Cycles         cycle   5659530.88
    Total L2 Elapsed Cycles          cycle     92268592
    Average SM Active Cycles         cycle   5962050.38
    Total SM Elapsed Cycles          cycle    143219344
    Average SMSP Active Cycles       cycle   5962526.05
    Total SMSP Elapsed Cycles        cycle    572877376
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0214%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.21
    Elapsed Cycles                cycle      5967118
    Memory Throughput                 %        93.13
    DRAM Throughput                   %         0.88
    Duration                         ms         4.91
    L1/TEX Cache Throughput           %        93.33
    L2 Cache Throughput               %        15.02
    SM Active Cycles              cycle   5953953.04
    Compute (SM) Throughput           %        93.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.00
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.31
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.87
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.70
    Mem Busy                              %        66.01
    Max Bandwidth                         %        93.13
    L1/TEX Hit Rate                       %        87.00
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.88
    Mem Pipes Busy                        %        93.13
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.64%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.68%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.46
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.873%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.46 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.97
    Warp Cycles Per Executed Instruction           cycle        22.97
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.873%                                                                                          
          On average, each warp of this kernel spends 8.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.8% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.15
    Issued Instructions                             inst    293283470
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.19
    Achieved Active Warps Per SM           warp        47.13
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle    117772288
    Average L1 Active Cycles         cycle   5953953.04
    Total L1 Elapsed Cycles          cycle    143210632
    Average L2 Active Cycles         cycle      5803292
    Total L2 Elapsed Cycles          cycle     95468192
    Average SM Active Cycles         cycle   5953953.04
    Total SM Elapsed Cycles          cycle    143210632
    Average SMSP Active Cycles       cycle   5948109.17
    Total SMSP Elapsed Cycles        cycle    572842528
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0212%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.41
    Elapsed Cycles                cycle      5970540
    Memory Throughput                 %        93.08
    DRAM Throughput                   %         1.03
    Duration                         ms         4.23
    L1/TEX Cache Throughput           %        93.31
    L2 Cache Throughput               %        14.67
    SM Active Cycles              cycle   5955599.50
    Compute (SM) Throughput           %        93.08
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.04
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.30
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.85
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.97
    Mem Busy                              %        65.28
    Max Bandwidth                         %        93.08
    L1/TEX Hit Rate                       %        87.19
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.20
    Mem Pipes Busy                        %        93.08
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.62%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.67%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.33
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.67
    Active Warps Per Scheduler          warp        11.79
    Eligible Warps Per Scheduler        warp         2.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.925%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.79 active warps per scheduler, but only an average of 2.45 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.96
    Warp Cycles Per Executed Instruction           cycle        22.96
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.86
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.925%                                                                                          
          On average, each warp of this kernel spends 8.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.7% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3054945.33
    Executed Instructions                           inst    293274752
    Avg. Issued Instructions Per Scheduler          inst   3055036.07
    Issued Instructions                             inst    293283463
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.12
    Achieved Active Warps Per SM           warp        47.10
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260236
    Total DRAM Elapsed Cycles        cycle    101547008
    Average L1 Active Cycles         cycle   5955599.50
    Total L1 Elapsed Cycles          cycle    143289944
    Average L2 Active Cycles         cycle   5862139.50
    Total L2 Elapsed Cycles          cycle     95397344
    Average SM Active Cycles         cycle   5955599.50
    Total SM Elapsed Cycles          cycle    143289944
    Average SMSP Active Cycles       cycle   5951487.74
    Total SMSP Elapsed Cycles        cycle    573159776
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      8945024
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02143%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.00
    SM Frequency                    Ghz         1.75
    Elapsed Cycles                cycle     11624803
    Memory Throughput                 %        93.19
    DRAM Throughput                   %         1.02
    Duration                         ms         6.62
    L1/TEX Cache Throughput           %        93.48
    L2 Cache Throughput               %        15.17
    SM Active Cycles              cycle  11588628.38
    Compute (SM) Throughput           %        93.19
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.65
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.34
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.99
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.96
    Mem Busy                              %        62.08
    Max Bandwidth                         %        93.19
    L1/TEX Hit Rate                       %        87.22
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.67
    Mem Pipes Busy                        %        93.19
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.14%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.49%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.808%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.33 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.09
    Warp Cycles Per Executed Instruction           cycle        23.09
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.808%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.3% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.86
    Issued Instructions                             inst    571176563
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.74
    Achieved Active Warps Per SM           warp        47.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406168
    Total DRAM Elapsed Cycles        cycle    158842880
    Average L1 Active Cycles         cycle  11588628.38
    Total L1 Elapsed Cycles          cycle    278994120
    Average L2 Active Cycles         cycle     11103098
    Total L2 Elapsed Cycles          cycle    181224544
    Average SM Active Cycles         cycle  11588628.38
    Total SM Elapsed Cycles          cycle    278994120
    Average SMSP Active Cycles       cycle  11584499.15
    Total SMSP Elapsed Cycles        cycle   1115976480
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02421%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.74
    Elapsed Cycles                cycle     11624788
    Memory Throughput                 %        93.21
    DRAM Throughput                   %         1.01
    Duration                         ms         6.68
    L1/TEX Cache Throughput           %        93.48
    L2 Cache Throughput               %        15.32
    SM Active Cycles              cycle  11588545.62
    Compute (SM) Throughput           %        93.21
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.30
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.34
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.94
    Mem Busy                              %        62.10
    Max Bandwidth                         %        93.21
    L1/TEX Hit Rate                       %        87.23
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.51
    Mem Pipes Busy                        %        93.21
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.5%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.791%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.33 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.08
    Warp Cycles Per Executed Instruction           cycle        23.08
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.791%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.0% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.93
    Issued Instructions                             inst    571176569
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.74
    Achieved Active Warps Per SM           warp        47.39
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406064
    Total DRAM Elapsed Cycles        cycle    160219136
    Average L1 Active Cycles         cycle  11588545.62
    Total L1 Elapsed Cycles          cycle    278940472
    Average L2 Active Cycles         cycle  11086320.94
    Total L2 Elapsed Cycles          cycle    180539200
    Average SM Active Cycles         cycle  11588545.62
    Total SM Elapsed Cycles          cycle    278940472
    Average SMSP Active Cycles       cycle  11584306.94
    Total SMSP Elapsed Cycles        cycle   1115761888
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02427%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.69
    Elapsed Cycles                cycle     11626942
    Memory Throughput                 %        93.17
    DRAM Throughput                   %         0.99
    Duration                         ms         6.86
    L1/TEX Cache Throughput           %        93.49
    L2 Cache Throughput               %        15.15
    SM Active Cycles              cycle  11587633.25
    Compute (SM) Throughput           %        93.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.24
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.91
    Mem Busy                              %        62.09
    Max Bandwidth                         %        93.17
    L1/TEX Hit Rate                       %        87.32
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.61
    Mem Pipes Busy                        %        93.17
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.13%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.49%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.825%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.33 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.08
    Warp Cycles Per Executed Instruction           cycle        23.08
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.825%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.9% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.79
    Issued Instructions                             inst    571176556
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.76
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       408792
    Total DRAM Elapsed Cycles        cycle    164495360
    Average L1 Active Cycles         cycle  11587633.25
    Total L1 Elapsed Cycles          cycle    279044032
    Average L2 Active Cycles         cycle  11199898.31
    Total L2 Elapsed Cycles          cycle    182730208
    Average SM Active Cycles         cycle  11587633.25
    Total SM Elapsed Cycles          cycle    279044032
    Average SMSP Active Cycles       cycle  11583557.80
    Total SMSP Elapsed Cycles        cycle   1116176128
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02422%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.68
    Elapsed Cycles                cycle     11626983
    Memory Throughput                 %        93.17
    DRAM Throughput                   %         0.98
    Duration                         ms         6.92
    L1/TEX Cache Throughput           %        93.49
    L2 Cache Throughput               %        15.02
    SM Active Cycles              cycle  11587231.46
    Compute (SM) Throughput           %        93.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.47
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.88
    Mem Busy                              %        62.13
    Max Bandwidth                         %        93.17
    L1/TEX Hit Rate                       %        87.19
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.33
    Mem Pipes Busy                        %        93.17
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.13%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.49%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.826%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.35 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.08
    Warp Cycles Per Executed Instruction           cycle        23.08
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.826%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.2% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.96
    Issued Instructions                             inst    571176572
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.78
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405912
    Total DRAM Elapsed Cycles        cycle    165963776
    Average L1 Active Cycles         cycle  11587231.46
    Total L1 Elapsed Cycles          cycle    279045792
    Average L2 Active Cycles         cycle  11195495.50
    Total L2 Elapsed Cycles          cycle    184360592
    Average SM Active Cycles         cycle  11587231.46
    Total SM Elapsed Cycles          cycle    279045792
    Average SMSP Active Cycles       cycle  11582197.74
    Total SMSP Elapsed Cycles        cycle   1116183168
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.024%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     11652302
    Memory Throughput                 %        92.97
    DRAM Throughput                   %         0.80
    Duration                         ms         8.44
    L1/TEX Cache Throughput           %        93.52
    L2 Cache Throughput               %        14.99
    SM Active Cycles              cycle  11583423.17
    Compute (SM) Throughput           %        92.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.26
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.36
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.02
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.54
    Mem Busy                              %        61.85
    Max Bandwidth                         %        92.97
    L1/TEX Hit Rate                       %        87.20
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.32
    Mem Pipes Busy                        %        92.97
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.03%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.43%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.39
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.61
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.40
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.028%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.40 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.08
    Warp Cycles Per Executed Instruction           cycle        23.08
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.028%                                                                                          
          On average, each warp of this kernel spends 8.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.8% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.76
    Issued Instructions                             inst    571176553
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.74
    Achieved Active Warps Per SM           warp        47.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406052
    Total DRAM Elapsed Cycles        cycle    202481664
    Average L1 Active Cycles         cycle  11583423.17
    Total L1 Elapsed Cycles          cycle    279653288
    Average L2 Active Cycles         cycle  11506488.94
    Total L2 Elapsed Cycles          cycle    186422944
    Average SM Active Cycles         cycle  11583423.17
    Total SM Elapsed Cycles          cycle    279653288
    Average SMSP Active Cycles       cycle  11578527.56
    Total SMSP Elapsed Cycles        cycle   1118613152
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02439%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     11629488
    Memory Throughput                 %        93.15
    DRAM Throughput                   %         0.97
    Duration                         ms         6.98
    L1/TEX Cache Throughput           %        93.48
    L2 Cache Throughput               %        15.18
    SM Active Cycles              cycle  11589222.54
    Compute (SM) Throughput           %        93.15
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.91
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.34
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        58.99
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.86
    Mem Busy                              %        62.09
    Max Bandwidth                         %        93.15
    L1/TEX Hit Rate                       %        87.35
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.31
    Mem Pipes Busy                        %        93.15
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.12%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.48%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.846%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.34 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.09
    Warp Cycles Per Executed Instruction           cycle        23.09
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.846%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.9% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.82
    Issued Instructions                             inst    571176559
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.75
    Achieved Active Warps Per SM           warp        47.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406100
    Total DRAM Elapsed Cycles        cycle    167493632
    Average L1 Active Cycles         cycle  11589222.54
    Total L1 Elapsed Cycles          cycle    279106608
    Average L2 Active Cycles         cycle  11045073.56
    Total L2 Elapsed Cycles          cycle    182716128
    Average SM Active Cycles         cycle  11589222.54
    Total SM Elapsed Cycles          cycle    279106608
    Average SMSP Active Cycles       cycle  11584305.03
    Total SMSP Elapsed Cycles        cycle   1116426432
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02389%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.72
    Elapsed Cycles                cycle     11626596
    Memory Throughput                 %        93.18
    DRAM Throughput                   %         1.01
    Duration                         ms         6.74
    L1/TEX Cache Throughput           %        93.49
    L2 Cache Throughput               %        15.35
    SM Active Cycles              cycle  11588056.38
    Compute (SM) Throughput           %        93.18
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.37
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.34
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.93
    Mem Busy                              %        62.19
    Max Bandwidth                         %        93.18
    L1/TEX Hit Rate                       %        87.27
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.43
    Mem Pipes Busy                        %        93.18
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.13%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.49%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.822%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.33 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.08
    Warp Cycles Per Executed Instruction           cycle        23.08
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.822%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.9% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.95
    Issued Instructions                             inst    571176571
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.77
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406152
    Total DRAM Elapsed Cycles        cycle    161630208
    Average L1 Active Cycles         cycle  11588056.38
    Total L1 Elapsed Cycles          cycle    279035232
    Average L2 Active Cycles         cycle  11156496.12
    Total L2 Elapsed Cycles          cycle    181163824
    Average SM Active Cycles         cycle  11588056.38
    Total SM Elapsed Cycles          cycle    279035232
    Average SMSP Active Cycles       cycle  11584435.80
    Total SMSP Elapsed Cycles        cycle   1116140928
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02434%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.65
    Elapsed Cycles                cycle     11645294
    Memory Throughput                 %        93.03
    DRAM Throughput                   %         0.96
    Duration                         ms         7.06
    L1/TEX Cache Throughput           %        93.49
    L2 Cache Throughput               %        15.29
    SM Active Cycles              cycle  11587485.33
    Compute (SM) Throughput           %        93.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.91
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.04
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.84
    Mem Busy                              %        62.01
    Max Bandwidth                         %        93.03
    L1/TEX Hit Rate                       %        87.24
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.21
    Mem Pipes Busy                        %        93.03
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.45%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.972%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.35 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.09
    Warp Cycles Per Executed Instruction           cycle        23.09
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.972%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.5% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.75
    Issued Instructions                             inst    571176552
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.77
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406120
    Total DRAM Elapsed Cycles        cycle    169246720
    Average L1 Active Cycles         cycle  11587485.33
    Total L1 Elapsed Cycles          cycle    279485744
    Average L2 Active Cycles         cycle  11202522.50
    Total L2 Elapsed Cycles          cycle    182935568
    Average SM Active Cycles         cycle  11587485.33
    Total SM Elapsed Cycles          cycle    279485744
    Average SMSP Active Cycles       cycle  11582872.17
    Total SMSP Elapsed Cycles        cycle   1117942976
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0242%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.68
    Elapsed Cycles                cycle     11633416
    Memory Throughput                 %        93.12
    DRAM Throughput                   %         0.98
    Duration                         ms         6.92
    L1/TEX Cache Throughput           %        93.48
    L2 Cache Throughput               %        15.08
    SM Active Cycles              cycle  11588266.54
    Compute (SM) Throughput           %        93.12
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.98
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.34
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.88
    Mem Busy                              %        62.07
    Max Bandwidth                         %        93.12
    L1/TEX Hit Rate                       %        87.27
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.06
    Mem Pipes Busy                        %        93.12
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.11%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.47%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.877%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.35 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.06
    Warp Cycles Per Executed Instruction           cycle        23.06
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.877%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.5% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.86
    Issued Instructions                             inst    571176563
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.69
    Achieved Active Warps Per SM           warp        47.37
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406168
    Total DRAM Elapsed Cycles        cycle    166055936
    Average L1 Active Cycles         cycle  11588266.54
    Total L1 Elapsed Cycles          cycle    279200616
    Average L2 Active Cycles         cycle  11200020.31
    Total L2 Elapsed Cycles          cycle    184463088
    Average SM Active Cycles         cycle  11588266.54
    Total SM Elapsed Cycles          cycle    279200616
    Average SMSP Active Cycles       cycle  11583099.54
    Total SMSP Elapsed Cycles        cycle   1116802464
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02399%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.68
    Elapsed Cycles                cycle     11628913
    Memory Throughput                 %        93.16
    DRAM Throughput                   %         0.98
    Duration                         ms         6.92
    L1/TEX Cache Throughput           %        93.49
    L2 Cache Throughput               %        15.04
    SM Active Cycles              cycle  11587179.33
    Compute (SM) Throughput           %        93.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.86
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.88
    Mem Busy                              %        62.09
    Max Bandwidth                         %        93.16
    L1/TEX Hit Rate                       %        87.37
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.62
    Mem Pipes Busy                        %        93.16
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.12%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.48%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.841%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.33 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.09
    Warp Cycles Per Executed Instruction           cycle        23.09
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.841%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.1% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5949665
    Executed Instructions                           inst    571167840
    Avg. Issued Instructions Per Scheduler          inst   5949755.90
    Issued Instructions                             inst    571176566
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.77
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       407448
    Total DRAM Elapsed Cycles        cycle    165991424
    Average L1 Active Cycles         cycle  11587179.33
    Total L1 Elapsed Cycles          cycle    279092208
    Average L2 Active Cycles         cycle  11196716.62
    Total L2 Elapsed Cycles          cycle    184388800
    Average SM Active Cycles         cycle  11587179.33
    Total SM Elapsed Cycles          cycle    279092208
    Average SMSP Active Cycles       cycle  11584183.85
    Total SMSP Elapsed Cycles        cycle   1116368832
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17176640
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.024%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.68
    Elapsed Cycles                cycle     19975801
    Memory Throughput                 %        93.60
    DRAM Throughput                   %         0.82
    Duration                         ms        11.89
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        17.04
    SM Active Cycles              cycle  19964841.75
    Compute (SM) Throughput           %        93.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.12
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.57
    Mem Busy                              %        50.59
    Max Bandwidth                         %        93.60
    L1/TEX Hit Rate                       %        84.52
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.80
    Mem Pipes Busy                        %        93.60
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.39
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.61
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.403%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.10
    Warp Cycles Per Executed Instruction           cycle        23.10
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.403%                                                                                          
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.2% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.80
    Issued Instructions                             inst    985031693
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.00
    Achieved Active Warps Per SM           warp        47.52
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584352
    Total DRAM Elapsed Cycles        cycle    285132800
    Average L1 Active Cycles         cycle  19964841.75
    Total L1 Elapsed Cycles          cycle    479417624
    Average L2 Active Cycles         cycle  19335137.81
    Total L2 Elapsed Cycles          cycle    316745072
    Average SM Active Cycles         cycle  19964841.75
    Total SM Elapsed Cycles          cycle    479417624
    Average SMSP Active Cycles       cycle  19965808.02
    Total SMSP Elapsed Cycles        cycle   1917670496
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.71
    Elapsed Cycles                cycle     19981516
    Memory Throughput                 %        93.57
    DRAM Throughput                   %         0.83
    Duration                         ms        11.69
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        17.37
    SM Active Cycles              cycle  19966679.50
    Compute (SM) Throughput           %        93.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.21
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.60
    Mem Busy                              %        50.59
    Max Bandwidth                         %        93.57
    L1/TEX Hit Rate                       %        84.53
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.48
    Mem Pipes Busy                        %        93.57
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.05%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.39
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.61
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.426%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.10
    Warp Cycles Per Executed Instruction           cycle        23.10
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.426%                                                                                          
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.2% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.81
    Issued Instructions                             inst    985031694
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.94
    Achieved Active Warps Per SM           warp        47.49
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584220
    Total DRAM Elapsed Cycles        cycle    280221696
    Average L1 Active Cycles         cycle  19966679.50
    Total L1 Elapsed Cycles          cycle    479533216
    Average L2 Active Cycles         cycle  19191054.94
    Total L2 Elapsed Cycles          cycle    312852528
    Average SM Active Cycles         cycle  19966679.50
    Total SM Elapsed Cycles          cycle    479533216
    Average SMSP Active Cycles       cycle  19965919.52
    Total SMSP Elapsed Cycles        cycle   1918132864
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.72
    Elapsed Cycles                cycle     19986919
    Memory Throughput                 %        93.55
    DRAM Throughput                   %         0.84
    Duration                         ms        11.59
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        17.62
    SM Active Cycles              cycle  19966781.17
    Compute (SM) Throughput           %        93.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.12
    Dropped Samples                sample           10
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.62
    Mem Busy                              %        50.56
    Max Bandwidth                         %        93.55
    L1/TEX Hit Rate                       %        84.18
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.88
    Mem Pipes Busy                        %        93.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.04%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.446%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.10
    Warp Cycles Per Executed Instruction           cycle        23.10
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.446%                                                                                          
          On average, each warp of this kernel spends 8.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.1% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.71
    Issued Instructions                             inst    985031684
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.94
    Achieved Active Warps Per SM           warp        47.49
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       585220
    Total DRAM Elapsed Cycles        cycle    277870592
    Average L1 Active Cycles         cycle  19966781.17
    Total L1 Elapsed Cycles          cycle    479639824
    Average L2 Active Cycles         cycle  19186808.62
    Total L2 Elapsed Cycles          cycle    310329568
    Average SM Active Cycles         cycle  19966781.17
    Total SM Elapsed Cycles          cycle    479639824
    Average SMSP Active Cycles       cycle  19968979.38
    Total SMSP Elapsed Cycles        cycle   1918559296
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.72
    Elapsed Cycles                cycle     19981187
    Memory Throughput                 %        93.57
    DRAM Throughput                   %         0.85
    Duration                         ms        11.58
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        17.32
    SM Active Cycles              cycle  19966735.83
    Compute (SM) Throughput           %        93.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.21
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.62
    Mem Busy                              %        50.59
    Max Bandwidth                         %        93.57
    L1/TEX Hit Rate                       %        84.31
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.00
    Mem Pipes Busy                        %        93.57
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.04%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.428%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.09
    Warp Cycles Per Executed Instruction           cycle        23.09
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.428%                                                                                          
          On average, each warp of this kernel spends 8.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.1% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.80
    Issued Instructions                             inst    985031693
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.89
    Achieved Active Warps Per SM           warp        47.47
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       587764
    Total DRAM Elapsed Cycles        cycle    277772288
    Average L1 Active Cycles         cycle  19966735.83
    Total L1 Elapsed Cycles          cycle    479544864
    Average L2 Active Cycles         cycle  19150032.44
    Total L2 Elapsed Cycles          cycle    311352064
    Average SM Active Cycles         cycle  19966735.83
    Total SM Elapsed Cycles          cycle    479544864
    Average SMSP Active Cycles       cycle  19969572.68
    Total SMSP Elapsed Cycles        cycle   1918179456
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.63
    Elapsed Cycles                cycle     19979610
    Memory Throughput                 %        93.58
    DRAM Throughput                   %         0.80
    Duration                         ms        12.22
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        16.79
    SM Active Cycles              cycle  19964035.88
    Compute (SM) Throughput           %        93.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.06
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.40
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.53
    Mem Busy                              %        50.58
    Max Bandwidth                         %        93.58
    L1/TEX Hit Rate                       %        84.33
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.01
    Mem Pipes Busy                        %        93.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.05%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.421%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.11
    Warp Cycles Per Executed Instruction           cycle        23.11
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.421%                                                                                          
          On average, each warp of this kernel spends 8.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.1% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.88
    Issued Instructions                             inst    985031700
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.92
    Achieved Active Warps Per SM           warp        47.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       585828
    Total DRAM Elapsed Cycles        cycle    293036032
    Average L1 Active Cycles         cycle  19964035.88
    Total L1 Elapsed Cycles          cycle    479508560
    Average L2 Active Cycles         cycle  19302837.38
    Total L2 Elapsed Cycles          cycle    316683984
    Average SM Active Cycles         cycle  19964035.88
    Total SM Elapsed Cycles          cycle    479508560
    Average SMSP Active Cycles       cycle  19971439.17
    Total SMSP Elapsed Cycles        cycle   1918034240
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.69
    Elapsed Cycles                cycle     19989606
    Memory Throughput                 %        93.53
    DRAM Throughput                   %         0.83
    Duration                         ms        11.79
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        17.46
    SM Active Cycles              cycle  19966343.42
    Compute (SM) Throughput           %        93.53
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.55
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.59
    Mem Busy                              %        50.58
    Max Bandwidth                         %        93.53
    L1/TEX Hit Rate                       %        84.27
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.72
    Mem Pipes Busy                        %        93.53
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.03%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.39
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.61
    Active Warps Per Scheduler          warp        11.88
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.468%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.88 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.11
    Warp Cycles Per Executed Instruction           cycle        23.11
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.468%                                                                                          
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.2% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.73
    Issued Instructions                             inst    985031686
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.92
    Achieved Active Warps Per SM           warp        47.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       587120
    Total DRAM Elapsed Cycles        cycle    282806272
    Average L1 Active Cycles         cycle  19966343.42
    Total L1 Elapsed Cycles          cycle    479748024
    Average L2 Active Cycles         cycle  19176196.50
    Total L2 Elapsed Cycles          cycle    314161712
    Average SM Active Cycles         cycle  19966343.42
    Total SM Elapsed Cycles          cycle    479748024
    Average SMSP Active Cycles       cycle  19967003.82
    Total SMSP Elapsed Cycles        cycle   1918992096
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.65
    Elapsed Cycles                cycle     19980396
    Memory Throughput                 %        93.58
    DRAM Throughput                   %         0.80
    Duration                         ms        12.11
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        17.03
    SM Active Cycles              cycle  19966017.71
    Compute (SM) Throughput           %        93.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.91
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.54
    Mem Busy                              %        50.56
    Max Bandwidth                         %        93.58
    L1/TEX Hit Rate                       %        84.60
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.75
    Mem Pipes Busy                        %        93.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.05%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.57
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.425%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.57 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.10
    Warp Cycles Per Executed Instruction           cycle        23.10
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.425%                                                                                          
          On average, each warp of this kernel spends 8.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.1% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.68
    Issued Instructions                             inst    985031681
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.99
    Achieved Active Warps Per SM           warp        47.52
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584368
    Total DRAM Elapsed Cycles        cycle    290383872
    Average L1 Active Cycles         cycle  19966017.71
    Total L1 Elapsed Cycles          cycle    479528488
    Average L2 Active Cycles         cycle  19348099.81
    Total L2 Elapsed Cycles          cycle    313872864
    Average SM Active Cycles         cycle  19966017.71
    Total SM Elapsed Cycles          cycle    479528488
    Average SMSP Active Cycles       cycle  19971620.23
    Total SMSP Elapsed Cycles        cycle   1918113952
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.87
    Elapsed Cycles                cycle     19993622
    Memory Throughput                 %        93.52
    DRAM Throughput                   %         0.91
    Duration                         ms        10.66
    L1/TEX Cache Throughput           %        93.62
    L2 Cache Throughput               %        17.86
    SM Active Cycles              cycle     19971119
    Compute (SM) Throughput           %        93.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.68
    Dropped Samples                sample            3
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.38
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.75
    Mem Busy                              %        50.55
    Max Bandwidth                         %        93.52
    L1/TEX Hit Rate                       %        83.99
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.23
    Mem Pipes Busy                        %        93.52
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.02%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.56
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.485%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.56 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.10
    Warp Cycles Per Executed Instruction           cycle        23.10
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.485%                                                                                          
          On average, each warp of this kernel spends 8.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.9% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.74
    Issued Instructions                             inst    985031687
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.88
    Achieved Active Warps Per SM           warp        47.46
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584100
    Total DRAM Elapsed Cycles        cycle    255717376
    Average L1 Active Cycles         cycle     19971119
    Total L1 Elapsed Cycles          cycle    479836992
    Average L2 Active Cycles         cycle     18784793
    Total L2 Elapsed Cycles          cycle    304026320
    Average SM Active Cycles         cycle     19971119
    Total SM Elapsed Cycles          cycle    479836992
    Average SMSP Active Cycles       cycle  19975828.58
    Total SMSP Elapsed Cycles        cycle   1919347968
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.75
    Elapsed Cycles                cycle     19979464
    Memory Throughput                 %        93.58
    DRAM Throughput                   %         0.86
    Duration                         ms        11.38
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        17.36
    SM Active Cycles              cycle  19967201.96
    Compute (SM) Throughput           %        93.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.04
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.64
    Mem Busy                              %        50.58
    Max Bandwidth                         %        93.58
    L1/TEX Hit Rate                       %        84.40
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.11
    Mem Pipes Busy                        %        93.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.05%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.39
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.61
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.42%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.10
    Warp Cycles Per Executed Instruction           cycle        23.10
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.42%                                                                                           
          On average, each warp of this kernel spends 8.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.1% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.85
    Issued Instructions                             inst    985031698
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.86
    Achieved Active Warps Per SM           warp        47.45
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584288
    Total DRAM Elapsed Cycles        cycle    272998400
    Average L1 Active Cycles         cycle  19967201.96
    Total L1 Elapsed Cycles          cycle    479505912
    Average L2 Active Cycles         cycle     19218401
    Total L2 Elapsed Cycles          cycle    311471200
    Average SM Active Cycles         cycle  19967201.96
    Total SM Elapsed Cycles          cycle    479505912
    Average SMSP Active Cycles       cycle  19968253.08
    Total SMSP Elapsed Cycles        cycle   1918023648
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.69
    Elapsed Cycles                cycle     19984660
    Memory Throughput                 %        93.56
    DRAM Throughput                   %         0.83
    Duration                         ms        11.79
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        17.20
    SM Active Cycles              cycle  19966484.71
    Compute (SM) Throughput           %        93.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.96
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.39
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.59
    Mem Busy                              %        50.56
    Max Bandwidth                         %        93.56
    L1/TEX Hit Rate                       %        84.44
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.10
    Mem Pipes Busy                        %        93.56
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.04%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.39
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.61
    Active Warps Per Scheduler          warp        11.88
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.444%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.88 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.11
    Warp Cycles Per Executed Instruction           cycle        23.11
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.444%                                                                                          
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.2% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10260656
    Executed Instructions                           inst    985022976
    Avg. Issued Instructions Per Scheduler          inst  10260746.81
    Issued Instructions                             inst    985031694
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.91
    Achieved Active Warps Per SM           warp        47.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584096
    Total DRAM Elapsed Cycles        cycle    282734592
    Average L1 Active Cycles         cycle  19966484.71
    Total L1 Elapsed Cycles          cycle    479628768
    Average L2 Active Cycles         cycle  19337931.31
    Total L2 Elapsed Cycles          cycle    314080368
    Average SM Active Cycles         cycle  19966484.71
    Total SM Elapsed Cycles          cycle    479628768
    Average SMSP Active Cycles       cycle  19964937.32
    Total SMSP Elapsed Cycles        cycle   1918515072
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29415936
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.81
    Elapsed Cycles                cycle     31717350
    Memory Throughput                 %        93.53
    DRAM Throughput                   %         0.87
    Duration                         ms        17.48
    L1/TEX Cache Throughput           %        93.62
    L2 Cache Throughput               %        20.09
    SM Active Cycles              cycle  31683248.58
    Compute (SM) Throughput           %        93.53
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.86
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.36
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.18
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.67
    Mem Busy                              %        55.39
    Max Bandwidth                         %        93.53
    L1/TEX Hit Rate                       %        82.63
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.62
    Mem Pipes Busy                        %        93.53
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.34%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.61%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         2.51
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.469%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.83 active warps per scheduler, but only an average of 2.51 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.04
    Warp Cycles Per Executed Instruction           cycle        23.04
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.469%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.43
    Issued Instructions                             inst   1562071913
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.58
    Achieved Active Warps Per SM           warp        47.32
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       913836
    Total DRAM Elapsed Cycles        cycle    419063808
    Average L1 Active Cycles         cycle  31683248.58
    Total L1 Elapsed Cycles          cycle    761161576
    Average L2 Active Cycles         cycle  30553592.56
    Total L2 Elapsed Cycles          cycle    488933168
    Average SM Active Cycles         cycle  31683248.58
    Total SM Elapsed Cycles          cycle    761161576
    Average SMSP Active Cycles       cycle  31678009.89
    Total SMSP Elapsed Cycles        cycle   3044646304
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01769%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.78
    Elapsed Cycles                cycle     31738816
    Memory Throughput                 %        93.46
    DRAM Throughput                   %         0.86
    Duration                         ms        17.78
    L1/TEX Cache Throughput           %        93.63
    L2 Cache Throughput               %        19.93
    SM Active Cycles              cycle  31680902.12
    Compute (SM) Throughput           %        93.46
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.75
    Dropped Samples                sample            1
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.36
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.18
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.64
    Mem Busy                              %        55.38
    Max Bandwidth                         %        93.46
    L1/TEX Hit Rate                       %        82.91
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.25
    Mem Pipes Busy                        %        93.46
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.3%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.539%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.06
    Warp Cycles Per Executed Instruction           cycle        23.06
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.539%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 37.0% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.89
    Issued Instructions                             inst   1562071957
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.63
    Achieved Active Warps Per SM           warp        47.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       913852
    Total DRAM Elapsed Cycles        cycle    426388480
    Average L1 Active Cycles         cycle  31680902.12
    Total L1 Elapsed Cycles          cycle    761728928
    Average L2 Active Cycles         cycle     30536755
    Total L2 Elapsed Cycles          cycle    490747376
    Average SM Active Cycles         cycle  31680902.12
    Total SM Elapsed Cycles          cycle    761728928
    Average SMSP Active Cycles       cycle  31672969.22
    Total SMSP Elapsed Cycles        cycle   3046915712
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01762%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.72
    Elapsed Cycles                cycle     31736741
    Memory Throughput                 %        93.47
    DRAM Throughput                   %         0.83
    Duration                         ms        18.40
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        19.91
    SM Active Cycles              cycle  31676185.25
    Compute (SM) Throughput           %        93.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.73
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.37
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.59
    Mem Busy                              %        55.32
    Max Bandwidth                         %        93.47
    L1/TEX Hit Rate                       %        82.82
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.83
    Mem Pipes Busy                        %        93.47
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.31%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.53
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.526%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.53 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.06
    Warp Cycles Per Executed Instruction           cycle        23.06
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.526%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 37.0% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.71
    Issued Instructions                             inst   1562071940
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.73
    Achieved Active Warps Per SM           warp        47.39
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       911400
    Total DRAM Elapsed Cycles        cycle    441206784
    Average L1 Active Cycles         cycle  31676185.25
    Total L1 Elapsed Cycles          cycle    761624072
    Average L2 Active Cycles         cycle  30648809.62
    Total L2 Elapsed Cycles          cycle    492600960
    Average SM Active Cycles         cycle  31676185.25
    Total SM Elapsed Cycles          cycle    761624072
    Average SMSP Active Cycles       cycle  31670028.16
    Total SMSP Elapsed Cycles        cycle   3046496288
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01761%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.75
    Elapsed Cycles                cycle     31736699
    Memory Throughput                 %        93.47
    DRAM Throughput                   %         0.85
    Duration                         ms        18.08
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        19.90
    SM Active Cycles              cycle  31677629.29
    Compute (SM) Throughput           %        93.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample           13
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.37
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.63
    Mem Busy                              %        55.29
    Max Bandwidth                         %        93.47
    L1/TEX Hit Rate                       %        82.92
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.73
    Mem Pipes Busy                        %        93.47
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.31%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         2.51
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.533%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.83 active warps per scheduler, but only an average of 2.51 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.03
    Warp Cycles Per Executed Instruction           cycle        23.03
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.533%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.58
    Issued Instructions                             inst   1562071928
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.70
    Achieved Active Warps Per SM           warp        47.38
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       918432
    Total DRAM Elapsed Cycles        cycle    433649664
    Average L1 Active Cycles         cycle  31677629.29
    Total L1 Elapsed Cycles          cycle    761677160
    Average L2 Active Cycles         cycle  30566090.19
    Total L2 Elapsed Cycles          cycle    494173584
    Average SM Active Cycles         cycle  31677629.29
    Total SM Elapsed Cycles          cycle    761677160
    Average SMSP Active Cycles       cycle  31679024.76
    Total SMSP Elapsed Cycles        cycle   3046708640
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01751%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.77
    Elapsed Cycles                cycle     31739397
    Memory Throughput                 %        93.46
    DRAM Throughput                   %         0.85
    Duration                         ms        17.95
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        20.03
    SM Active Cycles              cycle  31677459.50
    Compute (SM) Throughput           %        93.46
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.30
    Dropped Samples                sample            1
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.37
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.64
    Mem Busy                              %        55.35
    Max Bandwidth                         %        93.46
    L1/TEX Hit Rate                       %        82.67
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.84
    Mem Pipes Busy                        %        93.46
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.3%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.84
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.539%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.84 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.05
    Warp Cycles Per Executed Instruction           cycle        23.05
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.539%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.65
    Issued Instructions                             inst   1562071934
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.64
    Achieved Active Warps Per SM           warp        47.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       920020
    Total DRAM Elapsed Cycles        cycle    430489600
    Average L1 Active Cycles         cycle  31677459.50
    Total L1 Elapsed Cycles          cycle    761730112
    Average L2 Active Cycles         cycle  30747208.31
    Total L2 Elapsed Cycles          cycle    494325120
    Average SM Active Cycles         cycle  31677459.50
    Total SM Elapsed Cycles          cycle    761730112
    Average SMSP Active Cycles       cycle  31677853.27
    Total SMSP Elapsed Cycles        cycle   3046920448
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01761%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.72
    Elapsed Cycles                cycle     31742193
    Memory Throughput                 %        93.45
    DRAM Throughput                   %         0.83
    Duration                         ms        18.40
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        19.62
    SM Active Cycles              cycle  31677338.04
    Compute (SM) Throughput           %        93.45
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.21
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.37
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.60
    Mem Busy                              %        55.33
    Max Bandwidth                         %        93.45
    L1/TEX Hit Rate                       %        82.40
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.38
    Mem Pipes Busy                        %        93.45
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.3%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.58%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.84
    Eligible Warps Per Scheduler        warp         2.53
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.549%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.84 active warps per scheduler, but only an average of 2.53 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.04
    Warp Cycles Per Executed Instruction           cycle        23.04
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.549%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.81
    Issued Instructions                             inst   1562071950
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.79
    Achieved Active Warps Per SM           warp        47.42
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       917568
    Total DRAM Elapsed Cycles        cycle    441268224
    Average L1 Active Cycles         cycle  31677338.04
    Total L1 Elapsed Cycles          cycle    761809256
    Average L2 Active Cycles         cycle  30779354.75
    Total L2 Elapsed Cycles          cycle    494613440
    Average SM Active Cycles         cycle  31677338.04
    Total SM Elapsed Cycles          cycle    761809256
    Average SMSP Active Cycles       cycle  31670215.76
    Total SMSP Elapsed Cycles        cycle   3047237024
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01762%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.77
    Elapsed Cycles                cycle     31742827
    Memory Throughput                 %        93.45
    DRAM Throughput                   %         0.85
    Duration                         ms        17.93
    L1/TEX Cache Throughput           %        93.64
    L2 Cache Throughput               %        19.88
    SM Active Cycles              cycle     31678208
    Compute (SM) Throughput           %        93.45
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.47
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.37
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.64
    Mem Busy                              %        55.31
    Max Bandwidth                         %        93.45
    L1/TEX Hit Rate                       %        82.69
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.49
    Mem Pipes Busy                        %        93.45
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.3%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.58%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.53
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.551%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.53 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.06
    Warp Cycles Per Executed Instruction           cycle        23.06
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.551%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.80
    Issued Instructions                             inst   1562071949
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.67
    Achieved Active Warps Per SM           warp        47.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       917448
    Total DRAM Elapsed Cycles        cycle    430055424
    Average L1 Active Cycles         cycle     31678208
    Total L1 Elapsed Cycles          cycle    761826048
    Average L2 Active Cycles         cycle  30537761.31
    Total L2 Elapsed Cycles          cycle    494966048
    Average SM Active Cycles         cycle     31678208
    Total SM Elapsed Cycles          cycle    761826048
    Average SMSP Active Cycles       cycle  31674077.03
    Total SMSP Elapsed Cycles        cycle   3047304192
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01747%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.68
    Elapsed Cycles                cycle     31730467
    Memory Throughput                 %        93.49
    DRAM Throughput                   %         0.81
    Duration                         ms        18.89
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        19.53
    SM Active Cycles              cycle  31673906.17
    Compute (SM) Throughput           %        93.49
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.48
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.37
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.19
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.55
    Mem Busy                              %        55.35
    Max Bandwidth                         %        93.49
    L1/TEX Hit Rate                       %        82.65
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.43
    Mem Pipes Busy                        %        93.49
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.32%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.84
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.515%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.84 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.05
    Warp Cycles Per Executed Instruction           cycle        23.05
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.515%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.54
    Issued Instructions                             inst   1562071924
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.68
    Achieved Active Warps Per SM           warp        47.37
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       917436
    Total DRAM Elapsed Cycles        cycle    452917248
    Average L1 Active Cycles         cycle  31673906.17
    Total L1 Elapsed Cycles          cycle    761529064
    Average L2 Active Cycles         cycle  30664623.81
    Total L2 Elapsed Cycles          cycle    503141008
    Average SM Active Cycles         cycle  31673906.17
    Total SM Elapsed Cycles          cycle    761529064
    Average SMSP Active Cycles       cycle  31677247.20
    Total SMSP Elapsed Cycles        cycle   3046116256
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01725%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.78
    Elapsed Cycles                cycle     31736304
    Memory Throughput                 %        93.47
    DRAM Throughput                   %         0.86
    Duration                         ms        17.78
    L1/TEX Cache Throughput           %        93.63
    L2 Cache Throughput               %        19.98
    SM Active Cycles              cycle  31679756.17
    Compute (SM) Throughput           %        93.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            6
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.36
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.18
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.65
    Mem Busy                              %        55.34
    Max Bandwidth                         %        93.47
    L1/TEX Hit Rate                       %        82.48
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.49
    Mem Pipes Busy                        %        93.47
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.31%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.59%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.38
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.62
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.532%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.06
    Warp Cycles Per Executed Instruction           cycle        23.06
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.532%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.59
    Issued Instructions                             inst   1562071929
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.69
    Achieved Active Warps Per SM           warp        47.37
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       918712
    Total DRAM Elapsed Cycles        cycle    426354688
    Average L1 Active Cycles         cycle  31679756.17
    Total L1 Elapsed Cycles          cycle    761668448
    Average L2 Active Cycles         cycle  30543772.12
    Total L2 Elapsed Cycles          cycle    490708688
    Average SM Active Cycles         cycle  31679756.17
    Total SM Elapsed Cycles          cycle    761668448
    Average SMSP Active Cycles       cycle  31668281.23
    Total SMSP Elapsed Cycles        cycle   3046673792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01762%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.72
    Elapsed Cycles                cycle     31720316
    Memory Throughput                 %        93.52
    DRAM Throughput                   %         0.83
    Duration                         ms        18.39
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        19.95
    SM Active Cycles              cycle  31675336.17
    Compute (SM) Throughput           %        93.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.26
    Dropped Samples                sample            9
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.37
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.20
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.59
    Mem Busy                              %        55.36
    Max Bandwidth                         %        93.52
    L1/TEX Hit Rate                       %        83.05
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       103.16
    Mem Pipes Busy                        %        93.52
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.33%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.6%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.84
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.484%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.84 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.06
    Warp Cycles Per Executed Instruction           cycle        23.06
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.484%                                                                                          
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 36.9% of the total average of 23.1 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16271491.83
    Executed Instructions                           inst   1562063216
    Avg. Issued Instructions Per Scheduler          inst  16271582.42
    Issued Instructions                             inst   1562071912
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.63
    Achieved Active Warps Per SM           warp        47.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       911552
    Total DRAM Elapsed Cycles        cycle    440963072
    Average L1 Active Cycles         cycle  31675336.17
    Total L1 Elapsed Cycles          cycle    761283816
    Average L2 Active Cycles         cycle  30543658.88
    Total L2 Elapsed Cycles          cycle    494272272
    Average SM Active Cycles         cycle  31675336.17
    Total SM Elapsed Cycles          cycle    761283816
    Average SMSP Active Cycles       cycle  31678162.33
    Total SMSP Elapsed Cycles        cycle   3045135264
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46310768
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01749%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.77
    Elapsed Cycles                cycle     47309326
    Memory Throughput                 %        93.53
    DRAM Throughput                   %         0.85
    Duration                         ms        26.73
    L1/TEX Cache Throughput           %        93.63
    L2 Cache Throughput               %        20.72
    SM Active Cycles              cycle  47261450.88
    Compute (SM) Throughput           %        93.53
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.29
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.34
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.62
    Mem Busy                              %        67.74
    Max Bandwidth                         %        93.53
    L1/TEX Hit Rate                       %        80.44
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       101.91
    Mem Pipes Busy                        %        93.53
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.89%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.79%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         2.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.468%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.82 active warps per scheduler, but only an average of 2.45 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.00
    Warp Cycles Per Executed Instruction           cycle        23.00
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.468%                                                                                          
          On average, each warp of this kernel spends 8.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.7% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.31
    Issued Instructions                             inst   2329218846
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.42
    Achieved Active Warps Per SM           warp        47.24
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1356080
    Total DRAM Elapsed Cycles        cycle    640950272
    Average L1 Active Cycles         cycle  47261450.88
    Total L1 Elapsed Cycles          cycle   1135422616
    Average L2 Active Cycles         cycle     46092827
    Total L2 Elapsed Cycles          cycle    737697936
    Average SM Active Cycles         cycle  47261450.88
    Total SM Elapsed Cycles          cycle   1135422616
    Average SMSP Active Cycles       cycle  47235376.08
    Total SMSP Elapsed Cycles        cycle   4541690464
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1849%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.78
    Elapsed Cycles                cycle     47300842
    Memory Throughput                 %        93.55
    DRAM Throughput                   %         0.85
    Duration                         ms        26.50
    L1/TEX Cache Throughput           %        93.61
    L2 Cache Throughput               %        20.69
    SM Active Cycles              cycle  47268640.67
    Compute (SM) Throughput           %        93.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.94
    Dropped Samples                sample            7
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.33
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.09
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.63
    Mem Busy                              %        67.78
    Max Bandwidth                         %        93.55
    L1/TEX Hit Rate                       %        81.27
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.69
    Mem Pipes Busy                        %        93.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.9%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.33
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.67
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         2.42
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.451%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.82 active warps per scheduler, but only an average of 2.42 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.03
    Warp Cycles Per Executed Instruction           cycle        23.03
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.451%                                                                                          
          On average, each warp of this kernel spends 8.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.0% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.10
    Issued Instructions                             inst   2329218826
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.49
    Achieved Active Warps Per SM           warp        47.27
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1352280
    Total DRAM Elapsed Cycles        cycle    635451392
    Average L1 Active Cycles         cycle  47268640.67
    Total L1 Elapsed Cycles          cycle   1135217920
    Average L2 Active Cycles         cycle  45527816.50
    Total L2 Elapsed Cycles          cycle    731367520
    Average SM Active Cycles         cycle  47268640.67
    Total SM Elapsed Cycles          cycle   1135217920
    Average SMSP Active Cycles       cycle  47263685.95
    Total SMSP Elapsed Cycles        cycle   4540871680
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1842%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     47306598
    Memory Throughput                 %        93.54
    DRAM Throughput                   %         0.80
    Duration                         ms        28.41
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        21.01
    SM Active Cycles              cycle     47251548
    Compute (SM) Throughput           %        93.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.99
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.11
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.53
    Mem Busy                              %        67.73
    Max Bandwidth                         %        93.54
    L1/TEX Hit Rate                       %        80.88
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.05
    Mem Pipes Busy                        %        93.54
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.89%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.35
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.65
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         2.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.463%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.82 active warps per scheduler, but only an average of 2.45 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.03
    Warp Cycles Per Executed Instruction           cycle        23.03
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.463%                                                                                          
          On average, each warp of this kernel spends 8.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.4% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.18
    Issued Instructions                             inst   2329218833
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.44
    Achieved Active Warps Per SM           warp        47.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1355324
    Total DRAM Elapsed Cycles        cycle    681332736
    Average L1 Active Cycles         cycle     47251548
    Total L1 Elapsed Cycles          cycle   1135354976
    Average L2 Active Cycles         cycle  46124306.69
    Total L2 Elapsed Cycles          cycle    742563600
    Average SM Active Cycles         cycle     47251548
    Total SM Elapsed Cycles          cycle   1135354976
    Average SMSP Active Cycles       cycle  47251997.30
    Total SMSP Elapsed Cycles        cycle   4541419904
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1838%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     47276469
    Memory Throughput                 %        93.60
    DRAM Throughput                   %         0.79
    Duration                         ms        28.39
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        21.38
    SM Active Cycles              cycle  47249128.04
    Compute (SM) Throughput           %        93.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.93
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.12
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.52
    Mem Busy                              %        67.81
    Max Bandwidth                         %        93.60
    L1/TEX Hit Rate                       %        81.21
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.93
    Mem Pipes Busy                        %        93.60
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.92%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.81%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.37
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.63
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.403%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.45 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.00
    Warp Cycles Per Executed Instruction           cycle        23.00
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.403%                                                                                          
          On average, each warp of this kernel spends 8.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.4% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.18
    Issued Instructions                             inst   2329218833
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.45
    Achieved Active Warps Per SM           warp        47.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1350196
    Total DRAM Elapsed Cycles        cycle    680898560
    Average L1 Active Cycles         cycle  47249128.04
    Total L1 Elapsed Cycles          cycle   1134631040
    Average L2 Active Cycles         cycle  46233318.81
    Total L2 Elapsed Cycles          cycle    742020368
    Average SM Active Cycles         cycle  47249128.04
    Total SM Elapsed Cycles          cycle   1134631040
    Average SMSP Active Cycles       cycle  47235160.22
    Total SMSP Elapsed Cycles        cycle   4538524160
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1843%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.65
    Elapsed Cycles                cycle     47286257
    Memory Throughput                 %        93.58
    DRAM Throughput                   %         0.80
    Duration                         ms        28.66
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        21.00
    SM Active Cycles              cycle  47247757.83
    Compute (SM) Throughput           %        93.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.21
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.12
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.53
    Mem Busy                              %        67.79
    Max Bandwidth                         %        93.58
    L1/TEX Hit Rate                       %        81.09
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.11
    Mem Pipes Busy                        %        93.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.46
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.423%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.46 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.99
    Warp Cycles Per Executed Instruction           cycle        22.99
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.423%                                                                                          
          On average, each warp of this kernel spends 8.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.6% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.18
    Issued Instructions                             inst   2329218833
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.47
    Achieved Active Warps Per SM           warp        47.26
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1370192
    Total DRAM Elapsed Cycles        cycle    687228928
    Average L1 Active Cycles         cycle  47247757.83
    Total L1 Elapsed Cycles          cycle   1134869072
    Average L2 Active Cycles         cycle  46248994.69
    Total L2 Elapsed Cycles          cycle    742823328
    Average SM Active Cycles         cycle  47247757.83
    Total SM Elapsed Cycles          cycle   1134869072
    Average SMSP Active Cycles       cycle  47244285.89
    Total SMSP Elapsed Cycles        cycle   4539476288
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1842%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.63
    Elapsed Cycles                cycle     47286216
    Memory Throughput                 %        93.58
    DRAM Throughput                   %         0.78
    Duration                         ms        28.92
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        20.92
    SM Active Cycles              cycle     47250115
    Compute (SM) Throughput           %        93.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.80
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.12
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.50
    Mem Busy                              %        67.78
    Max Bandwidth                         %        93.58
    L1/TEX Hit Rate                       %        80.68
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.79
    Mem Pipes Busy                        %        93.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.46
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.423%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.46 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.00
    Warp Cycles Per Executed Instruction           cycle        23.00
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.423%                                                                                          
          On average, each warp of this kernel spends 8.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.7% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.18
    Issued Instructions                             inst   2329218833
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.33
    Achieved Active Warps Per SM           warp        47.20
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1355436
    Total DRAM Elapsed Cycles        cycle    693532672
    Average L1 Active Cycles         cycle     47250115
    Total L1 Elapsed Cycles          cycle   1134868464
    Average L2 Active Cycles         cycle  46663903.81
    Total L2 Elapsed Cycles          cycle    749636928
    Average SM Active Cycles         cycle     47250115
    Total SM Elapsed Cycles          cycle   1134868464
    Average SMSP Active Cycles       cycle  47240910.08
    Total SMSP Elapsed Cycles        cycle   4539473856
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1842%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     47288494
    Memory Throughput                 %        93.57
    DRAM Throughput                   %         0.79
    Duration                         ms        28.40
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        21.16
    SM Active Cycles              cycle  47251803.71
    Compute (SM) Throughput           %        93.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.34
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.11
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.51
    Mem Busy                              %        67.77
    Max Bandwidth                         %        93.57
    L1/TEX Hit Rate                       %        80.90
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.84
    Mem Pipes Busy                        %        93.57
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.35
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.65
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.427%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.45 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.99
    Warp Cycles Per Executed Instruction           cycle        22.99
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.427%                                                                                          
          On average, each warp of this kernel spends 8.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.6% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.29
    Issued Instructions                             inst   2329218844
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.39
    Achieved Active Warps Per SM           warp        47.23
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1342616
    Total DRAM Elapsed Cycles        cycle    681068544
    Average L1 Active Cycles         cycle  47251803.71
    Total L1 Elapsed Cycles          cycle   1134923096
    Average L2 Active Cycles         cycle  46257952.81
    Total L2 Elapsed Cycles          cycle    742981056
    Average SM Active Cycles         cycle  47251803.71
    Total SM Elapsed Cycles          cycle   1134923096
    Average SMSP Active Cycles       cycle  47252295.10
    Total SMSP Elapsed Cycles        cycle   4539692384
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1842%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.62
    Elapsed Cycles                cycle     47284403
    Memory Throughput                 %        93.58
    DRAM Throughput                   %         0.77
    Duration                         ms        29.15
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        21.10
    SM Active Cycles              cycle  47248447.79
    Compute (SM) Throughput           %        93.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.01
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.12
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.49
    Mem Busy                              %        67.79
    Max Bandwidth                         %        93.58
    L1/TEX Hit Rate                       %        80.52
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.68
    Mem Pipes Busy                        %        93.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.36
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.64
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.44
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.418%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.44 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.98
    Warp Cycles Per Executed Instruction           cycle        22.98
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.418%                                                                                          
          On average, each warp of this kernel spends 8.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.4% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.18
    Issued Instructions                             inst   2329218833
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.51
    Achieved Active Warps Per SM           warp        47.28
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1354336
    Total DRAM Elapsed Cycles        cycle    699062272
    Average L1 Active Cycles         cycle  47248447.79
    Total L1 Elapsed Cycles          cycle   1134813864
    Average L2 Active Cycles         cycle  46162783.75
    Total L2 Elapsed Cycles          cycle    749531568
    Average SM Active Cycles         cycle  47248447.79
    Total SM Elapsed Cycles          cycle   1134813864
    Average SMSP Active Cycles       cycle  47244859.95
    Total SMSP Elapsed Cycles        cycle   4539255456
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1822%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.53
    Elapsed Cycles                cycle     47281358
    Memory Throughput                 %        93.59
    DRAM Throughput                   %         0.73
    Duration                         ms        30.90
    L1/TEX Cache Throughput           %        93.68
    L2 Cache Throughput               %        21.46
    SM Active Cycles              cycle  47236171.62
    Compute (SM) Throughput           %        93.59
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.34
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.36
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.41
    Mem Busy                              %        67.82
    Max Bandwidth                         %        93.59
    L1/TEX Hit Rate                       %        80.84
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.90
    Mem Pipes Busy                        %        93.59
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.92%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.81%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.35
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.65
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.412%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.45 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.97
    Warp Cycles Per Executed Instruction           cycle        22.97
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.412%                                                                                          
          On average, each warp of this kernel spends 8.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.7% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.26
    Issued Instructions                             inst   2329218841
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.44
    Achieved Active Warps Per SM           warp        47.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1357056
    Total DRAM Elapsed Cycles        cycle    741059584
    Average L1 Active Cycles         cycle  47236171.62
    Total L1 Elapsed Cycles          cycle   1134744056
    Average L2 Active Cycles         cycle  46488497.31
    Total L2 Elapsed Cycles          cycle    755321920
    Average SM Active Cycles         cycle  47236171.62
    Total SM Elapsed Cycles          cycle   1134744056
    Average SMSP Active Cycles       cycle  47248029.73
    Total SMSP Elapsed Cycles        cycle   4538976224
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1821%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     47298804
    Memory Throughput                 %        93.55
    DRAM Throughput                   %         0.80
    Duration                         ms        28.41
    L1/TEX Cache Throughput           %        93.65
    L2 Cache Throughput               %        20.97
    SM Active Cycles              cycle  47248241.54
    Compute (SM) Throughput           %        93.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.80
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.05
    Executed Ipc Elapsed  inst/cycle         2.05
    Issue Slots Busy               %        51.35
    Issued Ipc Active     inst/cycle         2.05
    SM Busy                        %        59.12
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.53
    Mem Busy                              %        67.76
    Max Bandwidth                         %        93.55
    L1/TEX Hit Rate                       %        81.23
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.02
    Mem Pipes Busy                        %        93.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 46.9%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.8%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.35
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        48.65
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.45
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.447%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.45 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        23.01
    Warp Cycles Per Executed Instruction           cycle        23.01
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.447%                                                                                          
          On average, each warp of this kernel spends 8.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 35.3% of the total average of 23.0 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24262605.33
    Executed Instructions                           inst   2329210112
    Avg. Issued Instructions Per Scheduler          inst  24262696.32
    Issued Instructions                             inst   2329218847
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.50
    Achieved Active Warps Per SM           warp        47.28
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1359108
    Total DRAM Elapsed Cycles        cycle    681222144
    Average L1 Active Cycles         cycle  47248241.54
    Total L1 Elapsed Cycles          cycle   1135168816
    Average L2 Active Cycles         cycle     46190495
    Total L2 Elapsed Cycles          cycle    742326976
    Average SM Active Cycles         cycle  47248241.54
    Total SM Elapsed Cycles          cycle   1135168816
    Average SMSP Active Cycles       cycle  47252572.14
    Total SMSP Elapsed Cycles        cycle   4540675264
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     68810496
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1841%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

