==PROF== Connected to process 15345 (/usr/bin/python3.12)
==LOG== Target process 15401 terminated before first instrumented API call.
==LOG== Target process 15421 terminated before first instrumented API call.
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "tiledMatrixMulKernel": 0%....50%....100% - 39 passes
running l2 cache tests for ../manually_optimized_gemm.cubin
==PROF== Disconnected from process 15345
[15345] python3.12@127.0.0.1
  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.13
    Elapsed Cycles                cycle      1939104
    Memory Throughput                 %        85.73
    DRAM Throughput                   %         1.20
    Duration                         us       910.69
    L1/TEX Cache Throughput           %        88.81
    L2 Cache Throughput               %         3.13
    SM Active Cycles              cycle   1871840.17
    Compute (SM) Throughput           %        85.73
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.43
    Executed Ipc Elapsed  inst/cycle         2.35
    Issue Slots Busy               %        61.01
    Issued Ipc Active     inst/cycle         2.44
    SM Busy                        %        61.01
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.31
    Mem Busy                              %        46.77
    Max Bandwidth                         %        85.73
    L1/TEX Hit Rate                       %        19.08
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.85
    Mem Pipes Busy                        %        85.73
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.67
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        39.33
    Active Warps Per Scheduler          warp        11.57
    Eligible Warps Per Scheduler        warp         1.08
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.07
    Warp Cycles Per Executed Instruction           cycle        19.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.06%                                                                                    
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 44.1% of the       
          total average of 19.1 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141950.75
    Issued Instructions                             inst    109627272
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.04
    Achieved Active Warps Per SM           warp        46.58
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65692
    Total DRAM Elapsed Cycles        cycle     21837824
    Average L1 Active Cycles         cycle   1871840.17
    Total L1 Elapsed Cycles          cycle     46536088
    Average L2 Active Cycles         cycle    863890.19
    Total L2 Elapsed Cycles          cycle     28162512
    Average SM Active Cycles         cycle   1871840.17
    Total SM Elapsed Cycles          cycle     46536088
    Average SMSP Active Cycles       cycle   1882340.10
    Total SMSP Elapsed Cycles        cycle    186144352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.412%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.03% above the average, while the minimum instance value is 1.35% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.13
    Elapsed Cycles                cycle      1942127
    Memory Throughput                 %        85.60
    DRAM Throughput                   %         1.20
    Duration                         us       912.10
    L1/TEX Cache Throughput           %        88.39
    L2 Cache Throughput               %         3.09
    SM Active Cycles              cycle   1880560.58
    Compute (SM) Throughput           %        85.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.42
    Executed Ipc Elapsed  inst/cycle         2.34
    Issue Slots Busy               %        60.72
    Issued Ipc Active     inst/cycle         2.43
    SM Busy                        %        60.72
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.31
    Mem Busy                              %        46.71
    Max Bandwidth                         %        85.60
    L1/TEX Hit Rate                       %        22.33
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.93
    Mem Pipes Busy                        %        85.60
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.86
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        39.14
    Active Warps Per Scheduler          warp        11.60
    Eligible Warps Per Scheduler        warp         1.08
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.06
    Warp Cycles Per Executed Instruction           cycle        19.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.16%                                                                                    
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 44.2% of the       
          total average of 19.1 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141938.16
    Issued Instructions                             inst    109626063
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.49
    Achieved Active Warps Per SM           warp        46.32
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65772
    Total DRAM Elapsed Cycles        cycle     21870592
    Average L1 Active Cycles         cycle   1880560.58
    Total L1 Elapsed Cycles          cycle     46607784
    Average L2 Active Cycles         cycle    882920.56
    Total L2 Elapsed Cycles          cycle     28191104
    Average SM Active Cycles         cycle   1880560.58
    Total SM Elapsed Cycles          cycle     46607784
    Average SMSP Active Cycles       cycle   1876351.14
    Total SMSP Elapsed Cycles        cycle    186431136
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.174%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.33% above the average, while the minimum instance value is 1.72% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.13
    Elapsed Cycles                cycle      1935840
    Memory Throughput                 %        85.87
    DRAM Throughput                   %         1.21
    Duration                         us       909.09
    L1/TEX Cache Throughput           %        88.11
    L2 Cache Throughput               %         3.05
    SM Active Cycles              cycle   1886566.67
    Compute (SM) Throughput           %        85.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.08
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.41
    Executed Ipc Elapsed  inst/cycle         2.35
    Issue Slots Busy               %        60.53
    Issued Ipc Active     inst/cycle         2.42
    SM Busy                        %        60.53
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.31
    Mem Busy                              %        46.85
    Max Bandwidth                         %        85.87
    L1/TEX Hit Rate                       %        19.50
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.20
    Mem Pipes Busy                        %        85.87
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.98
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        39.02
    Active Warps Per Scheduler          warp        11.63
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.06
    Warp Cycles Per Executed Instruction           cycle        19.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.18%                                                                                    
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 44.2% of the       
          total average of 19.1 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141943.67
    Issued Instructions                             inst    109626592
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.21
    Achieved Active Warps Per SM           warp        46.18
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65692
    Total DRAM Elapsed Cycles        cycle     21798912
    Average L1 Active Cycles         cycle   1886566.67
    Total L1 Elapsed Cycles          cycle     46458320
    Average L2 Active Cycles         cycle    889160.94
    Total L2 Elapsed Cycles          cycle     28269168
    Average SM Active Cycles         cycle   1886566.67
    Total SM Elapsed Cycles          cycle     46458320
    Average SMSP Active Cycles       cycle   1872663.31
    Total SMSP Elapsed Cycles        cycle    185833280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.283%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.50% above the average, while the minimum instance value is 1.69% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.13
    Elapsed Cycles                cycle      1937243
    Memory Throughput                 %        85.81
    DRAM Throughput                   %         1.21
    Duration                         us       909.60
    L1/TEX Cache Throughput           %        88.59
    L2 Cache Throughput               %         3.10
    SM Active Cycles              cycle   1876410.79
    Compute (SM) Throughput           %        85.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.08
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.42
    Executed Ipc Elapsed  inst/cycle         2.35
    Issue Slots Busy               %        60.86
    Issued Ipc Active     inst/cycle         2.43
    SM Busy                        %        60.86
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.32
    Mem Busy                              %        46.84
    Max Bandwidth                         %        85.81
    L1/TEX Hit Rate                       %        20.79
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.05
    Mem Pipes Busy                        %        85.81
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.63
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        39.37
    Active Warps Per Scheduler          warp        11.55
    Eligible Warps Per Scheduler        warp         1.08
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.05
    Warp Cycles Per Executed Instruction           cycle        19.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.74%                                                                                    
          On average, each warp of this kernel spends 8.5 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 44.7% of the       
          total average of 19.1 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141926.29
    Issued Instructions                             inst    109624924
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66012
    Total DRAM Elapsed Cycles        cycle     21811200
    Average L1 Active Cycles         cycle   1876410.79
    Total L1 Elapsed Cycles          cycle     46493384
    Average L2 Active Cycles         cycle    889514.31
    Total L2 Elapsed Cycles          cycle     28367872
    Average SM Active Cycles         cycle   1876410.79
    Total SM Elapsed Cycles          cycle     46493384
    Average SMSP Active Cycles       cycle   1883286.81
    Total SMSP Elapsed Cycles        cycle    185973536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.65%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.26% above the average, while the minimum instance value is 2.34% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.13
    Elapsed Cycles                cycle      1941005
    Memory Throughput                 %        85.64
    DRAM Throughput                   %         1.20
    Duration                         us       911.36
    L1/TEX Cache Throughput           %        88.61
    L2 Cache Throughput               %         3.10
    SM Active Cycles              cycle   1875942.17
    Compute (SM) Throughput           %        85.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.08
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.42
    Executed Ipc Elapsed  inst/cycle         2.34
    Issue Slots Busy               %        60.87
    Issued Ipc Active     inst/cycle         2.43
    SM Busy                        %        60.87
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.31
    Mem Busy                              %        46.76
    Max Bandwidth                         %        85.64
    L1/TEX Hit Rate                       %        20.36
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.55
    Mem Pipes Busy                        %        85.64
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        60.89
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        39.11
    Active Warps Per Scheduler          warp        11.60
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.06
    Warp Cycles Per Executed Instruction           cycle        19.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.3%                                                                                     
          On average, each warp of this kernel spends 8.4 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 44.3% of the       
          total average of 19.1 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141923.81
    Issued Instructions                             inst    109624686
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.75
    Achieved Active Warps Per SM           warp        46.44
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65820
    Total DRAM Elapsed Cycles        cycle     21852160
    Average L1 Active Cycles         cycle   1875942.17
    Total L1 Elapsed Cycles          cycle     46583360
    Average L2 Active Cycles         cycle    898700.50
    Total L2 Elapsed Cycles          cycle     28424288
    Average SM Active Cycles         cycle   1875942.17
    Total SM Elapsed Cycles          cycle     46583360
    Average SMSP Active Cycles       cycle   1875419.67
    Total SMSP Elapsed Cycles        cycle    186333440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.134%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.15% above the average, while the minimum instance value is 1.57% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       434.99
    Elapsed Cycles                cycle      1910287
    Memory Throughput                 %        87.02
    DRAM Throughput                   %         0.25
    Duration                         ms         4.39
    L1/TEX Cache Throughput           %        90.04
    L2 Cache Throughput               %         1.55
    SM Active Cycles              cycle   1846075.04
    Compute (SM) Throughput           %        87.02
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.24
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.46
    Executed Ipc Elapsed  inst/cycle         2.38
    Issue Slots Busy               %        61.85
    Issued Ipc Active     inst/cycle         2.47
    SM Busy                        %        61.85
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       479.61
    Mem Busy                              %        47.64
    Max Bandwidth                         %        87.02
    L1/TEX Hit Rate                       %        17.64
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.62
    Mem Pipes Busy                        %        87.02
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.90
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        38.10
    Active Warps Per Scheduler          warp        11.64
    Eligible Warps Per Scheduler        warp         1.08
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.81
    Warp Cycles Per Executed Instruction           cycle        18.89
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.93%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.9% of the       
          total average of 18.8 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141732.28
    Issued Instructions                             inst    109606299
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.96
    Achieved Active Warps Per SM           warp        46.54
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65820
    Total DRAM Elapsed Cycles        cycle    105308160
    Average L1 Active Cycles         cycle   1846075.04
    Total L1 Elapsed Cycles          cycle     45846848
    Average L2 Active Cycles         cycle   1362795.75
    Total L2 Elapsed Cycles          cycle     57967248
    Average SM Active Cycles         cycle   1846075.04
    Total SM Elapsed Cycles          cycle     45846848
    Average SMSP Active Cycles       cycle   1844396.94
    Total SMSP Elapsed Cycles        cycle    183387392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.197%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 16.47% above the average, while the minimum instance value is 1.99% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       773.95
    Elapsed Cycles                cycle      1940925
    Memory Throughput                 %        86.34
    DRAM Throughput                   %         0.44
    Duration                         ms         2.49
    L1/TEX Cache Throughput           %        90.07
    L2 Cache Throughput               %         2.70
    SM Active Cycles              cycle   1845533.17
    Compute (SM) Throughput           %        86.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.46
    Executed Ipc Elapsed  inst/cycle         2.36
    Issue Slots Busy               %        61.87
    Issued Ipc Active     inst/cycle         2.47
    SM Busy                        %        61.87
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       848.42
    Mem Busy                              %        47.24
    Max Bandwidth                         %        86.34
    L1/TEX Hit Rate                       %        19.68
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.09
    Mem Pipes Busy                        %        86.34
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.87
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        38.13
    Active Warps Per Scheduler          warp        11.66
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.85
    Warp Cycles Per Executed Instruction           cycle        18.93
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.87%                                                                                    
          On average, each warp of this kernel spends 8.8 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 18.9 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141808.44
    Issued Instructions                             inst    109613610
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.26
    Achieved Active Warps Per SM           warp        46.68
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65952
    Total DRAM Elapsed Cycles        cycle     59648000
    Average L1 Active Cycles         cycle   1845533.17
    Total L1 Elapsed Cycles          cycle     46204984
    Average L2 Active Cycles         cycle   1039961.38
    Total L2 Elapsed Cycles          cycle     32833648
    Average SM Active Cycles         cycle   1845533.17
    Total SM Elapsed Cycles          cycle     46204984
    Average SMSP Active Cycles       cycle   1845628.48
    Total SMSP Elapsed Cycles        cycle    184819936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.115%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.09% above the average, while the minimum instance value is 1.20% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       989.95
    Elapsed Cycles                cycle      1921616
    Memory Throughput                 %        86.51
    DRAM Throughput                   %         0.56
    Duration                         ms         1.94
    L1/TEX Cache Throughput           %        89.40
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle   1859289.96
    Compute (SM) Throughput           %        86.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.67
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.45
    Executed Ipc Elapsed  inst/cycle         2.37
    Issue Slots Busy               %        61.41
    Issued Ipc Active     inst/cycle         2.46
    SM Busy                        %        61.41
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.08
    Mem Busy                              %        47.29
    Max Bandwidth                         %        86.51
    L1/TEX Hit Rate                       %        19.63
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.45
    Mem Pipes Busy                        %        86.51
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.41
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        38.59
    Active Warps Per Scheduler          warp        11.60
    Eligible Warps Per Scheduler        warp         1.08
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.89
    Warp Cycles Per Executed Instruction           cycle        18.97
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.12%                                                                                    
          On average, each warp of this kernel spends 8.7 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.1% of the       
          total average of 18.9 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141840.96
    Issued Instructions                             inst    109616732
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.63
    Achieved Active Warps Per SM           warp        46.38
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65692
    Total DRAM Elapsed Cycles        cycle     46544896
    Average L1 Active Cycles         cycle   1859289.96
    Total L1 Elapsed Cycles          cycle     46118760
    Average L2 Active Cycles         cycle    943470.75
    Total L2 Elapsed Cycles          cycle     30280080
    Average SM Active Cycles         cycle   1859289.96
    Total SM Elapsed Cycles          cycle     46118760
    Average SMSP Active Cycles       cycle   1859267.15
    Total SMSP Elapsed Cycles        cycle    184475040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.301%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.63% above the average, while the minimum instance value is 1.19% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       989.95
    Elapsed Cycles                cycle      1922402
    Memory Throughput                 %        86.47
    DRAM Throughput                   %         0.57
    Duration                         ms         1.94
    L1/TEX Cache Throughput           %        89.08
    L2 Cache Throughput               %         2.90
    SM Active Cycles              cycle   1866090.38
    Compute (SM) Throughput           %        86.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.67
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.44
    Executed Ipc Elapsed  inst/cycle         2.37
    Issue Slots Busy               %        61.19
    Issued Ipc Active     inst/cycle         2.45
    SM Busy                        %        61.19
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.08
    Mem Busy                              %        47.29
    Max Bandwidth                         %        86.47
    L1/TEX Hit Rate                       %        20.27
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.53
    Mem Pipes Busy                        %        86.47
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.38
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        38.62
    Active Warps Per Scheduler          warp        11.59
    Eligible Warps Per Scheduler        warp         1.08
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.88
    Warp Cycles Per Executed Instruction           cycle        18.96
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.3%                                                                                     
          On average, each warp of this kernel spends 8.7 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.3% of the       
          total average of 18.9 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141845.92
    Issued Instructions                             inst    109617208
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.30
    Achieved Active Warps Per SM           warp        46.23
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65820
    Total DRAM Elapsed Cycles        cycle     46565376
    Average L1 Active Cycles         cycle   1866090.38
    Total L1 Elapsed Cycles          cycle     46137632
    Average L2 Active Cycles         cycle    949234.69
    Total L2 Elapsed Cycles          cycle     30292288
    Average SM Active Cycles         cycle   1866090.38
    Total SM Elapsed Cycles          cycle     46137632
    Average SMSP Active Cycles       cycle   1860212.89
    Total SMSP Elapsed Cycles        cycle    184550528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.123%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.22% above the average, while the minimum instance value is 1.41% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.35
    Elapsed Cycles                cycle      1924636
    Memory Throughput                 %        86.37
    DRAM Throughput                   %         0.77
    Duration                         ms         1.43
    L1/TEX Cache Throughput           %        88.78
    L2 Cache Throughput               %         2.85
    SM Active Cycles              cycle   1872415.54
    Compute (SM) Throughput           %        86.37
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.43
    Executed Ipc Elapsed  inst/cycle         2.36
    Issue Slots Busy               %        60.98
    Issued Ipc Active     inst/cycle         2.44
    SM Busy                        %        60.98
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (33.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.47
    Mem Busy                              %        47.21
    Max Bandwidth                         %        86.37
    L1/TEX Hit Rate                       %        17.84
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        94.67
    Mem Pipes Busy                        %        86.37
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        61.20
    Issued Warp Per Scheduler                        0.61
    No Eligible                            %        38.80
    Active Warps Per Scheduler          warp        11.58
    Eligible Warps Per Scheduler        warp         1.08
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.93
    Warp Cycles Per Executed Instruction           cycle        19.01
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.61
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.83%                                                                                    
          On average, each warp of this kernel spends 8.7 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 45.8% of the       
          total average of 18.9 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1137066.67
    Executed Instructions                           inst    109158400
    Avg. Issued Instructions Per Scheduler          inst   1141859.33
    Issued Instructions                             inst    109618496
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.21
    Achieved Active Warps Per SM           warp        46.18
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65692
    Total DRAM Elapsed Cycles        cycle     34187264
    Average L1 Active Cycles         cycle   1872415.54
    Total L1 Elapsed Cycles          cycle     46190888
    Average L2 Active Cycles         cycle    937037.19
    Total L2 Elapsed Cycles          cycle     30791008
    Average SM Active Cycles         cycle   1872415.54
    Total SM Elapsed Cycles          cycle     46190888
    Average SMSP Active Cycles       cycle      1865723
    Total SMSP Elapsed Cycles        cycle    184763552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.539%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.38% above the average, while the minimum instance value is 1.72% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     28672000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle      6326018
    Memory Throughput                 %        88.78
    DRAM Throughput                   %         0.54
    Duration                         ms         4.53
    L1/TEX Cache Throughput           %        90.49
    L2 Cache Throughput               %         3.73
    SM Active Cycles              cycle   6206650.83
    Compute (SM) Throughput           %        88.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.89
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.47
    Executed Ipc Elapsed  inst/cycle         2.43
    Issue Slots Busy               %        62.11
    Issued Ipc Active     inst/cycle         2.48
    SM Busy                        %        62.11
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.04
    Mem Busy                              %        48.95
    Max Bandwidth                         %        88.78
    L1/TEX Hit Rate                       %        29.87
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.48
    Mem Pipes Busy                        %        88.78
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.24%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.24%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.15
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.85
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.06
    Warp Cycles Per Executed Instruction           cycle        19.14
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.74%                                                                                    
          On average, each warp of this kernel spends 8.7 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 45.7% of the       
          total average of 19.1 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854945.39
    Issued Instructions                             inst    370074757
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.56
    Achieved Active Warps Per SM           warp        47.31
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147292
    Total DRAM Elapsed Cycles        cycle    108743680
    Average L1 Active Cycles         cycle   6206650.83
    Total L1 Elapsed Cycles          cycle    151823816
    Average L2 Active Cycles         cycle   3344515.44
    Total L2 Elapsed Cycles          cycle    103184848
    Average SM Active Cycles         cycle   6206650.83
    Total SM Elapsed Cycles          cycle    151823816
    Average SMSP Active Cycles       cycle   6203128.04
    Total SMSP Elapsed Cycles        cycle    607295264
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.275%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.17% above the average, while the minimum instance value is 1.58% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.06%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       839.99
    Elapsed Cycles                cycle      6286987
    Memory Throughput                 %        89.33
    DRAM Throughput                   %         0.33
    Duration                         ms         7.48
    L1/TEX Cache Throughput           %        90.73
    L2 Cache Throughput               %         3.91
    SM Active Cycles              cycle   6190126.83
    Compute (SM) Throughput           %        89.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.06
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.48
    Executed Ipc Elapsed  inst/cycle         2.44
    Issue Slots Busy               %        62.27
    Issued Ipc Active     inst/cycle         2.49
    SM Busy                        %        62.27
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       627.84
    Mem Busy                              %        49.28
    Max Bandwidth                         %        89.33
    L1/TEX Hit Rate                       %        27.46
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.07
    Mem Pipes Busy                        %        89.33
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.39%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.39%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.26
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.74
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        18.99
    Warp Cycles Per Executed Instruction           cycle        19.07
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.49%                                                                                    
          On average, each warp of this kernel spends 8.8 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.5% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854799.49
    Issued Instructions                             inst    370060751
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.55
    Achieved Active Warps Per SM           warp        47.31
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146848
    Total DRAM Elapsed Cycles        cycle    179478528
    Average L1 Active Cycles         cycle   6190126.83
    Total L1 Elapsed Cycles          cycle    150887656
    Average L2 Active Cycles         cycle   3604291.50
    Total L2 Elapsed Cycles          cycle    100591808
    Average SM Active Cycles         cycle   6190126.83
    Total SM Elapsed Cycles          cycle    150887656
    Average SMSP Active Cycles       cycle   6190983.24
    Total SMSP Elapsed Cycles        cycle    603550624
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 15.55%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.03
    Elapsed Cycles                cycle      6301656
    Memory Throughput                 %        89.12
    DRAM Throughput                   %         0.40
    Duration                         ms         6.09
    L1/TEX Cache Throughput           %        90.65
    L2 Cache Throughput               %         3.91
    SM Active Cycles              cycle   6195701.29
    Compute (SM) Throughput           %        89.12
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.57
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.48
    Executed Ipc Elapsed  inst/cycle         2.44
    Issue Slots Busy               %        62.22
    Issued Ipc Active     inst/cycle         2.49
    SM Busy                        %        62.22
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       772.10
    Mem Busy                              %        49.16
    Max Bandwidth                         %        89.12
    L1/TEX Hit Rate                       %        28.09
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.05
    Mem Pipes Busy                        %        89.12
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.34%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.34%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.18
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.82
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.06%                                                                                    
          On average, each warp of this kernel spends 8.8 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.1% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854879.76
    Issued Instructions                             inst    370068457
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.56
    Achieved Active Warps Per SM           warp        47.31
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146908
    Total DRAM Elapsed Cycles        cycle    146003968
    Average L1 Active Cycles         cycle   6195701.29
    Total L1 Elapsed Cycles          cycle    151239728
    Average L2 Active Cycles         cycle   3530637.38
    Total L2 Elapsed Cycles          cycle     99365248
    Average SM Active Cycles         cycle   6195701.29
    Total SM Elapsed Cycles          cycle    151239728
    Average SMSP Active Cycles       cycle   6199791.58
    Total SMSP Elapsed Cycles        cycle    604958912
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 15.42%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle      6307765
    Memory Throughput                 %        89.04
    DRAM Throughput                   %         0.45
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        90.50
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   6205587.79
    Compute (SM) Throughput           %        89.04
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.98
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.47
    Executed Ipc Elapsed  inst/cycle         2.43
    Issue Slots Busy               %        62.12
    Issued Ipc Active     inst/cycle         2.48
    SM Busy                        %        62.12
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       861.23
    Mem Busy                              %        49.09
    Max Bandwidth                         %        89.04
    L1/TEX Hit Rate                       %        28.97
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.14
    Mem Pipes Busy                        %        89.04
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.31%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.31%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.11
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.89
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46%                                                                                       
          On average, each warp of this kernel spends 8.8 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854902.54
    Issued Instructions                             inst    370070644
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.48
    Achieved Active Warps Per SM           warp        47.27
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146984
    Total DRAM Elapsed Cycles        cycle    130961408
    Average L1 Active Cycles         cycle   6205587.79
    Total L1 Elapsed Cycles          cycle    151386112
    Average L2 Active Cycles         cycle   3321984.44
    Total L2 Elapsed Cycles          cycle    100799216
    Average SM Active Cycles         cycle   6205587.79
    Total SM Elapsed Cycles          cycle    151386112
    Average SMSP Active Cycles       cycle   6206481.85
    Total SMSP Elapsed Cycles        cycle    605544448
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.127%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.72% above the average, while the minimum instance value is 1.58% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.3%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.20
    Elapsed Cycles                cycle      6319466
    Memory Throughput                 %        88.87
    DRAM Throughput                   %         0.47
    Duration                         ms         5.27
    L1/TEX Cache Throughput           %        90.56
    L2 Cache Throughput               %         3.84
    SM Active Cycles              cycle   6201893.08
    Compute (SM) Throughput           %        88.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.48
    Executed Ipc Elapsed  inst/cycle         2.43
    Issue Slots Busy               %        62.16
    Issued Ipc Active     inst/cycle         2.49
    SM Busy                        %        62.16
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       892.30
    Mem Busy                              %        49.02
    Max Bandwidth                         %        88.87
    L1/TEX Hit Rate                       %        29.20
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.44
    Mem Pipes Busy                        %        88.87
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.27%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.27%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.15
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.85
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.93%                                                                                    
          On average, each warp of this kernel spends 8.7 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 45.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854898.71
    Issued Instructions                             inst    370070276
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.58
    Achieved Active Warps Per SM           warp        47.32
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146848
    Total DRAM Elapsed Cycles        cycle    126284800
    Average L1 Active Cycles         cycle   6201893.08
    Total L1 Elapsed Cycles          cycle    151667080
    Average L2 Active Cycles         cycle   3374470.81
    Total L2 Elapsed Cycles          cycle     99846064
    Average SM Active Cycles         cycle   6201893.08
    Total SM Elapsed Cycles          cycle    151667080
    Average SMSP Active Cycles       cycle   6202901.02
    Total SMSP Elapsed Cycles        cycle    606668320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.103%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.44% above the average, while the minimum instance value is 1.65% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.66%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.27
    Elapsed Cycles                cycle      6301263
    Memory Throughput                 %        89.13
    DRAM Throughput                   %         0.50
    Duration                         ms         4.94
    L1/TEX Cache Throughput           %        90.55
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   6202282.79
    Compute (SM) Throughput           %        89.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.26
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.48
    Executed Ipc Elapsed  inst/cycle         2.44
    Issue Slots Busy               %        62.15
    Issued Ipc Active     inst/cycle         2.49
    SM Busy                        %        62.15
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       950.80
    Mem Busy                              %        49.15
    Max Bandwidth                         %        89.13
    L1/TEX Hit Rate                       %        29.37
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.93
    Mem Pipes Busy                        %        89.13
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.34%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.34%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.14
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.86
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.12
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.85%                                                                                    
          On average, each warp of this kernel spends 8.7 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 45.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854895.65
    Issued Instructions                             inst    370069982
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.58
    Achieved Active Warps Per SM           warp        47.32
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146848
    Total DRAM Elapsed Cycles        cycle    118513664
    Average L1 Active Cycles         cycle   6202282.79
    Total L1 Elapsed Cycles          cycle    151229880
    Average L2 Active Cycles         cycle   3355715.31
    Total L2 Elapsed Cycles          cycle    101931648
    Average SM Active Cycles         cycle   6202282.79
    Total SM Elapsed Cycles          cycle    151229880
    Average SMSP Active Cycles       cycle   6203313.66
    Total SMSP Elapsed Cycles        cycle    604919520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.18%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.83% above the average, while the minimum instance value is 1.99% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.28%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       959.98
    Elapsed Cycles                cycle      6294643
    Memory Throughput                 %        89.22
    DRAM Throughput                   %         0.37
    Duration                         ms         6.56
    L1/TEX Cache Throughput           %        90.67
    L2 Cache Throughput               %         3.92
    SM Active Cycles              cycle   6194359.04
    Compute (SM) Throughput           %        89.22
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.03
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.48
    Executed Ipc Elapsed  inst/cycle         2.44
    Issue Slots Busy               %        62.23
    Issued Ipc Active     inst/cycle         2.49
    SM Busy                        %        62.23
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       716.66
    Mem Busy                              %        49.22
    Max Bandwidth                         %        89.22
    L1/TEX Hit Rate                       %        27.84
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.08
    Mem Pipes Busy                        %        89.22
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.36%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.36%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.23
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.77
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.01
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.35%                                                                                    
          On average, each warp of this kernel spends 8.8 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.3% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854840.52
    Issued Instructions                             inst    370064690
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.52
    Achieved Active Warps Per SM           warp        47.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146848
    Total DRAM Elapsed Cycles        cycle    157235200
    Average L1 Active Cycles         cycle   6194359.04
    Total L1 Elapsed Cycles          cycle    151071392
    Average L2 Active Cycles         cycle   3473986.06
    Total L2 Elapsed Cycles          cycle     99140576
    Average SM Active Cycles         cycle   6194359.04
    Total SM Elapsed Cycles          cycle    151071392
    Average SMSP Active Cycles       cycle   6194991.48
    Total SMSP Elapsed Cycles        cycle    604285568
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 15.2%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.12
    Elapsed Cycles                cycle      6310551
    Memory Throughput                 %        89.00
    DRAM Throughput                   %         0.44
    Duration                         ms         5.61
    L1/TEX Cache Throughput           %        90.56
    L2 Cache Throughput               %         3.91
    SM Active Cycles              cycle      6201337
    Compute (SM) Throughput           %        89.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.59
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.48
    Executed Ipc Elapsed  inst/cycle         2.43
    Issue Slots Busy               %        62.16
    Issued Ipc Active     inst/cycle         2.49
    SM Busy                        %        62.16
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       837.70
    Mem Busy                              %        49.07
    Max Bandwidth                         %        89.00
    L1/TEX Hit Rate                       %        29.03
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.74
    Mem Pipes Busy                        %        89.00
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.3%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.3%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.17
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.83
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.99%                                                                                    
          On average, each warp of this kernel spends 8.8 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854905.35
    Issued Instructions                             inst    370070914
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.51
    Achieved Active Warps Per SM           warp        47.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146844
    Total DRAM Elapsed Cycles        cycle    134513664
    Average L1 Active Cycles         cycle      6201337
    Total L1 Elapsed Cycles          cycle    151453176
    Average L2 Active Cycles         cycle      3334164
    Total L2 Elapsed Cycles          cycle     99622336
    Average SM Active Cycles         cycle      6201337
    Total SM Elapsed Cycles          cycle    151453176
    Average SMSP Active Cycles       cycle   6200472.24
    Total SMSP Elapsed Cycles        cycle    605812704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.001%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.34% above the average, while the minimum instance value is 1.48% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.52%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.20
    Elapsed Cycles                cycle      6317569
    Memory Throughput                 %        88.90
    DRAM Throughput                   %         0.47
    Duration                         ms         5.26
    L1/TEX Cache Throughput           %        90.57
    L2 Cache Throughput               %         3.88
    SM Active Cycles              cycle   6200890.54
    Compute (SM) Throughput           %        88.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.91
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.48
    Executed Ipc Elapsed  inst/cycle         2.43
    Issue Slots Busy               %        62.17
    Issued Ipc Active     inst/cycle         2.49
    SM Busy                        %        62.17
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       892.93
    Mem Busy                              %        49.01
    Max Bandwidth                         %        88.90
    L1/TEX Hit Rate                       %        28.82
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.83
    Mem Pipes Busy                        %        88.90
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.28%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.28%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.17
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.83
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 45.94%                                                                                    
          On average, each warp of this kernel spends 8.7 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 45.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854947.06
    Issued Instructions                             inst    370074918
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.62
    Achieved Active Warps Per SM           warp        47.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146908
    Total DRAM Elapsed Cycles        cycle    126245888
    Average L1 Active Cycles         cycle   6200890.54
    Total L1 Elapsed Cycles          cycle    151621560
    Average L2 Active Cycles         cycle   3370813.69
    Total L2 Elapsed Cycles          cycle     99816320
    Average SM Active Cycles         cycle   6200890.54
    Total SM Elapsed Cycles          cycle    151621560
    Average SMSP Active Cycles       cycle   6200821.27
    Total SMSP Elapsed Cycles        cycle    606486240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.04%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.33% above the average, while the minimum instance value is 1.74% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.65%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.30
    Elapsed Cycles                cycle      6307052
    Memory Throughput                 %        89.05
    DRAM Throughput                   %         0.51
    Duration                         ms         4.83
    L1/TEX Cache Throughput           %        90.48
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   6207143.92
    Compute (SM) Throughput           %        89.05
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.59
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.47
    Executed Ipc Elapsed  inst/cycle         2.43
    Issue Slots Busy               %        62.10
    Issued Ipc Active     inst/cycle         2.48
    SM Busy                        %        62.10
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       972.26
    Mem Busy                              %        49.10
    Max Bandwidth                         %        89.05
    L1/TEX Hit Rate                       %        29.17
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.20
    Mem Pipes Busy                        %        89.05
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.32%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.3 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.32%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.10
    Issued Warp Per Scheduler                        0.62
    No Eligible                            %        37.90
    Active Warps Per Scheduler          warp        11.83
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.05
    Warp Cycles Per Executed Instruction           cycle        19.13
    Avg. Active Threads Per Warp                                31.94
    Avg. Not Predicated Off Threads Per Warp                    31.55
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.02%                                                                                    
          On average, each warp of this kernel spends 8.8 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3839068.50
    Executed Instructions                           inst    368550576
    Avg. Issued Instructions Per Scheduler          inst   3854897.56
    Issued Instructions                             inst    370070166
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.58
    Achieved Active Warps Per SM           warp        47.32
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146844
    Total DRAM Elapsed Cycles        cycle    115896320
    Average L1 Active Cycles         cycle   6207143.92
    Total L1 Elapsed Cycles          cycle    151369128
    Average L2 Active Cycles         cycle   3384211.44
    Total L2 Elapsed Cycles          cycle    102071376
    Average SM Active Cycles         cycle   6207143.92
    Total SM Elapsed Cycles          cycle    151369128
    Average SMSP Active Cycles       cycle   6207129.98
    Total SMSP Elapsed Cycles        cycle    605476512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.159%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.72% above the average, while the minimum instance value is 1.76% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst     96877920
    Branch Efficiency                   %        99.96
    Avg. Divergent Branches                     383.50
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.39%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 2654017 excessive sectors (27% of the     
          total 9787009 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source        
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.00
    Elapsed Cycles                cycle     14638431
    Memory Throughput                 %        90.88
    DRAM Throughput                   %         0.30
    Duration                         ms        14.57
    L1/TEX Cache Throughput           %        91.20
    L2 Cache Throughput               %         4.06
    SM Active Cycles              cycle  14586089.21
    Compute (SM) Throughput           %        90.88
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.94
    Dropped Samples                sample            6
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.57
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.57
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       574.67
    Mem Busy                              %        50.08
    Max Bandwidth                         %        90.88
    L1/TEX Hit Rate                       %        20.98
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.29
    Mem Pipes Busy                        %        90.88
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.59
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.41
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.88%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127061.84
    Issued Instructions                             inst    876197937
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.09
    Achieved Active Warps Per SM           warp        47.56
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       261576
    Total DRAM Elapsed Cycles        cycle    349282304
    Average L1 Active Cycles         cycle  14586089.21
    Total L1 Elapsed Cycles          cycle    351322328
    Average L2 Active Cycles         cycle   9718981.50
    Total L2 Elapsed Cycles          cycle    237710752
    Average SM Active Cycles         cycle  14586089.21
    Total SM Elapsed Cycles          cycle    351322328
    Average SMSP Active Cycles       cycle  14581404.07
    Total SMSP Elapsed Cycles        cycle   1405289312
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.12%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.08
    Elapsed Cycles                cycle     14640281
    Memory Throughput                 %        90.87
    DRAM Throughput                   %         0.32
    Duration                         ms        13.56
    L1/TEX Cache Throughput           %        91.23
    L2 Cache Throughput               %         4.15
    SM Active Cycles              cycle  14581900.96
    Compute (SM) Throughput           %        90.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.16
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.59
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.59
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       614.35
    Mem Busy                              %        50.06
    Max Bandwidth                         %        90.87
    L1/TEX Hit Rate                       %        21.26
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.00
    Mem Pipes Busy                        %        90.87
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.59
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.41
    Active Warps Per Scheduler          warp        11.90
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.85%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127090.42
    Issued Instructions                             inst    876200680
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.13
    Achieved Active Warps Per SM           warp        47.58
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260252
    Total DRAM Elapsed Cycles        cycle    325067776
    Average L1 Active Cycles         cycle  14581900.96
    Total L1 Elapsed Cycles          cycle    351366720
    Average L2 Active Cycles         cycle   9632210.50
    Total L2 Elapsed Cycles          cycle    234244480
    Average SM Active Cycles         cycle  14581900.96
    Total SM Elapsed Cycles          cycle    351366720
    Average SMSP Active Cycles       cycle  14581665.50
    Total SMSP Elapsed Cycles        cycle   1405466880
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.2%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.03
    Elapsed Cycles                cycle     14639227
    Memory Throughput                 %        90.87
    DRAM Throughput                   %         0.31
    Duration                         ms        14.14
    L1/TEX Cache Throughput           %        91.20
    L2 Cache Throughput               %         4.20
    SM Active Cycles              cycle  14587040.88
    Compute (SM) Throughput           %        90.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.75
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.57
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.57
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       597.08
    Mem Busy                              %        50.07
    Max Bandwidth                         %        90.87
    L1/TEX Hit Rate                       %        20.85
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.87
    Mem Pipes Busy                        %        90.87
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.56
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.44
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.83%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127062.34
    Issued Instructions                             inst    876197985
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.08
    Achieved Active Warps Per SM           warp        47.56
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       263912
    Total DRAM Elapsed Cycles        cycle    339177472
    Average L1 Active Cycles         cycle  14587040.88
    Total L1 Elapsed Cycles          cycle    351341400
    Average L2 Active Cycles         cycle   9719640.12
    Total L2 Elapsed Cycles          cycle    230833072
    Average SM Active Cycles         cycle  14587040.88
    Total SM Elapsed Cycles          cycle    351341400
    Average SMSP Active Cycles       cycle  14589547.46
    Total SMSP Elapsed Cycles        cycle   1405365600
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.52%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.05
    Elapsed Cycles                cycle     14640546
    Memory Throughput                 %        90.86
    DRAM Throughput                   %         0.31
    Duration                         ms        13.94
    L1/TEX Cache Throughput           %        91.22
    L2 Cache Throughput               %         4.11
    SM Active Cycles              cycle  14582845.42
    Compute (SM) Throughput           %        90.86
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.19
    Dropped Samples                sample            5
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.59
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.59
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       598.24
    Mem Busy                              %        50.07
    Max Bandwidth                         %        90.86
    L1/TEX Hit Rate                       %        20.53
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.00
    Mem Pipes Busy                        %        90.86
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.60
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.40
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.77%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127045.83
    Issued Instructions                             inst    876196400
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.12
    Achieved Active Warps Per SM           warp        47.58
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260672
    Total DRAM Elapsed Cycles        cycle    334362624
    Average L1 Active Cycles         cycle  14582845.42
    Total L1 Elapsed Cycles          cycle    351373080
    Average L2 Active Cycles         cycle   9756999.31
    Total L2 Elapsed Cycles          cycle    234248880
    Average SM Active Cycles         cycle  14582845.42
    Total SM Elapsed Cycles          cycle    351373080
    Average SMSP Active Cycles       cycle  14580207.32
    Total SMSP Elapsed Cycles        cycle   1405492320
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.37%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.08
    Elapsed Cycles                cycle     14644101
    Memory Throughput                 %        90.84
    DRAM Throughput                   %         0.33
    Duration                         ms        13.56
    L1/TEX Cache Throughput           %        91.22
    L2 Cache Throughput               %         4.11
    SM Active Cycles              cycle  14583901.21
    Compute (SM) Throughput           %        90.84
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.94
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.58
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.58
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       628.47
    Mem Busy                              %        50.05
    Max Bandwidth                         %        90.84
    L1/TEX Hit Rate                       %        20.83
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.72
    Mem Pipes Busy                        %        90.84
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.58
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.42
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.01
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.94%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127063.39
    Issued Instructions                             inst    876198085
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.11
    Achieved Active Warps Per SM           warp        47.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       266304
    Total DRAM Elapsed Cycles        cycle    325153792
    Average L1 Active Cycles         cycle  14583901.21
    Total L1 Elapsed Cycles          cycle    351458408
    Average L2 Active Cycles         cycle   9726658.56
    Total L2 Elapsed Cycles          cycle    234305552
    Average SM Active Cycles         cycle  14583901.21
    Total SM Elapsed Cycles          cycle    351458408
    Average SMSP Active Cycles       cycle  14585258.09
    Total SMSP Elapsed Cycles        cycle   1405833632
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.33%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.11
    Elapsed Cycles                cycle     14635809
    Memory Throughput                 %        90.89
    DRAM Throughput                   %         0.33
    Duration                         ms        13.19
    L1/TEX Cache Throughput           %        91.19
    L2 Cache Throughput               %         4.13
    SM Active Cycles              cycle  14587622.54
    Compute (SM) Throughput           %        90.89
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.88
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.57
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.57
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       631.63
    Mem Busy                              %        50.08
    Max Bandwidth                         %        90.89
    L1/TEX Hit Rate                       %        20.62
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.71
    Mem Pipes Busy                        %        90.89
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.24%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.24%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.59
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.41
    Active Warps Per Scheduler          warp        11.90
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.89%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127072.20
    Issued Instructions                             inst    876198931
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.09
    Achieved Active Warps Per SM           warp        47.56
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260260
    Total DRAM Elapsed Cycles        cycle    316185600
    Average L1 Active Cycles         cycle  14587622.54
    Total L1 Elapsed Cycles          cycle    351259392
    Average L2 Active Cycles         cycle   9685265.44
    Total L2 Elapsed Cycles          cycle    234172496
    Average SM Active Cycles         cycle  14587622.54
    Total SM Elapsed Cycles          cycle    351259392
    Average SMSP Active Cycles       cycle  14581315.22
    Total SMSP Elapsed Cycles        cycle   1405037568
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.28%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.00
    Elapsed Cycles                cycle     14639734
    Memory Throughput                 %        90.87
    DRAM Throughput                   %         0.30
    Duration                         ms        14.57
    L1/TEX Cache Throughput           %        91.21
    L2 Cache Throughput               %         4.07
    SM Active Cycles              cycle  14584405.67
    Compute (SM) Throughput           %        90.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.68
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.58
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.58
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       573.85
    Mem Busy                              %        50.07
    Max Bandwidth                         %        90.87
    L1/TEX Hit Rate                       %        21.08
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.80
    Mem Pipes Busy                        %        90.87
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.58
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.42
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.9%                                                                                     
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127056.09
    Issued Instructions                             inst    876197385
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.10
    Achieved Active Warps Per SM           warp        47.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       261228
    Total DRAM Elapsed Cycles        cycle    349315072
    Average L1 Active Cycles         cycle  14584405.67
    Total L1 Elapsed Cycles          cycle    351353552
    Average L2 Active Cycles         cycle   9723453.75
    Total L2 Elapsed Cycles          cycle    237731744
    Average SM Active Cycles         cycle  14584405.67
    Total SM Elapsed Cycles          cycle    351353552
    Average SMSP Active Cycles       cycle  14584203.56
    Total SMSP Elapsed Cycles        cycle   1405414208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.13%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.24
    Elapsed Cycles                cycle     14642260
    Memory Throughput                 %        90.85
    DRAM Throughput                   %         0.37
    Duration                         ms        11.76
    L1/TEX Cache Throughput           %        91.16
    L2 Cache Throughput               %         4.09
    SM Active Cycles              cycle  14592807.50
    Compute (SM) Throughput           %        90.85
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.70
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.55
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.55
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       710.56
    Mem Busy                              %        50.06
    Max Bandwidth                         %        90.85
    L1/TEX Hit Rate                       %        20.59
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.80
    Mem Pipes Busy                        %        90.85
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.58
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.42
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.01
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.7%                                                                                     
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.7% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127120.16
    Issued Instructions                             inst    876203535
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.10
    Achieved Active Warps Per SM           warp        47.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       261152
    Total DRAM Elapsed Cycles        cycle    282024960
    Average L1 Active Cycles         cycle  14592807.50
    Total L1 Elapsed Cycles          cycle    351414048
    Average L2 Active Cycles         cycle   9626956.75
    Total L2 Elapsed Cycles          cycle    234263296
    Average SM Active Cycles         cycle  14592807.50
    Total SM Elapsed Cycles          cycle    351414048
    Average SMSP Active Cycles       cycle  14585549.58
    Total SMSP Elapsed Cycles        cycle   1405656192
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.19%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.05
    Elapsed Cycles                cycle     14633142
    Memory Throughput                 %        90.91
    DRAM Throughput                   %         0.31
    Duration                         ms        13.94
    L1/TEX Cache Throughput           %        91.25
    L2 Cache Throughput               %         4.13
    SM Active Cycles              cycle  14578485.54
    Compute (SM) Throughput           %        90.91
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.65
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.61
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.61
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       597.88
    Mem Busy                              %        50.09
    Max Bandwidth                         %        90.91
    L1/TEX Hit Rate                       %        20.69
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.52
    Mem Pipes Busy                        %        90.91
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.24%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.24%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.56
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.44
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.83%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127067.40
    Issued Instructions                             inst    876198470
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.13
    Achieved Active Warps Per SM           warp        47.58
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260384
    Total DRAM Elapsed Cycles        cycle    334192640
    Average L1 Active Cycles         cycle  14578485.54
    Total L1 Elapsed Cycles          cycle    351195280
    Average L2 Active Cycles         cycle   9706532.81
    Total L2 Elapsed Cycles          cycle    234130304
    Average SM Active Cycles         cycle  14578485.54
    Total SM Elapsed Cycles          cycle    351195280
    Average SMSP Active Cycles       cycle  14589737.12
    Total SMSP Elapsed Cycles        cycle   1404781120
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.31%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.08
    Elapsed Cycles                cycle     14635280
    Memory Throughput                 %        90.90
    DRAM Throughput                   %         0.32
    Duration                         ms        13.55
    L1/TEX Cache Throughput           %        91.22
    L2 Cache Throughput               %         4.13
    SM Active Cycles              cycle  14583728.33
    Compute (SM) Throughput           %        90.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.47
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.49
    Executed Ipc Elapsed  inst/cycle         2.48
    Issue Slots Busy               %        62.58
    Issued Ipc Active     inst/cycle         2.50
    SM Busy                        %        62.58
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       615.67
    Mem Busy                              %        50.08
    Max Bandwidth                         %        90.90
    L1/TEX Hit Rate                       %        20.91
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.95
    Mem Pipes Busy                        %        90.90
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.24%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.24%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.58
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.42
    Active Warps Per Scheduler          warp        11.89
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.00
    Warp Cycles Per Executed Instruction           cycle        19.08
    Avg. Active Threads Per Warp                                31.95
    Avg. Not Predicated Off Threads Per Warp                    31.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.9%                                                                                     
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   9090166.67
    Executed Instructions                           inst    872656000
    Avg. Issued Instructions Per Scheduler          inst   9127072.03
    Issued Instructions                             inst    876198915
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.13
    Achieved Active Warps Per SM           warp        47.58
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260724
    Total DRAM Elapsed Cycles        cycle    324957184
    Average L1 Active Cycles         cycle  14583728.33
    Total L1 Elapsed Cycles          cycle    351246664
    Average L2 Active Cycles         cycle   9673974.06
    Total L2 Elapsed Cycles          cycle    234164464
    Average SM Active Cycles         cycle  14583728.33
    Total SM Elapsed Cycles          cycle    351246664
    Average SMSP Active Cycles       cycle  14585114.43
    Total SMSP Elapsed Cycles        cycle   1404986656
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    229341440
    Branch Efficiency                   %        99.97
    Avg. Divergent Branches                     681.33
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 13.26%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 4210560 excessive sectors (20% of the     
          total 20987010 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle     28491362
    Memory Throughput                 %        91.16
    DRAM Throughput                   %         0.33
    Duration                         ms        20.87
    L1/TEX Cache Throughput           %        91.39
    L2 Cache Throughput               %         4.40
    SM Active Cycles              cycle  28419017.12
    Compute (SM) Throughput           %        91.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.50
    Dropped Samples                sample            2
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.69
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.69
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       624.19
    Mem Busy                              %        50.36
    Max Bandwidth                         %        91.16
    L1/TEX Hit Rate                       %        17.84
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.25
    Mem Pipes Busy                        %        91.16
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.71
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.29
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.97%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814471.08
    Issued Instructions                             inst   1710189224
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.39
    Achieved Active Warps Per SM           warp        47.70
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       407148
    Total DRAM Elapsed Cycles        cycle    500530176
    Average L1 Active Cycles         cycle  28419017.12
    Total L1 Elapsed Cycles          cycle    683792200
    Average L2 Active Cycles         cycle  19374099.94
    Total L2 Elapsed Cycles          cycle    460816896
    Average SM Active Cycles         cycle  28419017.12
    Total SM Elapsed Cycles          cycle    683792200
    Average SMSP Active Cycles       cycle  28409247.72
    Total SMSP Elapsed Cycles        cycle   2735168800
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.33%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.35
    Elapsed Cycles                cycle     28476147
    Memory Throughput                 %        91.20
    DRAM Throughput                   %         0.32
    Duration                         ms        21.09
    L1/TEX Cache Throughput           %        91.42
    L2 Cache Throughput               %         4.54
    SM Active Cycles              cycle  28408113.17
    Compute (SM) Throughput           %        91.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.32
    Dropped Samples                sample            4
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.71
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.71
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       617.37
    Mem Busy                              %        50.39
    Max Bandwidth                         %        91.20
    L1/TEX Hit Rate                       %        19.43
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.72
    Mem Pipes Busy                        %        91.20
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.95%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.95%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.69
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.31
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.98%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814496.44
    Issued Instructions                             inst   1710191658
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.42
    Achieved Active Warps Per SM           warp        47.72
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406956
    Total DRAM Elapsed Cycles        cycle    505821184
    Average L1 Active Cycles         cycle  28408113.17
    Total L1 Elapsed Cycles          cycle    683427216
    Average L2 Active Cycles         cycle  19537524.56
    Total L2 Elapsed Cycles          cycle    455608432
    Average SM Active Cycles         cycle  28408113.17
    Total SM Elapsed Cycles          cycle    683427216
    Average SMSP Active Cycles       cycle  28414790.86
    Total SMSP Elapsed Cycles        cycle   2733708864
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.69%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle     28482335
    Memory Throughput                 %        91.18
    DRAM Throughput                   %         0.32
    Duration                         ms        20.87
    L1/TEX Cache Throughput           %        91.40
    L2 Cache Throughput               %         4.45
    SM Active Cycles              cycle  28414445.54
    Compute (SM) Throughput           %        91.18
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.11
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.70
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.70
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       622.52
    Mem Busy                              %        50.38
    Max Bandwidth                         %        91.18
    L1/TEX Hit Rate                       %        19.02
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.25
    Mem Pipes Busy                        %        91.18
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.69
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.31
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.02
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.98%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814542.29
    Issued Instructions                             inst   1710196060
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.41
    Achieved Active Warps Per SM           warp        47.72
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405924
    Total DRAM Elapsed Cycles        cycle    500370432
    Average L1 Active Cycles         cycle  28414445.54
    Total L1 Elapsed Cycles          cycle    683575776
    Average L2 Active Cycles         cycle  19356902.50
    Total L2 Elapsed Cycles          cycle    460667392
    Average SM Active Cycles         cycle  28414445.54
    Total SM Elapsed Cycles          cycle    683575776
    Average SMSP Active Cycles       cycle  28418425.24
    Total SMSP Elapsed Cycles        cycle   2734303104
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.31%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle     28484639
    Memory Throughput                 %        91.18
    DRAM Throughput                   %         0.32
    Duration                         ms        20.87
    L1/TEX Cache Throughput           %        91.44
    L2 Cache Throughput               %         4.39
    SM Active Cycles              cycle  28403334.38
    Compute (SM) Throughput           %        91.18
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.94
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.72
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.72
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       622.67
    Mem Busy                              %        50.38
    Max Bandwidth                         %        91.18
    L1/TEX Hit Rate                       %        18.47
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       101.25
    Mem Pipes Busy                        %        91.18
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.70
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.30
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.96%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814507.10
    Issued Instructions                             inst   1710192682
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.44
    Achieved Active Warps Per SM           warp        47.73
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406060
    Total DRAM Elapsed Cycles        cycle    500411392
    Average L1 Active Cycles         cycle  28403334.38
    Total L1 Elapsed Cycles          cycle    683631120
    Average L2 Active Cycles         cycle  19294778.06
    Total L2 Elapsed Cycles          cycle    460716480
    Average SM Active Cycles         cycle  28403334.38
    Total SM Elapsed Cycles          cycle    683631120
    Average SMSP Active Cycles       cycle  28413468.59
    Total SMSP Elapsed Cycles        cycle   2734524480
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.25%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle     28457274
    Memory Throughput                 %        91.27
    DRAM Throughput                   %         0.32
    Duration                         ms        20.85
    L1/TEX Cache Throughput           %        91.38
    L2 Cache Throughput               %         4.45
    SM Active Cycles              cycle     28420118
    Compute (SM) Throughput           %        91.27
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.63
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.68
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.68
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       623.33
    Mem Busy                              %        50.43
    Max Bandwidth                         %        91.27
    L1/TEX Hit Rate                       %        19.11
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.86
    Mem Pipes Busy                        %        91.27
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.97%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.97%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.67
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.33
    Active Warps Per Scheduler          warp        11.92
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.98%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814519.18
    Issued Instructions                             inst   1710193841
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.40
    Achieved Active Warps Per SM           warp        47.71
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406100
    Total DRAM Elapsed Cycles        cycle    499931136
    Average L1 Active Cycles         cycle     28420118
    Total L1 Elapsed Cycles          cycle    682974200
    Average L2 Active Cycles         cycle  19336276.88
    Total L2 Elapsed Cycles          cycle    460273936
    Average SM Active Cycles         cycle     28420118
    Total SM Elapsed Cycles          cycle    682974200
    Average SMSP Active Cycles       cycle  28426820.42
    Total SMSP Elapsed Cycles        cycle   2731896800
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.31%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.41
    Elapsed Cycles                cycle     28488994
    Memory Throughput                 %        91.16
    DRAM Throughput                   %         0.34
    Duration                         ms        20.21
    L1/TEX Cache Throughput           %        91.43
    L2 Cache Throughput               %         4.46
    SM Active Cycles              cycle  28406623.33
    Compute (SM) Throughput           %        91.16
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample           15
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.71
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.71
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       642.99
    Mem Busy                              %        50.37
    Max Bandwidth                         %        91.16
    L1/TEX Hit Rate                       %        18.32
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.31
    Mem Pipes Busy                        %        91.16
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.69
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.31
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.97%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814469.02
    Issued Instructions                             inst   1710189026
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.44
    Achieved Active Warps Per SM           warp        47.73
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405988
    Total DRAM Elapsed Cycles        cycle    484515840
    Average L1 Active Cycles         cycle  28406623.33
    Total L1 Elapsed Cycles          cycle    683734912
    Average L2 Active Cycles         cycle  19178659.94
    Total L2 Elapsed Cycles          cycle    459629776
    Average SM Active Cycles         cycle  28406623.33
    Total SM Elapsed Cycles          cycle    683734912
    Average SMSP Active Cycles       cycle  28418777.96
    Total SMSP Elapsed Cycles        cycle   2734939648
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.19%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.42
    Elapsed Cycles                cycle     28482999
    Memory Throughput                 %        91.18
    DRAM Throughput                   %         0.34
    Duration                         ms        19.99
    L1/TEX Cache Throughput           %        91.38
    L2 Cache Throughput               %         4.40
    SM Active Cycles              cycle  28421238.21
    Compute (SM) Throughput           %        91.18
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.94
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.68
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.68
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       650.21
    Mem Busy                              %        50.38
    Max Bandwidth                         %        91.18
    L1/TEX Hit Rate                       %        19.10
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.47
    Mem Pipes Busy                        %        91.18
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.68
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.32
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.99%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814475.78
    Issued Instructions                             inst   1710189675
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.40
    Achieved Active Warps Per SM           warp        47.71
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406140
    Total DRAM Elapsed Cycles        cycle    479313920
    Average L1 Active Cycles         cycle  28421238.21
    Total L1 Elapsed Cycles          cycle    683591736
    Average L2 Active Cycles         cycle  19268948.81
    Total L2 Elapsed Cycles          cycle    465319408
    Average SM Active Cycles         cycle  28421238.21
    Total SM Elapsed Cycles          cycle    683591736
    Average SMSP Active Cycles       cycle  28422212.40
    Total SMSP Elapsed Cycles        cycle   2734366944
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.05%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     28482619
    Memory Throughput                 %        91.18
    DRAM Throughput                   %         0.33
    Duration                         ms        20.64
    L1/TEX Cache Throughput           %        91.43
    L2 Cache Throughput               %         4.47
    SM Active Cycles              cycle  28407047.67
    Compute (SM) Throughput           %        91.18
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.73
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.71
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.71
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       635.18
    Mem Busy                              %        50.39
    Max Bandwidth                         %        91.18
    L1/TEX Hit Rate                       %        18.48
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.14
    Mem Pipes Busy                        %        91.18
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.69
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.31
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.02%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814478.34
    Issued Instructions                             inst   1710189921
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.44
    Achieved Active Warps Per SM           warp        47.73
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       409684
    Total DRAM Elapsed Cycles        cycle    494937088
    Average L1 Active Cycles         cycle  28407047.67
    Total L1 Elapsed Cycles          cycle    683582320
    Average L2 Active Cycles         cycle  19454696.19
    Total L2 Elapsed Cycles          cycle    455651536
    Average SM Active Cycles         cycle  28407047.67
    Total SM Elapsed Cycles          cycle    683582320
    Average SMSP Active Cycles       cycle  28416329.73
    Total SMSP Elapsed Cycles        cycle   2734329280
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.61%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.33
    Elapsed Cycles                cycle     28480796
    Memory Throughput                 %        91.19
    DRAM Throughput                   %         0.32
    Duration                         ms        21.33
    L1/TEX Cache Throughput           %        91.43
    L2 Cache Throughput               %         4.43
    SM Active Cycles              cycle  28404711.71
    Compute (SM) Throughput           %        91.19
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.91
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.72
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.72
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       611.01
    Mem Busy                              %        50.39
    Max Bandwidth                         %        91.19
    L1/TEX Hit Rate                       %        18.81
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.00
    Mem Pipes Busy                        %        91.19
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.95%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.95%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.70
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.30
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.02
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.07%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.1% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814427.02
    Issued Instructions                             inst   1710184994
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.44
    Achieved Active Warps Per SM           warp        47.73
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       407352
    Total DRAM Elapsed Cycles        cycle    511587328
    Average L1 Active Cycles         cycle  28404711.71
    Total L1 Elapsed Cycles          cycle    683539048
    Average L2 Active Cycles         cycle  19512032.06
    Total L2 Elapsed Cycles          cycle    460800624
    Average SM Active Cycles         cycle  28404711.71
    Total SM Elapsed Cycles          cycle    683539048
    Average SMSP Active Cycles       cycle  28411048.64
    Total SMSP Elapsed Cycles        cycle   2734156192
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.46%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle     28487238
    Memory Throughput                 %        91.17
    DRAM Throughput                   %         0.32
    Duration                         ms        20.87
    L1/TEX Cache Throughput           %        91.41
    L2 Cache Throughput               %         4.45
    SM Active Cycles              cycle  28412277.46
    Compute (SM) Throughput           %        91.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.04
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.70
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.70
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       622.86
    Mem Busy                              %        50.37
    Max Bandwidth                         %        91.17
    L1/TEX Hit Rate                       %        18.63
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.75
    Mem Pipes Busy                        %        91.17
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 24.94%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.68
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.32
    Active Warps Per Scheduler          warp        11.93
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.10
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.98%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.0% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  17742702.50
    Executed Instructions                           inst   1703299440
    Avg. Issued Instructions Per Scheduler          inst  17814468.51
    Issued Instructions                             inst   1710188977
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.42
    Achieved Active Warps Per SM           warp        47.72
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406216
    Total DRAM Elapsed Cycles        cycle    500457472
    Average L1 Active Cycles         cycle  28412277.46
    Total L1 Elapsed Cycles          cycle    683693256
    Average L2 Active Cycles         cycle  19373219.94
    Total L2 Elapsed Cycles          cycle    460747264
    Average SM Active Cycles         cycle  28412277.46
    Total SM Elapsed Cycles          cycle    683693256
    Average SMSP Active Cycles       cycle  28421112.80
    Total SMSP Elapsed Cycles        cycle   2734773024
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    447587040
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    1064.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 18.33%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 12249202 excessive sectors (27% of the    
          total 44964885 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     49162817
    Memory Throughput                 %        91.26
    DRAM Throughput                   %         0.27
    Duration                         ms        35.63
    L1/TEX Cache Throughput           %        91.52
    L2 Cache Throughput               %         3.97
    SM Active Cycles              cycle  49025485.29
    Compute (SM) Throughput           %        91.26
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.07
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.76
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.76
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       525.74
    Mem Busy                              %        50.16
    Max Bandwidth                         %        91.26
    L1/TEX Hit Rate                       %        12.05
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.12
    Mem Pipes Busy                        %        91.26
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.75
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.25
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.12
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.83%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768364.43
    Issued Instructions                             inst   2953762985
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.59
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       585304
    Total DRAM Elapsed Cycles        cycle    854293504
    Average L1 Active Cycles         cycle  49025485.29
    Total L1 Elapsed Cycles          cycle   1179907096
    Average L2 Active Cycles         cycle  29884276.50
    Total L2 Elapsed Cycles          cycle    786517632
    Average SM Active Cycles         cycle  49025485.29
    Total SM Elapsed Cycles          cycle   1179907096
    Average SMSP Active Cycles       cycle  49034078.78
    Total SMSP Elapsed Cycles        cycle   4719628384
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.18
    Elapsed Cycles                cycle     49151433
    Memory Throughput                 %        91.28
    DRAM Throughput                   %         0.24
    Duration                         ms        41.48
    L1/TEX Cache Throughput           %        91.55
    L2 Cache Throughput               %         4.02
    SM Active Cycles              cycle  49009112.83
    Compute (SM) Throughput           %        91.28
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.45
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.78
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.78
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       451.51
    Mem Busy                              %        50.18
    Max Bandwidth                         %        91.28
    L1/TEX Hit Rate                       %        12.56
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.34
    Mem Pipes Busy                        %        91.28
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.75
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.25
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.88%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768250.28
    Issued Instructions                             inst   2953752027
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.62
    Achieved Active Warps Per SM           warp        47.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       585240
    Total DRAM Elapsed Cycles        cycle    994642944
    Average L1 Active Cycles         cycle  49009112.83
    Total L1 Elapsed Cycles          cycle   1179634352
    Average L2 Active Cycles         cycle  30127866.31
    Total L2 Elapsed Cycles          cycle    786420320
    Average SM Active Cycles         cycle  49009112.83
    Total SM Elapsed Cycles          cycle   1179634352
    Average SMSP Active Cycles       cycle  49029509.97
    Total SMSP Elapsed Cycles        cycle   4718537408
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     49165314
    Memory Throughput                 %        91.26
    DRAM Throughput                   %         0.27
    Duration                         ms        35.63
    L1/TEX Cache Throughput           %        91.51
    L2 Cache Throughput               %         3.99
    SM Active Cycles              cycle     49026991
    Compute (SM) Throughput           %        91.26
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.59
    Dropped Samples                sample           15
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.76
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.76
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       525.14
    Mem Busy                              %        50.15
    Max Bandwidth                         %        91.26
    L1/TEX Hit Rate                       %        12.58
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.53
    Mem Pipes Busy                        %        91.26
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.75
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.25
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.12
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.81%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768357.42
    Issued Instructions                             inst   2953762312
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.59
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584664
    Total DRAM Elapsed Cycles        cycle    854336512
    Average L1 Active Cycles         cycle     49026991
    Total L1 Elapsed Cycles          cycle   1179967096
    Average L2 Active Cycles         cycle  29620780.38
    Total L2 Elapsed Cycles          cycle    786551040
    Average SM Active Cycles         cycle     49026991
    Total SM Elapsed Cycles          cycle   1179967096
    Average SMSP Active Cycles       cycle  49032696.14
    Total SMSP Elapsed Cycles        cycle   4719868384
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     49157082
    Memory Throughput                 %        91.27
    DRAM Throughput                   %         0.27
    Duration                         ms        35.57
    L1/TEX Cache Throughput           %        91.52
    L2 Cache Throughput               %         3.92
    SM Active Cycles              cycle  49021567.54
    Compute (SM) Throughput           %        91.27
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.62
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.76
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.76
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       526.08
    Mem Busy                              %        50.16
    Max Bandwidth                         %        91.27
    L1/TEX Hit Rate                       %        12.66
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.18
    Mem Pipes Busy                        %        91.27
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.76
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.24
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.12
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.78%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768331.15
    Issued Instructions                             inst   2953759790
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.60
    Achieved Active Warps Per SM           warp        47.81
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584824
    Total DRAM Elapsed Cycles        cycle    853044224
    Average L1 Active Cycles         cycle  49021567.54
    Total L1 Elapsed Cycles          cycle   1179769456
    Average L2 Active Cycles         cycle  29969365.38
    Total L2 Elapsed Cycles          cycle    798421168
    Average SM Active Cycles         cycle  49021567.54
    Total SM Elapsed Cycles          cycle   1179769456
    Average SMSP Active Cycles       cycle  49026894.49
    Total SMSP Elapsed Cycles        cycle   4719077824
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.41
    Elapsed Cycles                cycle     49173369
    Memory Throughput                 %        91.24
    DRAM Throughput                   %         0.28
    Duration                         ms        34.87
    L1/TEX Cache Throughput           %        91.52
    L2 Cache Throughput               %         3.96
    SM Active Cycles              cycle  49025519.38
    Compute (SM) Throughput           %        91.24
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.18
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.76
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.76
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       536.09
    Mem Busy                              %        50.15
    Max Bandwidth                         %        91.24
    L1/TEX Hit Rate                       %        12.00
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.75
    Mem Pipes Busy                        %        91.24
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.75
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.25
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.12
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.8%                                                                                     
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768243.38
    Issued Instructions                             inst   2953751364
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.59
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584256
    Total DRAM Elapsed Cycles        cycle    836296704
    Average L1 Active Cycles         cycle  49025519.38
    Total L1 Elapsed Cycles          cycle   1180159664
    Average L2 Active Cycles         cycle  29420115.06
    Total L2 Elapsed Cycles          cycle    792178272
    Average SM Active Cycles         cycle  49025519.38
    Total SM Elapsed Cycles          cycle   1180159664
    Average SMSP Active Cycles       cycle  49030501.19
    Total SMSP Elapsed Cycles        cycle   4720638656
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     49158811
    Memory Throughput                 %        91.27
    DRAM Throughput                   %         0.27
    Duration                         ms        35.62
    L1/TEX Cache Throughput           %        91.52
    L2 Cache Throughput               %         3.99
    SM Active Cycles              cycle  49023982.33
    Compute (SM) Throughput           %        91.27
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.04
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.76
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.76
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       525.14
    Mem Busy                              %        50.16
    Max Bandwidth                         %        91.27
    L1/TEX Hit Rate                       %        11.37
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.45
    Mem Pipes Busy                        %        91.27
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.76
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.24
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.12
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.81%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768312.70
    Issued Instructions                             inst   2953758019
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.58
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584588
    Total DRAM Elapsed Cycles        cycle    854223872
    Average L1 Active Cycles         cycle  49023982.33
    Total L1 Elapsed Cycles          cycle   1179810976
    Average L2 Active Cycles         cycle  29515426.44
    Total L2 Elapsed Cycles          cycle    786402720
    Average SM Active Cycles         cycle  49023982.33
    Total SM Elapsed Cycles          cycle   1179810976
    Average SMSP Active Cycles       cycle  49026619.83
    Total SMSP Elapsed Cycles        cycle   4719243904
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle     49157754
    Memory Throughput                 %        91.27
    DRAM Throughput                   %         0.28
    Duration                         ms        35.24
    L1/TEX Cache Throughput           %        91.53
    L2 Cache Throughput               %         3.92
    SM Active Cycles              cycle  49020331.12
    Compute (SM) Throughput           %        91.27
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.83
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.77
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.77
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       531.51
    Mem Busy                              %        50.16
    Max Bandwidth                         %        91.27
    L1/TEX Hit Rate                       %        12.69
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.19
    Mem Pipes Busy                        %        91.27
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.76
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.24
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.12
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.79%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768395.10
    Issued Instructions                             inst   2953765930
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.59
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       585300
    Total DRAM Elapsed Cycles        cycle    845020160
    Average L1 Active Cycles         cycle  49020331.12
    Total L1 Elapsed Cycles          cycle   1179785600
    Average L2 Active Cycles         cycle  29733222.50
    Total L2 Elapsed Cycles          cycle    801600528
    Average SM Active Cycles         cycle  49020331.12
    Total SM Elapsed Cycles          cycle   1179785600
    Average SMSP Active Cycles       cycle  49029280.12
    Total SMSP Elapsed Cycles        cycle   4719142400
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.00
    Elapsed Cycles                cycle     49147777
    Memory Throughput                 %        91.29
    DRAM Throughput                   %         0.20
    Duration                         ms        48.90
    L1/TEX Cache Throughput           %        91.56
    L2 Cache Throughput               %         4.00
    SM Active Cycles              cycle  49000658.42
    Compute (SM) Throughput           %        91.29
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.32
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.79
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.79
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       387.50
    Mem Busy                              %        50.18
    Max Bandwidth                         %        91.29
    L1/TEX Hit Rate                       %        12.05
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.52
    Mem Pipes Busy                        %        91.29
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.76
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.24
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.95%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.9% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768204.83
    Issued Instructions                             inst   2953747664
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.61
    Achieved Active Warps Per SM           warp        47.81
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       592196
    Total DRAM Elapsed Cycles        cycle   1172700160
    Average L1 Active Cycles         cycle  49000658.42
    Total L1 Elapsed Cycles          cycle   1179546600
    Average L2 Active Cycles         cycle  29746690.56
    Total L2 Elapsed Cycles          cycle    798101312
    Average SM Active Cycles         cycle  49000658.42
    Total SM Elapsed Cycles          cycle   1179546600
    Average SMSP Active Cycles       cycle  49022707.72
    Total SMSP Elapsed Cycles        cycle   4718186400
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle     49165726
    Memory Throughput                 %        91.26
    DRAM Throughput                   %         0.27
    Duration                         ms        36.02
    L1/TEX Cache Throughput           %        91.51
    L2 Cache Throughput               %         3.95
    SM Active Cycles              cycle  49026380.33
    Compute (SM) Throughput           %        91.26
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.81
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.76
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.76
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       519.65
    Mem Busy                              %        50.16
    Max Bandwidth                         %        91.26
    L1/TEX Hit Rate                       %        12.83
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.74
    Mem Pipes Busy                        %        91.26
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.75
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.25
    Active Warps Per Scheduler          warp        11.95
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.83%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768334.91
    Issued Instructions                             inst   2953760151
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.58
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584912
    Total DRAM Elapsed Cycles        cycle    863731712
    Average L1 Active Cycles         cycle  49026380.33
    Total L1 Elapsed Cycles          cycle   1179977224
    Average L2 Active Cycles         cycle  29341085.31
    Total L2 Elapsed Cycles          cycle    795150432
    Average SM Active Cycles         cycle  49026380.33
    Total SM Elapsed Cycles          cycle   1179977224
    Average SMSP Active Cycles       cycle  49034106.61
    Total SMSP Elapsed Cycles        cycle   4719908896
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.27
    Elapsed Cycles                cycle     49156562
    Memory Throughput                 %        91.27
    DRAM Throughput                   %         0.25
    Duration                         ms        38.55
    L1/TEX Cache Throughput           %        91.53
    L2 Cache Throughput               %         3.98
    SM Active Cycles              cycle  49016288.67
    Compute (SM) Throughput           %        91.27
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.50
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.49
    Issue Slots Busy               %        62.77
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.77
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       486.34
    Mem Busy                              %        50.17
    Max Bandwidth                         %        91.27
    L1/TEX Hit Rate                       %        11.68
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.66
    Mem Pipes Busy                        %        91.27
    --------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.75
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.25
    Active Warps Per Scheduler          warp        11.94
    Eligible Warps Per Scheduler        warp         1.10
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.57
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 46.85%                                                                                    
          On average, each warp of this kernel spends 8.9 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 46.8% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     30645352
    Executed Instructions                           inst   2941953792
    Avg. Issued Instructions Per Scheduler          inst  30768264.40
    Issued Instructions                             inst   2953753382
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.58
    Achieved Active Warps Per SM           warp        47.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       585948
    Total DRAM Elapsed Cycles        cycle    924528640
    Average L1 Active Cycles         cycle  49016288.67
    Total L1 Elapsed Cycles          cycle   1179757152
    Average L2 Active Cycles         cycle  30406531.19
    Total L2 Elapsed Cycles          cycle    795019696
    Average SM Active Cycles         cycle  49016288.67
    Total SM Elapsed Cycles          cycle   1179757152
    Average SMSP Active Cycles       cycle  49032308.43
    Total SMSP Elapsed Cycles        cycle   4719028608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst    772959360
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                       1532
    ------------------------- ----------- ------------

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.26
    Elapsed Cycles                cycle     77813552
    Memory Throughput                 %        91.55
    DRAM Throughput                   %         0.25
    Duration                         ms        61.76
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.81
    SM Active Cycles              cycle  77724404.25
    Compute (SM) Throughput           %        91.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.09
    Dropped Samples                sample            4
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.84
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.84
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       474.96
    Mem Busy                              %        50.64
    Max Bandwidth                         %        91.55
    L1/TEX Hit Rate                       %        13.91
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.75
    Mem Pipes Busy                        %        91.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.84
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.16
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.19%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843506.70
    Issued Instructions                             inst   4688976643
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.67
    Achieved Active Warps Per SM           warp        47.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       916624
    Total DRAM Elapsed Cycles        cycle   1480931328
    Average L1 Active Cycles         cycle  77724404.25
    Total L1 Elapsed Cycles          cycle   1867367032
    Average L2 Active Cycles         cycle  56457562.31
    Total L2 Elapsed Cycles          cycle   1218370000
    Average SM Active Cycles         cycle  77724404.25
    Total SM Elapsed Cycles          cycle   1867367032
    Average SMSP Active Cycles       cycle  77727139.99
    Total SMSP Elapsed Cycles        cycle   7469468128
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 20.24%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.35
    Elapsed Cycles                cycle     77807866
    Memory Throughput                 %        91.55
    DRAM Throughput                   %         0.27
    Duration                         ms        57.64
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.72
    SM Active Cycles              cycle  77723659.79
    Compute (SM) Throughput           %        91.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.86
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         2.05
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.84
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.84
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       512.33
    Mem Busy                              %        50.64
    Max Bandwidth                         %        91.55
    L1/TEX Hit Rate                       %        13.74
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.25
    Mem Pipes Busy                        %        91.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.85
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.15
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.2%                                                                                     
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843554.36
    Issued Instructions                             inst   4688981219
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.68
    Achieved Active Warps Per SM           warp        47.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       922760
    Total DRAM Elapsed Cycles        cycle   1382098944
    Average L1 Active Cycles         cycle  77723659.79
    Total L1 Elapsed Cycles          cycle   1867388720
    Average L2 Active Cycles         cycle  56792482.75
    Total L2 Elapsed Cycles          cycle   1244915280
    Average SM Active Cycles         cycle  77723659.79
    Total SM Elapsed Cycles          cycle   1867388720
    Average SMSP Active Cycles       cycle  77720479.96
    Total SMSP Elapsed Cycles        cycle   7469554880
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.92%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.37
    Elapsed Cycles                cycle     77811552
    Memory Throughput                 %        91.54
    DRAM Throughput                   %         0.27
    Duration                         ms        56.62
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.71
    SM Active Cycles              cycle  77719569.58
    Compute (SM) Throughput           %        91.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.76
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.85
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.85
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       523.53
    Mem Busy                              %        50.64
    Max Bandwidth                         %        91.54
    L1/TEX Hit Rate                       %        13.94
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.89
    Mem Pipes Busy                        %        91.54
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.85
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.15
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.16%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843398.85
    Issued Instructions                             inst   4688966290
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.69
    Achieved Active Warps Per SM           warp        47.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       926384
    Total DRAM Elapsed Cycles        cycle   1357839872
    Average L1 Active Cycles         cycle  77719569.58
    Total L1 Elapsed Cycles          cycle   1867475592
    Average L2 Active Cycles         cycle  56713876.25
    Total L2 Elapsed Cycles          cycle   1250147104
    Average SM Active Cycles         cycle  77719569.58
    Total SM Elapsed Cycles          cycle   1867475592
    Average SMSP Active Cycles       cycle  77719892.26
    Total SMSP Elapsed Cycles        cycle   7469902368
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.81%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle     77800371
    Memory Throughput                 %        91.56
    DRAM Throughput                   %         0.27
    Duration                         ms        55.77
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.63
    SM Active Cycles              cycle  77719616.46
    Compute (SM) Throughput           %        91.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.83
    Dropped Samples                sample            7
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.85
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.85
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       526.79
    Mem Busy                              %        50.64
    Max Bandwidth                         %        91.56
    L1/TEX Hit Rate                       %        14.26
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.14
    Mem Pipes Busy                        %        91.56
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.07%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.07%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.84
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.16
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.17%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843504.40
    Issued Instructions                             inst   4688976422
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.68
    Achieved Active Warps Per SM           warp        47.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       918108
    Total DRAM Elapsed Cycles        cycle   1337385984
    Average L1 Active Cycles         cycle  77719616.46
    Total L1 Elapsed Cycles          cycle   1867208416
    Average L2 Active Cycles         cycle  56320814.69
    Total L2 Elapsed Cycles          cycle   1268873328
    Average SM Active Cycles         cycle  77719616.46
    Total SM Elapsed Cycles          cycle   1867208416
    Average SMSP Active Cycles       cycle  77728675.01
    Total SMSP Elapsed Cycles        cycle   7468833664
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.38%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.35
    Elapsed Cycles                cycle     77809853
    Memory Throughput                 %        91.55
    DRAM Throughput                   %         0.27
    Duration                         ms        57.52
    L1/TEX Cache Throughput           %        91.64
    L2 Cache Throughput               %         4.71
    SM Active Cycles              cycle  77725877.88
    Compute (SM) Throughput           %        91.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.58
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.84
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.84
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       518.69
    Mem Busy                              %        50.64
    Max Bandwidth                         %        91.55
    L1/TEX Hit Rate                       %        13.67
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.30
    Mem Pipes Busy                        %        91.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.84
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.16
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.18%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843410.14
    Issued Instructions                             inst   4688967373
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.68
    Achieved Active Warps Per SM           warp        47.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       932352
    Total DRAM Elapsed Cycles        cycle   1379325952
    Average L1 Active Cycles         cycle  77725877.88
    Total L1 Elapsed Cycles          cycle   1867436136
    Average L2 Active Cycles         cycle     56655616
    Total L2 Elapsed Cycles          cycle   1247422992
    Average SM Active Cycles         cycle  77725877.88
    Total SM Elapsed Cycles          cycle   1867436136
    Average SMSP Active Cycles       cycle  77728072.12
    Total SMSP Elapsed Cycles        cycle   7469744544
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.83%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     77804130
    Memory Throughput                 %        91.55
    DRAM Throughput                   %         0.27
    Duration                         ms        56.51
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.72
    SM Active Cycles              cycle  77720095.38
    Compute (SM) Throughput           %        91.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.94
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.85
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.85
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       524.61
    Mem Busy                              %        50.64
    Max Bandwidth                         %        91.55
    L1/TEX Hit Rate                       %        13.56
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.08
    Mem Pipes Busy                        %        91.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.84
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.16
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.15%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843424.10
    Issued Instructions                             inst   4688968714
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.68
    Achieved Active Warps Per SM           warp        47.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       926348
    Total DRAM Elapsed Cycles        cycle   1354993664
    Average L1 Active Cycles         cycle  77720095.38
    Total L1 Elapsed Cycles          cycle   1867297656
    Average L2 Active Cycles         cycle  56855529.12
    Total L2 Elapsed Cycles          cycle   1247525984
    Average SM Active Cycles         cycle  77720095.38
    Total SM Elapsed Cycles          cycle   1867297656
    Average SMSP Active Cycles       cycle  77727679.78
    Total SMSP Elapsed Cycles        cycle   7469190624
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.9%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     77797788
    Memory Throughput                 %        91.56
    DRAM Throughput                   %         0.28
    Duration                         ms        56.38
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.72
    SM Active Cycles              cycle  77724101.25
    Compute (SM) Throughput           %        91.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.22
    Dropped Samples                sample            9
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.84
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.84
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       528.57
    Mem Busy                              %        50.65
    Max Bandwidth                         %        91.56
    L1/TEX Hit Rate                       %        14.11
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.72
    Mem Pipes Busy                        %        91.56
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.07%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.07%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.84
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.16
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.15%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843487.71
    Issued Instructions                             inst   4688974820
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.68
    Achieved Active Warps Per SM           warp        47.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       931200
    Total DRAM Elapsed Cycles        cycle   1351877632
    Average L1 Active Cycles         cycle  77724101.25
    Total L1 Elapsed Cycles          cycle   1867146424
    Average L2 Active Cycles         cycle  56289478.75
    Total L2 Elapsed Cycles          cycle   1244629248
    Average SM Active Cycles         cycle  77724101.25
    Total SM Elapsed Cycles          cycle   1867146424
    Average SMSP Active Cycles       cycle  77727634.36
    Total SMSP Elapsed Cycles        cycle   7468585696
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.75%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     77805194
    Memory Throughput                 %        91.55
    DRAM Throughput                   %         0.27
    Duration                         ms        56.38
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.72
    SM Active Cycles              cycle  77723813.88
    Compute (SM) Throughput           %        91.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.65
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.84
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.84
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       524.33
    Mem Busy                              %        50.64
    Max Bandwidth                         %        91.55
    L1/TEX Hit Rate                       %        13.50
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.71
    Mem Pipes Busy                        %        91.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.84
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.16
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.16%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843583.78
    Issued Instructions                             inst   4688984043
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.68
    Achieved Active Warps Per SM           warp        47.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       923808
    Total DRAM Elapsed Cycles        cycle   1352007680
    Average L1 Active Cycles         cycle  77723813.88
    Total L1 Elapsed Cycles          cycle   1867324216
    Average L2 Active Cycles         cycle     56088894
    Total L2 Elapsed Cycles          cycle   1244774864
    Average SM Active Cycles         cycle  77723813.88
    Total SM Elapsed Cycles          cycle   1867324216
    Average SMSP Active Cycles       cycle  77725458.22
    Total SMSP Elapsed Cycles        cycle   7469296864
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.68%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     77776094
    Memory Throughput                 %        91.59
    DRAM Throughput                   %         0.27
    Duration                         ms        56.36
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.74
    SM Active Cycles              cycle  77721932.58
    Compute (SM) Throughput           %        91.59
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.99
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.84
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.84
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       525.27
    Mem Busy                              %        50.66
    Max Bandwidth                         %        91.59
    L1/TEX Hit Rate                       %        14.15
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.58
    Mem Pipes Busy                        %        91.59
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.07%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.07%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.85
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.15
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.15%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.1% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843581.49
    Issued Instructions                             inst   4688983823
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.67
    Achieved Active Warps Per SM           warp        47.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       925132
    Total DRAM Elapsed Cycles        cycle   1351499776
    Average L1 Active Cycles         cycle  77721932.58
    Total L1 Elapsed Cycles          cycle   1866625432
    Average L2 Active Cycles         cycle  55909443.31
    Total L2 Elapsed Cycles          cycle   1244285200
    Average SM Active Cycles         cycle  77721932.58
    Total SM Elapsed Cycles          cycle   1866625432
    Average SMSP Active Cycles       cycle  77720670.42
    Total SMSP Elapsed Cycles        cycle   7466501728
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.62%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle     77813946
    Memory Throughput                 %        91.54
    DRAM Throughput                   %         0.27
    Duration                         ms        56.39
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %         4.74
    SM Active Cycles              cycle  77724098.83
    Compute (SM) Throughput           %        91.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.27
    Dropped Samples                sample            3
    Maximum Sampling Interval          us           16
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.50
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.84
    Issued Ipc Active     inst/cycle         2.51
    SM Busy                        %        62.84
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       527.48
    Mem Busy                              %        50.63
    Max Bandwidth                         %        91.54
    L1/TEX Hit Rate                       %        13.72
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.94
    Mem Pipes Busy                        %        91.54
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 23.2 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.06%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.84
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.16
    Active Warps Per Scheduler          warp        11.96
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.96
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.18%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  48648807.67
    Executed Instructions                           inst   4670285536
    Avg. Issued Instructions Per Scheduler          inst  48843517.70
    Issued Instructions                             inst   4688977699
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.68
    Achieved Active Warps Per SM           warp        47.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       929464
    Total DRAM Elapsed Cycles        cycle   1352158208
    Average L1 Active Cycles         cycle  77724098.83
    Total L1 Elapsed Cycles          cycle   1867534152
    Average L2 Active Cycles         cycle  56426155.19
    Total L2 Elapsed Cycles          cycle   1244924624
    Average SM Active Cycles         cycle  77724098.83
    Total SM Elapsed Cycles          cycle   1867534152
    Average SMSP Active Cycles       cycle  77722640.83
    Total SMSP Elapsed Cycles        cycle   7470136608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1226996624
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2084.83
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 19.79%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33567075 excessive sectors (27% of the    
          total 122978925 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle    116018681
    Memory Throughput                 %        91.63
    DRAM Throughput                   %         0.28
    Duration                         ms        83.17
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.54
    SM Active Cycles              cycle 115919171.17
    Compute (SM) Throughput           %        91.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.45
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       529.79
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.63
    L1/TEX Hit Rate                       %        10.25
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.70
    Mem Pipes Busy                        %        91.63
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.28%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.3% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892099.12
    Issued Instructions                             inst   6997641516
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1376928
    Total DRAM Elapsed Cycles        cycle   1994356736
    Average L1 Active Cycles         cycle 115919171.17
    Total L1 Elapsed Cycles          cycle   2784447712
    Average L2 Active Cycles         cycle  84764330.62
    Total L2 Elapsed Cycles          cycle   1893067184
    Average SM Active Cycles         cycle 115919171.17
    Total SM Elapsed Cycles          cycle   2784447712
    Average SMSP Active Cycles       cycle 115921429.06
    Total SMSP Elapsed Cycles        cycle  11137790848
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.4%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle    116024413
    Memory Throughput                 %        91.63
    DRAM Throughput                   %         0.27
    Duration                         ms        85.14
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.60
    SM Active Cycles              cycle 115920450.17
    Compute (SM) Throughput           %        91.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.32
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       515.81
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.63
    L1/TEX Hit Rate                       %        10.10
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.48
    Mem Pipes Busy                        %        91.63
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.26%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.3% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892123.23
    Issued Instructions                             inst   6997643830
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1372432
    Total DRAM Elapsed Cycles        cycle   2041723904
    Average L1 Active Cycles         cycle 115920450.17
    Total L1 Elapsed Cycles          cycle   2784585448
    Average L2 Active Cycles         cycle  84510248.44
    Total L2 Elapsed Cycles          cycle   1873548656
    Average SM Active Cycles         cycle 115920450.17
    Total SM Elapsed Cycles          cycle   2784585448
    Average SMSP Active Cycles       cycle 115919815.06
    Total SMSP Elapsed Cycles        cycle  11138341792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.5%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle    116011735
    Memory Throughput                 %        91.64
    DRAM Throughput                   %         0.27
    Duration                         ms        83.67
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.61
    SM Active Cycles              cycle    115921588
    Compute (SM) Throughput           %        91.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       523.88
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.64
    L1/TEX Hit Rate                       %        10.46
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.50
    Mem Pipes Busy                        %        91.64
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.23%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892190.84
    Issued Instructions                             inst   6997650321
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1369732
    Total DRAM Elapsed Cycles        cycle   2006342656
    Average L1 Active Cycles         cycle    115921588
    Total L1 Elapsed Cycles          cycle   2784281288
    Average L2 Active Cycles         cycle  84515357.44
    Total L2 Elapsed Cycles          cycle   1871634288
    Average SM Active Cycles         cycle    115921588
    Total SM Elapsed Cycles          cycle   2784281288
    Average SMSP Active Cycles       cycle 115919709.02
    Total SMSP Elapsed Cycles        cycle  11137125152
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.52%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle    116010767
    Memory Throughput                 %        91.64
    DRAM Throughput                   %         0.27
    Duration                         ms        84.99
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.60
    SM Active Cycles              cycle 115919795.12
    Compute (SM) Throughput           %        91.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       514.68
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.64
    L1/TEX Hit Rate                       %        10.12
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.39
    Mem Pipes Busy                        %        91.64
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.26%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.3% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892180.32
    Issued Instructions                             inst   6997649311
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1366960
    Total DRAM Elapsed Cycles        cycle   2038049792
    Average L1 Active Cycles         cycle 115919795.12
    Total L1 Elapsed Cycles          cycle   2784258072
    Average L2 Active Cycles         cycle  84397506.69
    Total L2 Elapsed Cycles          cycle   1876373536
    Average SM Active Cycles         cycle 115919795.12
    Total SM Elapsed Cycles          cycle   2784258072
    Average SMSP Active Cycles       cycle 115919452.15
    Total SMSP Elapsed Cycles        cycle  11137032288
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.46%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    116018922
    Memory Throughput                 %        91.63
    DRAM Throughput                   %         0.27
    Duration                         ms        84.03
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.60
    SM Active Cycles              cycle 115916781.67
    Compute (SM) Throughput           %        91.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.91
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         2.05
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       526.20
    Mem Busy                              %        50.71
    Max Bandwidth                         %        91.63
    L1/TEX Hit Rate                       %        10.14
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.15
    Mem Pipes Busy                        %        91.63
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.24%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892212.55
    Issued Instructions                             inst   6997652405
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.76
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1381748
    Total DRAM Elapsed Cycles        cycle   2015004672
    Average L1 Active Cycles         cycle 115916781.67
    Total L1 Elapsed Cycles          cycle   2784453728
    Average L2 Active Cycles         cycle  83894283.50
    Total L2 Elapsed Cycles          cycle   1878524832
    Average SM Active Cycles         cycle 115916781.67
    Total SM Elapsed Cycles          cycle   2784453728
    Average SMSP Active Cycles       cycle 115917671.12
    Total SMSP Elapsed Cycles        cycle  11137814912
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.36%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle    116012478
    Memory Throughput                 %        91.64
    DRAM Throughput                   %         0.28
    Duration                         ms        83.16
    L1/TEX Cache Throughput           %        91.72
    L2 Cache Throughput               %         4.56
    SM Active Cycles              cycle 115914407.54
    Compute (SM) Throughput           %        91.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.59
    Dropped Samples                sample           10
    Maximum Sampling Interval          ms         2.05
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       535.40
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.64
    L1/TEX Hit Rate                       %        10.21
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.98
    Mem Pipes Busy                        %        91.64
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.24%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892214.10
    Issued Instructions                             inst   6997652554
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.76
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1391432
    Total DRAM Elapsed Cycles        cycle   1994251264
    Average L1 Active Cycles         cycle 115914407.54
    Total L1 Elapsed Cycles          cycle   2784298904
    Average L2 Active Cycles         cycle  83908798.94
    Total L2 Elapsed Cycles          cycle   1893232256
    Average SM Active Cycles         cycle 115914407.54
    Total SM Elapsed Cycles          cycle   2784298904
    Average SMSP Active Cycles       cycle 115927432.95
    Total SMSP Elapsed Cycles        cycle  11137195616
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.25%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.41
    Elapsed Cycles                cycle    116026255
    Memory Throughput                 %        91.63
    DRAM Throughput                   %         0.28
    Duration                         ms        82.29
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.62
    SM Active Cycles              cycle 115921244.25
    Compute (SM) Throughput           %        91.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.98
    Dropped Samples                sample            5
    Maximum Sampling Interval          ms         2.05
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       530.44
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.63
    L1/TEX Hit Rate                       %        10.33
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.70
    Mem Pipes Busy                        %        91.63
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.25%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892248.71
    Issued Instructions                             inst   6997655876
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.76
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1364024
    Total DRAM Elapsed Cycles        cycle   1973270528
    Average L1 Active Cycles         cycle 115921244.25
    Total L1 Elapsed Cycles          cycle   2784629808
    Average L2 Active Cycles         cycle  84142604.12
    Total L2 Elapsed Cycles          cycle   1872746016
    Average SM Active Cycles         cycle 115921244.25
    Total SM Elapsed Cycles          cycle   2784629808
    Average SMSP Active Cycles       cycle 115914990.28
    Total SMSP Elapsed Cycles        cycle  11138519232
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.45%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle    116026067
    Memory Throughput                 %        91.63
    DRAM Throughput                   %         0.27
    Duration                         ms        83.17
    L1/TEX Cache Throughput           %        91.72
    L2 Cache Throughput               %         4.56
    SM Active Cycles              cycle 115915167.54
    Compute (SM) Throughput           %        91.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.52
    Dropped Samples                sample            9
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       525.96
    Mem Busy                              %        50.71
    Max Bandwidth                         %        91.63
    L1/TEX Hit Rate                       %        10.16
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.38
    Mem Pipes Busy                        %        91.63
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.41%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.03
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.27%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.3% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892120.73
    Issued Instructions                             inst   6997643590
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1367044
    Total DRAM Elapsed Cycles        cycle   1994484736
    Average L1 Active Cycles         cycle 115915167.54
    Total L1 Elapsed Cycles          cycle   2784625160
    Average L2 Active Cycles         cycle  84778693.88
    Total L2 Elapsed Cycles          cycle   1893945824
    Average SM Active Cycles         cycle 115915167.54
    Total SM Elapsed Cycles          cycle   2784625160
    Average SMSP Active Cycles       cycle 115917263.56
    Total SMSP Elapsed Cycles        cycle  11138500640
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.39%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.38
    Elapsed Cycles                cycle    116006553
    Memory Throughput                 %        91.64
    DRAM Throughput                   %         0.27
    Duration                         ms        84.06
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.65
    SM Active Cycles              cycle 115924021.88
    Compute (SM) Throughput           %        91.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.11
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       525.44
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.64
    L1/TEX Hit Rate                       %        10.21
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.01
    Mem Pipes Busy                        %        91.64
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.42%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.42%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.25%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892304.67
    Issued Instructions                             inst   6997661248
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.75
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1380308
    Total DRAM Elapsed Cycles        cycle   2015823872
    Average L1 Active Cycles         cycle 115924021.88
    Total L1 Elapsed Cycles          cycle   2784156904
    Average L2 Active Cycles         cycle  84178694.94
    Total L2 Elapsed Cycles          cycle   1856027808
    Average SM Active Cycles         cycle 115924021.88
    Total SM Elapsed Cycles          cycle   2784156904
    Average SMSP Active Cycles       cycle 115924742.08
    Total SMSP Elapsed Cycles        cycle  11136627616
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.58%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  tiledMatrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle    116002305
    Memory Throughput                 %        91.65
    DRAM Throughput                   %         0.27
    Duration                         ms        84.98
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %         4.61
    SM Active Cycles              cycle 115918986.54
    Compute (SM) Throughput           %        91.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.59
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           32
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.51
    Executed Ipc Elapsed  inst/cycle         2.50
    Issue Slots Busy               %        62.88
    Issued Ipc Active     inst/cycle         2.52
    SM Busy                        %        62.88
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (34.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Mbyte/s       520.42
    Mem Busy                              %        50.72
    Max Bandwidth                         %        91.65
    L1/TEX Hit Rate                       %        10.19
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.15
    Mem Pipes Busy                        %        91.65
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 18.42%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 25.6 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.42%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        62.88
    Issued Warp Per Scheduler                        0.63
    No Eligible                            %        37.12
    Active Warps Per Scheduler          warp        11.97
    Eligible Warps Per Scheduler        warp         1.09
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        19.04
    Warp Cycles Per Executed Instruction           cycle        19.11
    Avg. Active Threads Per Warp                                31.97
    Avg. Not Predicated Off Threads Per Warp                    31.58
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 47.23%                                                                                    
          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a branch target to be        
          computed, and the warp program counter to be updated. To reduce the number of stalled cycles, consider using  
          fewer jump/branch operations and reduce control flow divergence, e.g. by reducing or coalescing conditionals  
          in your code. See also the related No Instructions state. This stall type represents about 47.2% of the       
          total average of 19.0 cycles between issuing two instructions.                                                
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  72601978.67
    Executed Instructions                           inst   6969789952
    Avg. Issued Instructions Per Scheduler          inst  72892207.96
    Issued Instructions                             inst   6997651964
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.76
    Achieved Active Warps Per SM           warp        47.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1382084
    Total DRAM Elapsed Cycles        cycle   2037900288
    Average L1 Active Cycles         cycle 115918986.54
    Total L1 Elapsed Cycles          cycle   2784055208
    Average L2 Active Cycles         cycle  83440623.38
    Total L2 Elapsed Cycles          cycle   1876370224
    Average SM Active Cycles         cycle 115918986.54
    Total SM Elapsed Cycles          cycle   2784055208
    Average SMSP Active Cycles       cycle 115918651.23
    Total SMSP Elapsed Cycles        cycle  11136220832
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.26
    Branch Instructions              inst   1831065856
    Branch Efficiency                   %        99.98
    Avg. Divergent Branches                    2722.67
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 14.3%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 33488128 excessive sectors (20% of the    
          total 166655762 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

