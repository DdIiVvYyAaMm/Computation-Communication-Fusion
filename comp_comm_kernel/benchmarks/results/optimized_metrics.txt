==PROF== Connected to process 11211 (/usr/bin/python3.12)
==LOG== Target process 11281 terminated before first instrumented API call.
==LOG== Target process 11300 terminated before first instrumented API call.
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%
==WARNING== Launching the workload is taking more time than expected. If this continues to hang, terminate the profile and re-try by profiling the range of all related launches using '--replay-mode range'. See https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#replay for more details.
....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 38 passes
==PROF== Profiling "matrixMulKernel": 0%....50%....100% - 39 passes
running l2 cache tests for ../optimized_gemm.cubin
==PROF== Disconnected from process 11211
[11211] python3.12@127.0.0.1
  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       776973
    Memory Throughput                 %        90.25
    DRAM Throughput                   %         3.23
    Duration                         us       338.85
    L1/TEX Cache Throughput           %        91.17
    L2 Cache Throughput               %        11.56
    SM Active Cycles              cycle    769024.71
    Compute (SM) Throughput           %        90.25
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.10
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         2.00
    Issue Slots Busy               %        50.46
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.89
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.20
    Mem Busy                              %        67.74
    Max Bandwidth                         %        90.25
    L1/TEX Hit Rate                       %        89.28
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.35
    Mem Pipes Busy                        %        90.25
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.48%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.27
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.73
    Active Warps Per Scheduler          warp        11.37
    Eligible Warps Per Scheduler        warp         2.27
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.753%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.37 active warps per scheduler, but only an average of 2.27 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.62
    Warp Cycles Per Executed Instruction           cycle        22.62
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.753%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.4% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.15
    Issued Instructions                             inst     37249550
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.25
    Achieved Active Warps Per SM           warp        45.72
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8124416
    Average L1 Active Cycles         cycle    769024.71
    Total L1 Elapsed Cycles          cycle     18644856
    Average L2 Active Cycles         cycle    653229.25
    Total L2 Elapsed Cycles          cycle     11053904
    Average SM Active Cycles         cycle    769024.71
    Total SM Elapsed Cycles          cycle     18644856
    Average SMSP Active Cycles       cycle    771790.28
    Total SMSP Elapsed Cycles        cycle     74579424
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle       778856
    Memory Throughput                 %        90.03
    DRAM Throughput                   %         3.16
    Duration                         us       346.50
    L1/TEX Cache Throughput           %        91.23
    L2 Cache Throughput               %        11.37
    SM Active Cycles              cycle    768512.38
    Compute (SM) Throughput           %        90.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         2.10
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.49
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.92
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.07
    Mem Busy                              %        67.58
    Max Bandwidth                         %        90.03
    L1/TEX Hit Rate                       %        89.33
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.25
    Mem Pipes Busy                        %        90.03
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.39%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.53
    Issued Warp Per Scheduler                        0.51
    No Eligible                            %        49.47
    Active Warps Per Scheduler          warp        11.41
    Eligible Warps Per Scheduler        warp         2.28
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.973%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.41 active warps per scheduler, but only an average of 2.28 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.58
    Warp Cycles Per Executed Instruction           cycle        22.58
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.973%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.4% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.33
    Issued Instructions                             inst     37249568
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.03
    Achieved Active Warps Per SM           warp        45.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8308736
    Average L1 Active Cycles         cycle    768512.38
    Total L1 Elapsed Cycles          cycle     18690456
    Average L2 Active Cycles         cycle    653554.12
    Total L2 Elapsed Cycles          cycle     11208416
    Average SM Active Cycles         cycle    768512.38
    Total SM Elapsed Cycles          cycle     18690456
    Average SMSP Active Cycles       cycle    767884.05
    Total SMSP Elapsed Cycles        cycle     74761824
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       777410
    Memory Throughput                 %        90.19
    DRAM Throughput                   %         3.23
    Duration                         us       338.98
    L1/TEX Cache Throughput           %        91.35
    L2 Cache Throughput               %        11.42
    SM Active Cycles              cycle    767484.08
    Compute (SM) Throughput           %        90.19
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.71
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         2.00
    Issue Slots Busy               %        50.56
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        58.00
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.20
    Mem Busy                              %        67.71
    Max Bandwidth                         %        90.19
    L1/TEX Hit Rate                       %        89.25
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.69
    Mem Pipes Busy                        %        90.19
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.28
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.72
    Active Warps Per Scheduler          warp        11.38
    Eligible Warps Per Scheduler        warp         2.27
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.805%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.38 active warps per scheduler, but only an average of 2.27 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.63
    Warp Cycles Per Executed Instruction           cycle        22.63
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.805%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 32.7% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.22
    Issued Instructions                             inst     37249557
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.50
    Achieved Active Warps Per SM           warp        45.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8126464
    Average L1 Active Cycles         cycle    767484.08
    Total L1 Elapsed Cycles          cycle     18655632
    Average L2 Active Cycles         cycle    659133.81
    Total L2 Elapsed Cycles          cycle     11133888
    Average SM Active Cycles         cycle    767484.08
    Total SM Elapsed Cycles          cycle     18655632
    Average SMSP Active Cycles       cycle    771712.34
    Total SMSP Elapsed Cycles        cycle     74622528
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       778598
    Memory Throughput                 %        90.06
    DRAM Throughput                   %         3.23
    Duration                         us       339.52
    L1/TEX Cache Throughput           %        91.10
    L2 Cache Throughput               %        11.46
    SM Active Cycles              cycle    769626.92
    Compute (SM) Throughput           %        90.06
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.42
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.84
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.19
    Mem Busy                              %        67.60
    Max Bandwidth                         %        90.06
    L1/TEX Hit Rate                       %        89.34
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.74
    Mem Pipes Busy                        %        90.06
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.4%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.34
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.66
    Active Warps Per Scheduler          warp        11.38
    Eligible Warps Per Scheduler        warp         2.26
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 9.944%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.38 active warps per scheduler, but only an average of 2.26 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.61
    Warp Cycles Per Executed Instruction           cycle        22.61
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 9.944%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.0% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.24
    Issued Instructions                             inst     37249559
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.91
    Achieved Active Warps Per SM           warp        45.55
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8140800
    Average L1 Active Cycles         cycle    769626.92
    Total L1 Elapsed Cycles          cycle     18684320
    Average L2 Active Cycles         cycle    656940.12
    Total L2 Elapsed Cycles          cycle     11150800
    Average SM Active Cycles         cycle    769626.92
    Total SM Elapsed Cycles          cycle     18684320
    Average SMSP Active Cycles       cycle    770819.85
    Total SMSP Elapsed Cycles        cycle     74737280
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       779902
    Memory Throughput                 %        89.90
    DRAM Throughput                   %         3.22
    Duration                         us       340.06
    L1/TEX Cache Throughput           %        91.20
    L2 Cache Throughput               %        11.38
    SM Active Cycles              cycle       768717
    Compute (SM) Throughput           %        89.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.48
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.91
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.18
    Mem Busy                              %        67.49
    Max Bandwidth                         %        89.90
    L1/TEX Hit Rate                       %        89.41
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.27
    Mem Pipes Busy                        %        89.90
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.33%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp        11.42
    Eligible Warps Per Scheduler        warp         2.27
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.1%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.42 active warps per scheduler, but only an average of 2.27 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.62
    Warp Cycles Per Executed Instruction           cycle        22.62
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.1%                                                                                           
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 32.5% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.23
    Issued Instructions                             inst     37249558
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.96
    Achieved Active Warps Per SM           warp        45.58
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8154112
    Average L1 Active Cycles         cycle       768717
    Total L1 Elapsed Cycles          cycle     18715888
    Average L2 Active Cycles         cycle       659817
    Total L2 Elapsed Cycles          cycle     11169152
    Average SM Active Cycles         cycle       768717
    Total SM Elapsed Cycles          cycle     18715888
    Average SMSP Active Cycles       cycle    768713.03
    Total SMSP Elapsed Cycles        cycle     74863552
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       779991
    Memory Throughput                 %        89.90
    DRAM Throughput                   %         3.22
    Duration                         us       340.35
    L1/TEX Cache Throughput           %        90.94
    L2 Cache Throughput               %        11.47
    SM Active Cycles              cycle    770924.62
    Compute (SM) Throughput           %        89.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.01
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.33
    Issued Ipc Active     inst/cycle         2.01
    SM Busy                        %        57.74
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.17
    Mem Busy                              %        67.49
    Max Bandwidth                         %        89.90
    L1/TEX Hit Rate                       %        89.34
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.03
    Mem Pipes Busy                        %        89.90
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.33%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.41
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.59
    Active Warps Per Scheduler          warp        11.40
    Eligible Warps Per Scheduler        warp         2.27
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.1%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.40 active warps per scheduler, but only an average of 2.27 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.62
    Warp Cycles Per Executed Instruction           cycle        22.63
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.1%                                                                                           
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.1% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.35
    Issued Instructions                             inst     37249570
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.58
    Achieved Active Warps Per SM           warp        45.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8159232
    Average L1 Active Cycles         cycle    770924.62
    Total L1 Elapsed Cycles          cycle     18715776
    Average L2 Active Cycles         cycle    658029.81
    Total L2 Elapsed Cycles          cycle     11155136
    Average SM Active Cycles         cycle    770924.62
    Total SM Elapsed Cycles          cycle     18715776
    Average SMSP Active Cycles       cycle    769667.42
    Total SMSP Elapsed Cycles        cycle     74863104
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       780074
    Memory Throughput                 %        89.90
    DRAM Throughput                   %         3.22
    Duration                         us       340.38
    L1/TEX Cache Throughput           %        90.98
    L2 Cache Throughput               %        11.32
    SM Active Cycles              cycle    770641.17
    Compute (SM) Throughput           %        89.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.78
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.01
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.35
    Issued Ipc Active     inst/cycle         2.01
    SM Busy                        %        57.77
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.17
    Mem Busy                              %        67.48
    Max Bandwidth                         %        89.90
    L1/TEX Hit Rate                       %        89.33
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.23
    Mem Pipes Busy                        %        89.90
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.33%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.39
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.61
    Active Warps Per Scheduler          warp        11.40
    Eligible Warps Per Scheduler        warp         2.26
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.1%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.40 active warps per scheduler, but only an average of 2.26 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.62
    Warp Cycles Per Executed Instruction           cycle        22.63
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.1%                                                                                           
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 32.8% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.06
    Issued Instructions                             inst     37249542
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.79
    Achieved Active Warps Per SM           warp        45.50
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8161280
    Average L1 Active Cycles         cycle    770641.17
    Total L1 Elapsed Cycles          cycle     18717392
    Average L2 Active Cycles         cycle    657295.88
    Total L2 Elapsed Cycles          cycle     11155296
    Average SM Active Cycles         cycle    770641.17
    Total SM Elapsed Cycles          cycle     18717392
    Average SMSP Active Cycles       cycle    770067.27
    Total SMSP Elapsed Cycles        cycle     74869568
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       779578
    Memory Throughput                 %        89.94
    DRAM Throughput                   %         3.22
    Duration                         us       339.90
    L1/TEX Cache Throughput           %        91.03
    L2 Cache Throughput               %        11.44
    SM Active Cycles              cycle    770219.42
    Compute (SM) Throughput           %        89.94
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.65
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.01
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.38
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.79
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.18
    Mem Busy                              %        67.52
    Max Bandwidth                         %        89.94
    L1/TEX Hit Rate                       %        89.35
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.60
    Mem Pipes Busy                        %        89.94
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.35%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.31
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.69
    Active Warps Per Scheduler          warp        11.35
    Eligible Warps Per Scheduler        warp         2.27
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.06%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.35 active warps per scheduler, but only an average of 2.27 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.57
    Warp Cycles Per Executed Instruction           cycle        22.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.06%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.1% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.22
    Issued Instructions                             inst     37249557
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.82
    Achieved Active Warps Per SM           warp        45.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8148992
    Average L1 Active Cycles         cycle    770219.42
    Total L1 Elapsed Cycles          cycle     18708136
    Average L2 Active Cycles         cycle    660734.38
    Total L2 Elapsed Cycles          cycle     11164400
    Average SM Active Cycles         cycle    770219.42
    Total SM Elapsed Cycles          cycle     18708136
    Average SMSP Active Cycles       cycle    771314.71
    Total SMSP Elapsed Cycles        cycle     74832544
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       780601
    Memory Throughput                 %        89.82
    DRAM Throughput                   %         3.22
    Duration                         us       340.35
    L1/TEX Cache Throughput           %        91.20
    L2 Cache Throughput               %        11.48
    SM Active Cycles              cycle    768708.42
    Compute (SM) Throughput           %        89.82
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.48
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.91
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.3%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.17
    Mem Busy                              %        67.43
    Max Bandwidth                         %        89.82
    L1/TEX Hit Rate                       %        89.35
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        95.45
    Mem Pipes Busy                        %        89.82
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.3%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.28
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.72
    Active Warps Per Scheduler          warp        11.35
    Eligible Warps Per Scheduler        warp         2.27
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.18%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.35 active warps per scheduler, but only an average of 2.27 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.57
    Warp Cycles Per Executed Instruction           cycle        22.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.18%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 32.6% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.53
    Issued Instructions                             inst     37249587
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.01
    Achieved Active Warps Per SM           warp        45.60
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8161280
    Average L1 Active Cycles         cycle    768708.42
    Total L1 Elapsed Cycles          cycle     18732480
    Average L2 Active Cycles         cycle    657234.50
    Total L2 Elapsed Cycles          cycle     11178816
    Average SM Active Cycles         cycle    768708.42
    Total SM Elapsed Cycles          cycle     18732480
    Average SMSP Active Cycles       cycle    771753.52
    Total SMSP Elapsed Cycles        cycle     74929920
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (32, 32, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       779186
    Memory Throughput                 %        89.99
    DRAM Throughput                   %         3.22
    Duration                         us       339.78
    L1/TEX Cache Throughput           %        91.12
    L2 Cache Throughput               %        11.41
    SM Active Cycles              cycle    769438.17
    Compute (SM) Throughput           %        89.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.99
    Issue Slots Busy               %        50.43
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        57.85
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (34.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         6.19
    Mem Busy                              %        67.55
    Max Bandwidth                         %        89.99
    L1/TEX Hit Rate                       %        89.40
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.94
    Mem Pipes Busy                        %        89.99
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 39.37%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.39
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.61
    Active Warps Per Scheduler          warp        11.39
    Eligible Warps Per Scheduler        warp         2.26
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 10.01%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.39 active warps per scheduler, but only an average of 2.26 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.60
    Warp Cycles Per Executed Instruction           cycle        22.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    31.95
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.01%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 33.2% of the total average of 22.6 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    387925.33
    Executed Instructions                           inst     37240832
    Avg. Issued Instructions Per Scheduler          inst    388016.21
    Issued Instructions                             inst     37249556
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                7.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.99
    Achieved Active Warps Per SM           warp        45.60
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        65676
    Total DRAM Elapsed Cycles        cycle      8146944
    Average L1 Active Cycles         cycle    769438.17
    Total L1 Elapsed Cycles          cycle     18697776
    Average L2 Active Cycles         cycle    660553.88
    Total L2 Elapsed Cycles          cycle     11158720
    Average SM Active Cycles         cycle    769438.17
    Total SM Elapsed Cycles          cycle     18697776
    Average SMSP Active Cycles       cycle    769991.42
    Total SMSP Elapsed Cycles        cycle     74791104
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      1220608
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle      2517847
    Memory Throughput                 %        93.40
    DRAM Throughput                   %         2.22
    Duration                         ms         1.10
    L1/TEX Cache Throughput           %        93.60
    L2 Cache Throughput               %        15.55
    SM Active Cycles              cycle   2512406.58
    Compute (SM) Throughput           %        93.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         4.19
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.06
    Issue Slots Busy               %        51.60
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.38
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.25
    Mem Busy                              %        54.41
    Max Bandwidth                         %        93.40
    L1/TEX Hit Rate                       %        88.21
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.75
    Mem Pipes Busy                        %        93.40
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.21%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.51%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.59
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.41
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.51
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.597%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.51 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.71
    Warp Cycles Per Executed Instruction           cycle        22.72
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.25
    Issued Instructions                             inst    124451640
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.68
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146836
    Total DRAM Elapsed Cycles        cycle     26485248
    Average L1 Active Cycles         cycle   2512406.58
    Total L1 Elapsed Cycles          cycle     60425648
    Average L2 Active Cycles         cycle   2200007.56
    Total L2 Elapsed Cycles          cycle     36042720
    Average SM Active Cycles         cycle   2512406.58
    Total SM Elapsed Cycles          cycle     60425648
    Average SMSP Active Cycles       cycle   2512645.02
    Total SMSP Elapsed Cycles        cycle    241702592
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03994%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      2520104
    Memory Throughput                 %        93.32
    DRAM Throughput                   %         2.23
    Duration                         ms         1.10
    L1/TEX Cache Throughput           %        93.57
    L2 Cache Throughput               %        15.52
    SM Active Cycles              cycle   2513137.29
    Compute (SM) Throughput           %        93.32
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         6.36
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.06
    Executed Ipc Elapsed  inst/cycle         2.06
    Issue Slots Busy               %        51.58
    Issued Ipc Active     inst/cycle         2.06
    SM Busy                        %        59.36
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         4.28
    Mem Busy                              %        54.40
    Max Bandwidth                         %        93.32
    L1/TEX Hit Rate                       %        88.29
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.83
    Mem Pipes Busy                        %        93.32
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.16%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.48%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.60
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.40
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.52
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.682%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.52 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.70
    Warp Cycles Per Executed Instruction           cycle        22.71
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.22
    Issued Instructions                             inst    124451637
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.55
    Achieved Active Warps Per SM           warp        46.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146828
    Total DRAM Elapsed Cycles        cycle     26335744
    Average L1 Active Cycles         cycle   2513137.29
    Total L1 Elapsed Cycles          cycle     60480168
    Average L2 Active Cycles         cycle   2202037.25
    Total L2 Elapsed Cycles          cycle     36099600
    Average SM Active Cycles         cycle   2513137.29
    Total SM Elapsed Cycles          cycle     60480168
    Average SMSP Active Cycles       cycle   2512386.94
    Total SMSP Elapsed Cycles        cycle    241920672
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03991%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       780.02
    Elapsed Cycles                cycle      2502250
    Memory Throughput                 %        93.99
    DRAM Throughput                   %         0.76
    Duration                         ms         3.21
    L1/TEX Cache Throughput           %        94.23
    L2 Cache Throughput               %        14.30
    SM Active Cycles              cycle   2495523.50
    Compute (SM) Throughput           %        93.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         7.73
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.95
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.84
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.46
    Mem Busy                              %        55.48
    Max Bandwidth                         %        93.99
    L1/TEX Hit Rate                       %        87.22
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.71
    Mem Pipes Busy                        %        93.99
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.52%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.67%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.00
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.00
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.012%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.53
    Warp Cycles Per Executed Instruction           cycle        22.53
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.012%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.4% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.29
    Issued Instructions                             inst    124451644
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.55
    Achieved Active Warps Per SM           warp        46.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146828
    Total DRAM Elapsed Cycles        cycle     76916736
    Average L1 Active Cycles         cycle   2495523.50
    Total L1 Elapsed Cycles          cycle     60049080
    Average L2 Active Cycles         cycle   2587025.62
    Total L2 Elapsed Cycles          cycle     42339936
    Average SM Active Cycles         cycle   2495523.50
    Total SM Elapsed Cycles          cycle     60049080
    Average SMSP Active Cycles       cycle   2492841.75
    Total SMSP Elapsed Cycles        cycle    240196320
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03998%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.20
    Elapsed Cycles                cycle      2502520
    Memory Throughput                 %        93.97
    DRAM Throughput                   %         1.17
    Duration                         ms         2.09
    L1/TEX Cache Throughput           %        94.26
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   2494844.08
    Compute (SM) Throughput           %        93.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.96
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.83
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.25
    Mem Busy                              %        55.27
    Max Bandwidth                         %        93.97
    L1/TEX Hit Rate                       %        87.61
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.37
    Mem Pipes Busy                        %        93.97
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.51%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.66%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.029%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.55
    Warp Cycles Per Executed Instruction           cycle        22.55
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.029%                                                                                          
          On average, each warp of this kernel spends 7.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.8% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296370.98
    Issued Instructions                             inst    124451614
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.64
    Achieved Active Warps Per SM           warp        46.87
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     50008064
    Average L1 Active Cycles         cycle   2494844.08
    Total L1 Elapsed Cycles          cycle     60059840
    Average L2 Active Cycles         cycle   2408023.94
    Total L2 Elapsed Cycles          cycle     39537840
    Average SM Active Cycles         cycle   2494844.08
    Total SM Elapsed Cycles          cycle     60059840
    Average SMSP Active Cycles       cycle   2493268.76
    Total SMSP Elapsed Cycles        cycle    240239360
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03985%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      2502218
    Memory Throughput                 %        93.99
    DRAM Throughput                   %         1.47
    Duration                         ms         1.67
    L1/TEX Cache Throughput           %        94.23
    L2 Cache Throughput               %        14.49
    SM Active Cycles              cycle   2495511.08
    Compute (SM) Throughput           %        93.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.95
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.80
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.82
    Mem Busy                              %        55.16
    Max Bandwidth                         %        93.99
    L1/TEX Hit Rate                       %        87.64
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.80
    Mem Pipes Busy                        %        93.99
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.51%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.67%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.71
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.015%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.71 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.54
    Warp Cycles Per Executed Instruction           cycle        22.54
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.015%                                                                                          
          On average, each warp of this kernel spends 7.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.5% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.06
    Issued Instructions                             inst    124451622
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.61
    Achieved Active Warps Per SM           warp        46.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146844
    Total DRAM Elapsed Cycles        cycle     40005632
    Average L1 Active Cycles         cycle   2495511.08
    Total L1 Elapsed Cycles          cycle     60051000
    Average L2 Active Cycles         cycle   2448110.44
    Total L2 Elapsed Cycles          cycle     40426768
    Average SM Active Cycles         cycle   2495511.08
    Total SM Elapsed Cycles          cycle     60051000
    Average SMSP Active Cycles       cycle   2494071.28
    Total SMSP Elapsed Cycles        cycle    240204000
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03962%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      2505470
    Memory Throughput                 %        93.86
    DRAM Throughput                   %         1.47
    Duration                         ms         1.67
    L1/TEX Cache Throughput           %        94.18
    L2 Cache Throughput               %        14.44
    SM Active Cycles              cycle   2496900.33
    Compute (SM) Throughput           %        93.86
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.92
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.77
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.81
    Mem Busy                              %        55.08
    Max Bandwidth                         %        93.86
    L1/TEX Hit Rate                       %        87.70
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.18
    Mem Pipes Busy                        %        93.86
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.45%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.63%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.71
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.137%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.71 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.54
    Warp Cycles Per Executed Instruction           cycle        22.54
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.137%                                                                                          
          On average, each warp of this kernel spends 7.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.5% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.42
    Issued Instructions                             inst    124451656
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.56
    Achieved Active Warps Per SM           warp        46.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     40057856
    Average L1 Active Cycles         cycle   2496900.33
    Total L1 Elapsed Cycles          cycle     60129384
    Average L2 Active Cycles         cycle   2447746.50
    Total L2 Elapsed Cycles          cycle     40481344
    Average SM Active Cycles         cycle   2496900.33
    Total SM Elapsed Cycles          cycle     60129384
    Average SMSP Active Cycles       cycle   2494143.14
    Total SMSP Elapsed Cycles        cycle    240517536
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03956%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      2503149
    Memory Throughput                 %        93.95
    DRAM Throughput                   %         1.56
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        94.19
    L2 Cache Throughput               %        14.55
    SM Active Cycles              cycle   2496685.17
    Compute (SM) Throughput           %        93.95
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us           64
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.92
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.77
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.98
    Mem Busy                              %        55.10
    Max Bandwidth                         %        93.95
    L1/TEX Hit Rate                       %        87.73
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.06
    Mem Pipes Busy                        %        93.95
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.5%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.66%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.96
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.04
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.69
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.046%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.69 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.55
    Warp Cycles Per Executed Instruction           cycle        22.55
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.046%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.2% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.24
    Issued Instructions                             inst    124451639
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.59
    Achieved Active Warps Per SM           warp        46.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146828
    Total DRAM Elapsed Cycles        cycle     37757952
    Average L1 Active Cycles         cycle   2496685.17
    Total L1 Elapsed Cycles          cycle     60071152
    Average L2 Active Cycles         cycle   2419674.12
    Total L2 Elapsed Cycles          cycle     39997456
    Average SM Active Cycles         cycle   2496685.17
    Total SM Elapsed Cycles          cycle     60071152
    Average SMSP Active Cycles       cycle   2494774.05
    Total SMSP Elapsed Cycles        cycle    240284608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03958%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle      2502726
    Memory Throughput                 %        93.97
    DRAM Throughput                   %         1.57
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        94.21
    L2 Cache Throughput               %        14.53
    SM Active Cycles              cycle   2496038.08
    Compute (SM) Throughput           %        93.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.94
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.79
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         3.01
    Mem Busy                              %        55.11
    Max Bandwidth                         %        93.97
    L1/TEX Hit Rate                       %        87.73
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.49
    Mem Pipes Busy                        %        93.97
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.51%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.66%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.95
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.05
    Active Warps Per Scheduler          warp        11.71
    Eligible Warps Per Scheduler        warp         2.70
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 6.03%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.71 active warps per scheduler, but only an average of 2.70 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.55
    Warp Cycles Per Executed Instruction           cycle        22.55
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.03%                                                                                           
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.2% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.36
    Issued Instructions                             inst    124451651
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.61
    Achieved Active Warps Per SM           warp        46.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       147980
    Total DRAM Elapsed Cycles        cycle     37751808
    Average L1 Active Cycles         cycle   2496038.08
    Total L1 Elapsed Cycles          cycle     60060464
    Average L2 Active Cycles         cycle   2421189.62
    Total L2 Elapsed Cycles          cycle     39982032
    Average SM Active Cycles         cycle   2496038.08
    Total SM Elapsed Cycles          cycle     60060464
    Average SMSP Active Cycles       cycle   2495388.33
    Total SMSP Elapsed Cycles        cycle    240241856
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03962%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.69
    Elapsed Cycles                cycle      2500919
    Memory Throughput                 %        94.03
    DRAM Throughput                   %         1.66
    Duration                         ms         1.48
    L1/TEX Cache Throughput           %        94.22
    L2 Cache Throughput               %        14.72
    SM Active Cycles              cycle   2495878.83
    Compute (SM) Throughput           %        94.03
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.94
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.78
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         3.18
    Mem Busy                              %        55.12
    Max Bandwidth                         %        94.03
    L1/TEX Hit Rate                       %        87.81
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.18
    Mem Pipes Busy                        %        94.03
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.54%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.68%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.95
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.05
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.966%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.55
    Warp Cycles Per Executed Instruction           cycle        22.56
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.966%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.9% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.21
    Issued Instructions                             inst    124451636
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.66
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146832
    Total DRAM Elapsed Cycles        cycle     35384320
    Average L1 Active Cycles         cycle   2495878.83
    Total L1 Elapsed Cycles          cycle     60019592
    Average L2 Active Cycles         cycle   2383931.44
    Total L2 Elapsed Cycles          cycle     39301136
    Average SM Active Cycles         cycle   2495878.83
    Total SM Elapsed Cycles          cycle     60019592
    Average SMSP Active Cycles       cycle   2495194.10
    Total SMSP Elapsed Cycles        cycle    240078368
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03969%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (48, 48, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.69
    Elapsed Cycles                cycle      2500471
    Memory Throughput                 %        94.05
    DRAM Throughput                   %         1.66
    Duration                         ms         1.48
    L1/TEX Cache Throughput           %        94.24
    L2 Cache Throughput               %        14.73
    SM Active Cycles              cycle   2495354.38
    Compute (SM) Throughput           %        94.05
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.95
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.80
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         3.18
    Mem Busy                              %        55.11
    Max Bandwidth                         %        94.05
    L1/TEX Hit Rate                       %        87.86
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.27
    Mem Pipes Busy                        %        94.05
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.55%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.68%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.3 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.96
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.04
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.949%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.56
    Warp Cycles Per Executed Instruction           cycle        22.56
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    31.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.949%                                                                                          
          On average, each warp of this kernel spends 7.0 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.9% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1296280.50
    Executed Instructions                           inst    124442928
    Avg. Issued Instructions Per Scheduler          inst   1296371.41
    Issued Instructions                             inst    124451655
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2304
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread          589824
    Uses Green Context                                             0
    Waves Per SM                                                  16
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.67
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       146828
    Total DRAM Elapsed Cycles        cycle     35379200
    Average L1 Active Cycles         cycle   2495354.38
    Total L1 Elapsed Cycles          cycle     60008856
    Average L2 Active Cycles         cycle   2382177.62
    Total L2 Elapsed Cycles          cycle     39293472
    Average SM Active Cycles         cycle   2495354.38
    Total SM Elapsed Cycles          cycle     60008856
    Average SMSP Active Cycles       cycle   2495143.82
    Total SMSP Elapsed Cycles        cycle    240035424
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      3898032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03967%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 27361 excessive sectors (0% of the total  
          66908736 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.  
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.00
    SM Frequency                    Ghz         1.69
    Elapsed Cycles                cycle      5903844
    Memory Throughput                 %        94.13
    DRAM Throughput                   %         1.25
    Duration                         ms         3.48
    L1/TEX Cache Throughput           %        94.28
    L2 Cache Throughput               %        14.65
    SM Active Cycles              cycle   5894416.58
    Compute (SM) Throughput           %        94.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.39
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.07
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.86
    Issued Ipc Active     inst/cycle         2.07
    SM Busy                        %        59.72
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.39
    Mem Busy                              %        66.28
    Max Bandwidth                         %        94.13
    L1/TEX Hit Rate                       %        87.49
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.94
    Mem Pipes Busy                        %        94.13
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.88%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.88
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.12
    Active Warps Per Scheduler          warp        11.77
    Eligible Warps Per Scheduler        warp         2.56
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.873%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.77 active warps per scheduler, but only an average of 2.56 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.69
    Warp Cycles Per Executed Instruction           cycle        22.69
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.873%                                                                                          
          On average, each warp of this kernel spends 6.9 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 30.4% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057076.27
    Issued Instructions                             inst    293479322
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.06
    Achieved Active Warps Per SM           warp        47.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle     83529728
    Average L1 Active Cycles         cycle   5894416.58
    Total L1 Elapsed Cycles          cycle    141689280
    Average L2 Active Cycles         cycle   5696805.44
    Total L2 Elapsed Cycles          cycle     92783072
    Average SM Active Cycles         cycle   5894416.58
    Total SM Elapsed Cycles          cycle    141689280
    Average SMSP Active Cycles       cycle   5892195.18
    Total SMSP Elapsed Cycles        cycle    566757120
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02142%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.03
    Elapsed Cycles                cycle      5902598
    Memory Throughput                 %        94.14
    DRAM Throughput                   %         0.76
    Duration                         ms         5.70
    L1/TEX Cache Throughput           %        94.41
    L2 Cache Throughput               %        15.41
    SM Active Cycles              cycle   5885887.33
    Compute (SM) Throughput           %        94.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.00
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.94
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.79
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.46
    Mem Busy                              %        67.98
    Max Bandwidth                         %        94.14
    L1/TEX Hit Rate                       %        86.56
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.05
    Mem Pipes Busy                        %        94.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.67
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.855%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.67 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.72
    Warp Cycles Per Executed Instruction           cycle        22.72
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.855%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.8% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057076.18
    Issued Instructions                             inst    293479313
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.33
    Achieved Active Warps Per SM           warp        47.20
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260364
    Total DRAM Elapsed Cycles        cycle    136757248
    Average L1 Active Cycles         cycle   5885887.33
    Total L1 Elapsed Cycles          cycle    141662304
    Average L2 Active Cycles         cycle   5824106.06
    Total L2 Elapsed Cycles          cycle     95810256
    Average SM Active Cycles         cycle   5885887.33
    Total SM Elapsed Cycles          cycle    141662304
    Average SMSP Active Cycles       cycle   5881614.79
    Total SMSP Elapsed Cycles        cycle    566649216
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0212%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.30
    Elapsed Cycles                cycle      5902744
    Memory Throughput                 %        94.14
    DRAM Throughput                   %         0.96
    Duration                         ms         4.52
    L1/TEX Cache Throughput           %        94.38
    L2 Cache Throughput               %        15.14
    SM Active Cycles              cycle   5887788.21
    Compute (SM) Throughput           %        94.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.55
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.92
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.78
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.84
    Mem Busy                              %        67.12
    Max Bandwidth                         %        94.14
    L1/TEX Hit Rate                       %        86.83
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.62
    Mem Pipes Busy                        %        94.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.96
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.04
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.65
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.857%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.65 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.71
    Warp Cycles Per Executed Instruction           cycle        22.71
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.857%                                                                                          
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.3% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057076.03
    Issued Instructions                             inst    293479299
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.25
    Achieved Active Warps Per SM           warp        47.16
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260364
    Total DRAM Elapsed Cycles        cycle    108466176
    Average L1 Active Cycles         cycle   5887788.21
    Total L1 Elapsed Cycles          cycle    141665328
    Average L2 Active Cycles         cycle   5881265.94
    Total L2 Elapsed Cycles          cycle     95525776
    Average SM Active Cycles         cycle   5887788.21
    Total SM Elapsed Cycles          cycle    141665328
    Average SMSP Active Cycles       cycle   5884059.18
    Total SMSP Elapsed Cycles        cycle    566661312
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02148%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      5901071
    Memory Throughput                 %        94.17
    DRAM Throughput                   %         1.10
    Duration                         ms         3.93
    L1/TEX Cache Throughput           %        94.27
    L2 Cache Throughput               %        14.76
    SM Active Cycles              cycle   5894751.21
    Compute (SM) Throughput           %        94.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.50
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.07
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.86
    Issued Ipc Active     inst/cycle         2.07
    SM Busy                        %        59.71
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.12
    Mem Busy                              %        66.64
    Max Bandwidth                         %        94.17
    L1/TEX Hit Rate                       %        87.10
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.03
    Mem Pipes Busy                        %        94.17
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.17%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.92
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.08
    Active Warps Per Scheduler          warp        11.78
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.83%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.78 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.69
    Warp Cycles Per Executed Instruction           cycle        22.69
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.83%                                                                                           
          On average, each warp of this kernel spends 7.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.6% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057076.25
    Issued Instructions                             inst    293479320
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.14
    Achieved Active Warps Per SM           warp        47.11
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260236
    Total DRAM Elapsed Cycles        cycle     94341120
    Average L1 Active Cycles         cycle   5894751.21
    Total L1 Elapsed Cycles          cycle    141623640
    Average L2 Active Cycles         cycle   5861324.56
    Total L2 Elapsed Cycles          cycle     95353520
    Average SM Active Cycles         cycle   5894751.21
    Total SM Elapsed Cycles          cycle    141623640
    Average SMSP Active Cycles       cycle   5888477.27
    Total SMSP Elapsed Cycles        cycle    566494560
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02144%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.50
    Elapsed Cycles                cycle      5903412
    Memory Throughput                 %        94.13
    DRAM Throughput                   %         1.10
    Duration                         ms         3.94
    L1/TEX Cache Throughput           %        94.31
    L2 Cache Throughput               %        14.72
    SM Active Cycles              cycle   5892306.79
    Compute (SM) Throughput           %        94.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         6.95
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.88
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.74
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.12
    Mem Busy                              %        66.56
    Max Bandwidth                         %        94.13
    L1/TEX Hit Rate                       %        87.09
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.18
    Mem Pipes Busy                        %        94.13
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.92
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.08
    Active Warps Per Scheduler          warp        11.79
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.867%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.79 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.70
    Warp Cycles Per Executed Instruction           cycle        22.70
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.867%                                                                                          
          On average, each warp of this kernel spends 7.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.6% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057075.98
    Issued Instructions                             inst    293479294
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.13
    Achieved Active Warps Per SM           warp        47.10
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260244
    Total DRAM Elapsed Cycles        cycle     94377984
    Average L1 Active Cycles         cycle   5892306.79
    Total L1 Elapsed Cycles          cycle    141679456
    Average L2 Active Cycles         cycle   5865021.94
    Total L2 Elapsed Cycles          cycle     95387632
    Average SM Active Cycles         cycle   5892306.79
    Total SM Elapsed Cycles          cycle    141679456
    Average SMSP Active Cycles       cycle   5887712.73
    Total SMSP Elapsed Cycles        cycle    566717824
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02145%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Mhz       989.98
    Elapsed Cycles                cycle      5904614
    Memory Throughput                 %        94.11
    DRAM Throughput                   %         0.73
    Duration                         ms         5.96
    L1/TEX Cache Throughput           %        94.40
    L2 Cache Throughput               %        15.77
    SM Active Cycles              cycle   5886900.38
    Compute (SM) Throughput           %        94.11
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.47
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.93
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.78
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.40
    Mem Busy                              %        68.09
    Max Bandwidth                         %        94.11
    L1/TEX Hit Rate                       %        86.48
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.18
    Mem Pipes Busy                        %        94.11
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.14%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.88%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.69
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.887%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.69 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.72
    Warp Cycles Per Executed Instruction           cycle        22.72
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.887%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.1% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057076.05
    Issued Instructions                             inst    293479301
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.32
    Achieved Active Warps Per SM           warp        47.19
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle    143022080
    Average L1 Active Cycles         cycle   5886900.38
    Total L1 Elapsed Cycles          cycle    141710656
    Average L2 Active Cycles         cycle   5737915.19
    Total L2 Elapsed Cycles          cycle     93042544
    Average SM Active Cycles         cycle   5886900.38
    Total SM Elapsed Cycles          cycle    141710656
    Average SMSP Active Cycles       cycle   5881297.21
    Total SMSP Elapsed Cycles        cycle    566842624
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02151%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.30
    Elapsed Cycles                cycle      5900584
    Memory Throughput                 %        94.18
    DRAM Throughput                   %         0.96
    Duration                         ms         4.52
    L1/TEX Cache Throughput           %        94.34
    L2 Cache Throughput               %        15.05
    SM Active Cycles              cycle   5890120.21
    Compute (SM) Throughput           %        94.18
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.68
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          128
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.90
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.76
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.84
    Mem Busy                              %        67.05
    Max Bandwidth                         %        94.18
    L1/TEX Hit Rate                       %        86.69
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.01
    Mem Pipes Busy                        %        94.18
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.17%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.65
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.823%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.65 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.70
    Warp Cycles Per Executed Instruction           cycle        22.70
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.823%                                                                                          
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.3% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057075.76
    Issued Instructions                             inst    293479273
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.18
    Achieved Active Warps Per SM           warp        47.13
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260440
    Total DRAM Elapsed Cycles        cycle    108426240
    Average L1 Active Cycles         cycle   5890120.21
    Total L1 Elapsed Cycles          cycle    141613480
    Average L2 Active Cycles         cycle   5881826.69
    Total L2 Elapsed Cycles          cycle     95491056
    Average SM Active Cycles         cycle   5890120.21
    Total SM Elapsed Cycles          cycle    141613480
    Average SMSP Active Cycles       cycle   5882223.47
    Total SMSP Elapsed Cycles        cycle    566453920
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02149%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.41
    Elapsed Cycles                cycle      5902665
    Memory Throughput                 %        94.14
    DRAM Throughput                   %         1.04
    Duration                         ms         4.19
    L1/TEX Cache Throughput           %        94.34
    L2 Cache Throughput               %        15.08
    SM Active Cycles              cycle   5890375.12
    Compute (SM) Throughput           %        94.14
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         7.41
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.90
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.76
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.99
    Mem Busy                              %        66.92
    Max Bandwidth                         %        94.14
    L1/TEX Hit Rate                       %        87.00
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.31
    Mem Pipes Busy                        %        94.14
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.96
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.04
    Active Warps Per Scheduler          warp        11.79
    Eligible Warps Per Scheduler        warp         2.64
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.856%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.79 active warps per scheduler, but only an average of 2.64 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.70
    Warp Cycles Per Executed Instruction           cycle        22.70
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.856%                                                                                          
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.1% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057075.99
    Issued Instructions                             inst    293479295
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.18
    Achieved Active Warps Per SM           warp        47.13
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260480
    Total DRAM Elapsed Cycles        cycle    100390912
    Average L1 Active Cycles         cycle   5890375.12
    Total L1 Elapsed Cycles          cycle    141662648
    Average L2 Active Cycles         cycle   5816159.56
    Total L2 Elapsed Cycles          cycle     94404864
    Average SM Active Cycles         cycle   5890375.12
    Total SM Elapsed Cycles          cycle    141662648
    Average SMSP Active Cycles       cycle   5883846.33
    Total SMSP Elapsed Cycles        cycle    566650592
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02149%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle      5899830
    Memory Throughput                 %        94.19
    DRAM Throughput                   %         1.03
    Duration                         ms         4.23
    L1/TEX Cache Throughput           %        94.35
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   5889953.38
    Compute (SM) Throughput           %        94.19
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         6.42
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.90
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.76
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.97
    Mem Busy                              %        66.88
    Max Bandwidth                         %        94.19
    L1/TEX Hit Rate                       %        87.20
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.56
    Mem Pipes Busy                        %        94.19
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.18%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.9%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.91
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.09
    Active Warps Per Scheduler          warp        11.77
    Eligible Warps Per Scheduler        warp         2.61
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.81%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.77 active warps per scheduler, but only an average of 2.61 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.67
    Warp Cycles Per Executed Instruction           cycle        22.67
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.81%                                                                                           
          On average, each warp of this kernel spends 7.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.3% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057075.92
    Issued Instructions                             inst    293479288
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.22
    Achieved Active Warps Per SM           warp        47.14
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       260240
    Total DRAM Elapsed Cycles        cycle    101421056
    Average L1 Active Cycles         cycle   5889953.38
    Total L1 Elapsed Cycles          cycle    141594064
    Average L2 Active Cycles         cycle   5871922.19
    Total L2 Elapsed Cycles          cycle     95379968
    Average SM Active Cycles         cycle   5889953.38
    Total SM Elapsed Cycles          cycle    141594064
    Average SMSP Active Cycles       cycle   5888958.02
    Total SMSP Elapsed Cycles        cycle    566376256
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02147%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (64, 64, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.56
    Elapsed Cycles                cycle      5903566
    Memory Throughput                 %        94.13
    DRAM Throughput                   %         1.15
    Duration                         ms         3.78
    L1/TEX Cache Throughput           %        94.32
    L2 Cache Throughput               %        14.69
    SM Active Cycles              cycle   5891828.50
    Compute (SM) Throughput           %        94.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.13
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.89
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.74
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         2.21
    Mem Busy                              %        66.49
    Max Bandwidth                         %        94.13
    L1/TEX Hit Rate                       %        87.25
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.78
    Mem Pipes Busy                        %        94.13
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.15%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.88%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.92
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.08
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.87%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.72
    Warp Cycles Per Executed Instruction           cycle        22.72
    Avg. Active Threads Per Warp                                31.88
    Avg. Not Predicated Off Threads Per Warp                    31.85
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.87%                                                                                           
          On average, each warp of this kernel spends 7.1 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.3% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   3056985.33
    Executed Instructions                           inst    293470592
    Avg. Issued Instructions Per Scheduler          inst   3057076.11
    Issued Instructions                             inst    293479307
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               28.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.14
    Achieved Active Warps Per SM           warp        47.11
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       261388
    Total DRAM Elapsed Cycles        cycle     90750976
    Average L1 Active Cycles         cycle   5891828.50
    Total L1 Elapsed Cycles          cycle    141684984
    Average L2 Active Cycles         cycle   5859527.12
    Total L2 Elapsed Cycles          cycle     95362192
    Average SM Active Cycles         cycle   5891828.50
    Total SM Elapsed Cycles          cycle    141684984
    Average SMSP Active Cycles       cycle   5887526.80
    Total SMSP Elapsed Cycles        cycle    566739936
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst      9010304
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02143%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 32640 excessive sectors (0% of the total  
          149720190 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.41
    Elapsed Cycles                cycle     11497527
    Memory Throughput                 %        94.22
    DRAM Throughput                   %         0.83
    Duration                         ms         8.15
    L1/TEX Cache Throughput           %        94.57
    L2 Cache Throughput               %        15.16
    SM Active Cycles              cycle  11455562.29
    Compute (SM) Throughput           %        94.22
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.22
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.88
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.59
    Mem Busy                              %        63.02
    Max Bandwidth                         %        94.22
    L1/TEX Hit Rate                       %        87.23
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.29
    Mem Pipes Busy                        %        94.22
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.69%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.78%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.84
    Eligible Warps Per Scheduler        warp         2.58
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.777%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.84 active warps per scheduler, but only an average of 2.58 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.78
    Warp Cycles Per Executed Instruction           cycle        22.78
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.777%                                                                                          
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.2% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.65
    Issued Instructions                             inst    571482302
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.75
    Achieved Active Warps Per SM           warp        47.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405972
    Total DRAM Elapsed Cycles        cycle    195545088
    Average L1 Active Cycles         cycle  11455562.29
    Total L1 Elapsed Cycles          cycle    275938800
    Average L2 Active Cycles         cycle  11193348.75
    Total L2 Elapsed Cycles          cycle    183902592
    Average SM Active Cycles         cycle  11455562.29
    Total SM Elapsed Cycles          cycle    275938800
    Average SMSP Active Cycles       cycle  11451427.96
    Total SMSP Elapsed Cycles        cycle   1103755200
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02405%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.51
    Elapsed Cycles                cycle     11524722
    Memory Throughput                 %        94.00
    DRAM Throughput                   %         0.89
    Duration                         ms         7.61
    L1/TEX Cache Throughput           %        94.54
    L2 Cache Throughput               %        14.93
    SM Active Cycles              cycle  11458371.75
    Compute (SM) Throughput           %        94.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.78
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.95
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.86
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.71
    Mem Busy                              %        62.88
    Max Bandwidth                         %        94.00
    L1/TEX Hit Rate                       %        87.18
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.86
    Mem Pipes Busy                        %        94.00
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.57%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.71%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.56
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.998%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.56 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.79
    Warp Cycles Per Executed Instruction           cycle        22.79
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.998%                                                                                          
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.8% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.98
    Issued Instructions                             inst    571482334
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.64
    Achieved Active Warps Per SM           warp        47.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405976
    Total DRAM Elapsed Cycles        cycle    182421504
    Average L1 Active Cycles         cycle  11458371.75
    Total L1 Elapsed Cycles          cycle    276590096
    Average L2 Active Cycles         cycle  11358065.50
    Total L2 Elapsed Cycles          cycle    185981696
    Average SM Active Cycles         cycle  11458371.75
    Total SM Elapsed Cycles          cycle    276590096
    Average SMSP Active Cycles       cycle  11452160.08
    Total SMSP Elapsed Cycles        cycle   1106360384
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02413%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.23
    Elapsed Cycles                cycle     11498544
    Memory Throughput                 %        94.21
    DRAM Throughput                   %         0.72
    Duration                         ms         9.35
    L1/TEX Cache Throughput           %        94.57
    L2 Cache Throughput               %        15.51
    SM Active Cycles              cycle  11454731.21
    Compute (SM) Throughput           %        94.21
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.47
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.89
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.39
    Mem Busy                              %        62.98
    Max Bandwidth                         %        94.21
    L1/TEX Hit Rate                       %        87.06
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.27
    Mem Pipes Busy                        %        94.21
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.68%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.77%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.00
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.00
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.785%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.80
    Warp Cycles Per Executed Instruction           cycle        22.80
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.785%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.6% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.72
    Issued Instructions                             inst    571482309
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.74
    Achieved Active Warps Per SM           warp        47.39
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405972
    Total DRAM Elapsed Cycles        cycle    224176128
    Average L1 Active Cycles         cycle  11454731.21
    Total L1 Elapsed Cycles          cycle    275964384
    Average L2 Active Cycles         cycle     11167963
    Total L2 Elapsed Cycles          cycle    181727616
    Average SM Active Cycles         cycle  11454731.21
    Total SM Elapsed Cycles          cycle    275964384
    Average SMSP Active Cycles       cycle  11448647.55
    Total SMSP Elapsed Cycles        cycle   1103857536
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02429%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.29
    Elapsed Cycles                cycle     11503018
    Memory Throughput                 %        94.18
    DRAM Throughput                   %         0.77
    Duration                         ms         8.92
    L1/TEX Cache Throughput           %        94.57
    L2 Cache Throughput               %        15.49
    SM Active Cycles              cycle  11455144.25
    Compute (SM) Throughput           %        94.18
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.50
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.89
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.47
    Mem Busy                              %        62.95
    Max Bandwidth                         %        94.18
    L1/TEX Hit Rate                       %        87.12
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.71
    Mem Pipes Busy                        %        94.18
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.66%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.76%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.61
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.822%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.61 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.79
    Warp Cycles Per Executed Instruction           cycle        22.79
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.822%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.6% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.71
    Issued Instructions                             inst    571482308
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.71
    Achieved Active Warps Per SM           warp        47.38
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       410968
    Total DRAM Elapsed Cycles        cycle    213833728
    Average L1 Active Cycles         cycle  11455144.25
    Total L1 Elapsed Cycles          cycle    276070640
    Average L2 Active Cycles         cycle     11161289
    Total L2 Elapsed Cycles          cycle    181884560
    Average SM Active Cycles         cycle  11455144.25
    Total SM Elapsed Cycles          cycle    276070640
    Average SMSP Active Cycles       cycle  11449759.31
    Total SMSP Elapsed Cycles        cycle   1104282560
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02425%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.51
    Elapsed Cycles                cycle     11490349
    Memory Throughput                 %        94.28
    DRAM Throughput                   %         0.89
    Duration                         ms         7.58
    L1/TEX Cache Throughput           %        94.59
    L2 Cache Throughput               %        14.94
    SM Active Cycles              cycle  11453325.50
    Compute (SM) Throughput           %        94.28
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.35
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.89
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.71
    Mem Busy                              %        63.07
    Max Bandwidth                         %        94.28
    L1/TEX Hit Rate                       %        87.10
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.23
    Mem Pipes Busy                        %        94.28
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.72%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.79%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.56
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.718%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.56 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.79
    Warp Cycles Per Executed Instruction           cycle        22.79
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.718%                                                                                          
          On average, each warp of this kernel spends 7.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 31.8% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.88
    Issued Instructions                             inst    571482324
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.71
    Achieved Active Warps Per SM           warp        47.38
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405944
    Total DRAM Elapsed Cycles        cycle    181878784
    Average L1 Active Cycles         cycle  11453325.50
    Total L1 Elapsed Cycles          cycle    275765816
    Average L2 Active Cycles         cycle  11372500.06
    Total L2 Elapsed Cycles          cycle    185463104
    Average SM Active Cycles         cycle  11453325.50
    Total SM Elapsed Cycles          cycle    275765816
    Average SMSP Active Cycles       cycle  11451619.71
    Total SMSP Elapsed Cycles        cycle   1103063264
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02423%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.42
    Elapsed Cycles                cycle     11499660
    Memory Throughput                 %        94.21
    DRAM Throughput                   %         0.84
    Duration                         ms         8.07
    L1/TEX Cache Throughput           %        94.57
    L2 Cache Throughput               %        14.82
    SM Active Cycles              cycle  11454739.12
    Compute (SM) Throughput           %        94.21
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.37
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.88
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.61
    Mem Busy                              %        62.97
    Max Bandwidth                         %        94.21
    L1/TEX Hit Rate                       %        87.16
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.92
    Mem Pipes Busy                        %        94.21
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.68%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.77%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.795%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.81
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.795%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.5% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.88
    Issued Instructions                             inst    571482324
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.77
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405972
    Total DRAM Elapsed Cycles        cycle    193517568
    Average L1 Active Cycles         cycle  11454739.12
    Total L1 Elapsed Cycles          cycle    275991208
    Average L2 Active Cycles         cycle  11278189.88
    Total L2 Elapsed Cycles          cycle    187862256
    Average SM Active Cycles         cycle  11454739.12
    Total SM Elapsed Cycles          cycle    275991208
    Average SMSP Active Cycles       cycle  11449470.98
    Total SMSP Elapsed Cycles        cycle   1103964832
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02373%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.42
    Elapsed Cycles                cycle     11500271
    Memory Throughput                 %        94.20
    DRAM Throughput                   %         0.85
    Duration                         ms         8.07
    L1/TEX Cache Throughput           %        94.57
    L2 Cache Throughput               %        14.87
    SM Active Cycles              cycle  11455611.08
    Compute (SM) Throughput           %        94.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.50
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.88
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.63
    Mem Busy                              %        63.02
    Max Bandwidth                         %        94.20
    L1/TEX Hit Rate                       %        87.12
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.85
    Mem Pipes Busy                        %        94.20
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.67%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.77%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.799%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.80
    Warp Cycles Per Executed Instruction           cycle        22.81
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.799%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.3% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.79
    Issued Instructions                             inst    571482316
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.71
    Achieved Active Warps Per SM           warp        47.38
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       410664
    Total DRAM Elapsed Cycles        cycle    193527808
    Average L1 Active Cycles         cycle  11455611.08
    Total L1 Elapsed Cycles          cycle    276005792
    Average L2 Active Cycles         cycle  11501288.75
    Total L2 Elapsed Cycles          cycle    187874128
    Average SM Active Cycles         cycle  11455611.08
    Total SM Elapsed Cycles          cycle    276005792
    Average SMSP Active Cycles       cycle  11449603.86
    Total SMSP Elapsed Cycles        cycle   1104023168
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02419%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.30
    Elapsed Cycles                cycle     11500298
    Memory Throughput                 %        94.20
    DRAM Throughput                   %         0.77
    Duration                         ms         8.81
    L1/TEX Cache Throughput           %        94.57
    L2 Cache Throughput               %        15.09
    SM Active Cycles              cycle  11455466.25
    Compute (SM) Throughput           %        94.20
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.02
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.88
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.47
    Mem Busy                              %        63.00
    Max Bandwidth                         %        94.20
    L1/TEX Hit Rate                       %        87.07
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.89
    Mem Pipes Busy                        %        94.20
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.67%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.77%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.85
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.8%                                                                                      
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.85 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.80
    Warp Cycles Per Executed Instruction           cycle        22.80
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.8%                                                                                            
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.3% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.90
    Issued Instructions                             inst    571482326
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.75
    Achieved Active Warps Per SM           warp        47.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406164
    Total DRAM Elapsed Cycles        cycle    211324928
    Average L1 Active Cycles         cycle  11455466.25
    Total L1 Elapsed Cycles          cycle    276006688
    Average L2 Active Cycles         cycle  11405773.62
    Total L2 Elapsed Cycles          cycle    186115568
    Average SM Active Cycles         cycle  11455466.25
    Total SM Elapsed Cycles          cycle    276006688
    Average SMSP Active Cycles       cycle  11449473.98
    Total SMSP Elapsed Cycles        cycle   1104026752
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02422%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle     11478270
    Memory Throughput                 %        94.38
    DRAM Throughput                   %         0.82
    Duration                         ms         8.23
    L1/TEX Cache Throughput           %        94.57
    L2 Cache Throughput               %        15.02
    SM Active Cycles              cycle  11455122.04
    Compute (SM) Throughput           %        94.38
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.83
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          256
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.88
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.58
    Mem Busy                              %        63.10
    Max Bandwidth                         %        94.38
    L1/TEX Hit Rate                       %        87.21
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.61
    Mem Pipes Busy                        %        94.38
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.77%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.82%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.86
    Eligible Warps Per Scheduler        warp         2.59
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.619%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.86 active warps per scheduler, but only an average of 2.59 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.81
    Warp Cycles Per Executed Instruction           cycle        22.81
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.619%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.4% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.62
    Issued Instructions                             inst    571482300
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.77
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       405980
    Total DRAM Elapsed Cycles        cycle    197313536
    Average L1 Active Cycles         cycle  11455122.04
    Total L1 Elapsed Cycles          cycle    275477456
    Average L2 Active Cycles         cycle  11364551.06
    Total L2 Elapsed Cycles          cycle    185524352
    Average SM Active Cycles         cycle  11455122.04
    Total SM Elapsed Cycles          cycle    275477456
    Average SMSP Active Cycles       cycle  11450196.44
    Total SMSP Elapsed Cycles        cycle   1101909824
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02421%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (80, 80, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.32
    Elapsed Cycles                cycle     11499042
    Memory Throughput                 %        94.21
    DRAM Throughput                   %         0.78
    Duration                         ms         8.71
    L1/TEX Cache Throughput           %        94.58
    L2 Cache Throughput               %        15.29
    SM Active Cycles              cycle  11454531.92
    Compute (SM) Throughput           %        94.21
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.48
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        59.89
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.49
    Mem Busy                              %        62.97
    Max Bandwidth                         %        94.21
    L1/TEX Hit Rate                       %        87.16
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.01
    Mem Pipes Busy                        %        94.21
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.68%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.77%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.84
    Eligible Warps Per Scheduler        warp         2.60
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.789%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.84 active warps per scheduler, but only an average of 2.60 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.78
    Warp Cycles Per Executed Instruction           cycle        22.78
    Avg. Active Threads Per Warp                                31.85
    Avg. Not Predicated Off Threads Per Warp                    31.83
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.789%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.4% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      5952850
    Executed Instructions                           inst    571473600
    Avg. Issued Instructions Per Scheduler          inst   5952940.73
    Issued Instructions                             inst    571482310
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   6400
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         1638400
    Uses Green Context                                             0
    Waves Per SM                                               44.44
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.74
    Achieved Active Warps Per SM           warp        47.39
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       406052
    Total DRAM Elapsed Cycles        cycle    208900096
    Average L1 Active Cycles         cycle  11454531.92
    Total L1 Elapsed Cycles          cycle    275976104
    Average L2 Active Cycles         cycle  11293487.94
    Total L2 Elapsed Cycles          cycle    183981632
    Average SM Active Cycles         cycle  11454531.92
    Total SM Elapsed Cycles          cycle    275976104
    Average SMSP Active Cycles       cycle  11449827.39
    Total SMSP Elapsed Cycles        cycle   1103904416
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     17278560
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.02426%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 76082 excessive sectors (0% of the total  
          308029910 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.59
    Elapsed Cycles                cycle     19778788
    Memory Throughput                 %        94.53
    DRAM Throughput                   %         0.78
    Duration                         ms        12.44
    L1/TEX Cache Throughput           %        94.67
    L2 Cache Throughput               %        16.87
    SM Active Cycles              cycle  19750224.38
    Compute (SM) Throughput           %        94.53
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.29
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.13
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.50
    Mem Busy                              %        51.15
    Max Bandwidth                         %        94.53
    L1/TEX Hit Rate                       %        84.79
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.89
    Mem Pipes Busy                        %        94.53
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.88
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.471%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.88 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.85
    Warp Cycles Per Executed Instruction           cycle        22.85
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.471%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.6% of the total average of 22.9 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.59
    Issued Instructions                             inst    985471737
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.90
    Achieved Active Warps Per SM           warp        47.47
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584440
    Total DRAM Elapsed Cycles        cycle    298301440
    Average L1 Active Cycles         cycle  19750224.38
    Total L1 Elapsed Cycles          cycle    474689648
    Average L2 Active Cycles         cycle  19447616.94
    Total L2 Elapsed Cycles          cycle    316456288
    Average SM Active Cycles         cycle  19750224.38
    Total SM Elapsed Cycles          cycle    474689648
    Average SMSP Active Cycles       cycle  19749725.94
    Total SMSP Elapsed Cycles        cycle   1898758592
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.44
    Elapsed Cycles                cycle     19774235
    Memory Throughput                 %        94.55
    DRAM Throughput                   %         0.71
    Duration                         ms        13.73
    L1/TEX Cache Throughput           %        94.69
    L2 Cache Throughput               %        16.61
    SM Active Cycles              cycle  19745940.88
    Compute (SM) Throughput           %        94.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.43
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.36
    Mem Busy                              %        51.15
    Max Bandwidth                         %        94.55
    L1/TEX Hit Rate                       %        84.90
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.32
    Mem Pipes Busy                        %        94.55
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.47%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.88
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.449%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.88 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.86
    Warp Cycles Per Executed Instruction           cycle        22.86
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.449%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.5% of the total average of 22.9 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.84
    Issued Instructions                             inst    985471761
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.99
    Achieved Active Warps Per SM           warp        47.52
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584044
    Total DRAM Elapsed Cycles        cycle    329297920
    Average L1 Active Cycles         cycle  19745940.88
    Total L1 Elapsed Cycles          cycle    474580952
    Average L2 Active Cycles         cycle  19612799.56
    Total L2 Elapsed Cycles          cycle    319679856
    Average SM Active Cycles         cycle  19745940.88
    Total SM Elapsed Cycles          cycle    474580952
    Average SMSP Active Cycles       cycle  19747716.99
    Total SMSP Elapsed Cycles        cycle   1898323808
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.65
    Elapsed Cycles                cycle     19783948
    Memory Throughput                 %        94.50
    DRAM Throughput                   %         0.81
    Duration                         ms        11.99
    L1/TEX Cache Throughput           %        94.66
    L2 Cache Throughput               %        16.89
    SM Active Cycles              cycle  19751852.12
    Compute (SM) Throughput           %        94.50
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.83
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.13
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.56
    Mem Busy                              %        51.13
    Max Bandwidth                         %        94.50
    L1/TEX Hit Rate                       %        83.90
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.98
    Mem Pipes Busy                        %        94.50
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.45%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.79
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.496%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.79 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.85
    Warp Cycles Per Executed Instruction           cycle        22.85
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.496%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.6% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.43
    Issued Instructions                             inst    985471721
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.90
    Achieved Active Warps Per SM           warp        47.47
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584160
    Total DRAM Elapsed Cycles        cycle    287528960
    Average L1 Active Cycles         cycle  19751852.12
    Total L1 Elapsed Cycles          cycle    474813808
    Average L2 Active Cycles         cycle  19123427.06
    Total L2 Elapsed Cycles          cycle    310787248
    Average SM Active Cycles         cycle  19751852.12
    Total SM Elapsed Cycles          cycle    474813808
    Average SMSP Active Cycles       cycle  19749907.65
    Total SMSP Elapsed Cycles        cycle   1899255232
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.74
    Elapsed Cycles                cycle     19767263
    Memory Throughput                 %        94.58
    DRAM Throughput                   %         0.86
    Duration                         ms        11.36
    L1/TEX Cache Throughput           %        94.65
    L2 Cache Throughput               %        17.34
    SM Active Cycles              cycle  19752520.79
    Compute (SM) Throughput           %        94.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.09
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.13
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.65
    Mem Busy                              %        51.20
    Max Bandwidth                         %        94.58
    L1/TEX Hit Rate                       %        84.59
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.90
    Mem Pipes Busy                        %        94.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.49%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.80
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.415%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.80 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.84
    Warp Cycles Per Executed Instruction           cycle        22.84
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.415%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.2% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.96
    Issued Instructions                             inst    985471772
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.97
    Achieved Active Warps Per SM           warp        47.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       586856
    Total DRAM Elapsed Cycles        cycle    272429056
    Average L1 Active Cycles         cycle  19752520.79
    Total L1 Elapsed Cycles          cycle    474411432
    Average L2 Active Cycles         cycle  19002044.88
    Total L2 Elapsed Cycles          cycle    308080464
    Average SM Active Cycles         cycle  19752520.79
    Total SM Elapsed Cycles          cycle    474411432
    Average SMSP Active Cycles       cycle  19751064.18
    Total SMSP Elapsed Cycles        cycle   1897645728
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle     19768396
    Memory Throughput                 %        94.58
    DRAM Throughput                   %         0.79
    Duration                         ms        12.32
    L1/TEX Cache Throughput           %        94.67
    L2 Cache Throughput               %        16.83
    SM Active Cycles              cycle  19749971.04
    Compute (SM) Throughput           %        94.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         9.63
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.13
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.52
    Mem Busy                              %        51.17
    Max Bandwidth                         %        94.58
    L1/TEX Hit Rate                       %        84.75
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.48
    Mem Pipes Busy                        %        94.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.49%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.418%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.84
    Warp Cycles Per Executed Instruction           cycle        22.84
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.418%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.4% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.57
    Issued Instructions                             inst    985471735
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        99.01
    Achieved Active Warps Per SM           warp        47.52
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584256
    Total DRAM Elapsed Cycles        cycle    295366144
    Average L1 Active Cycles         cycle  19749971.04
    Total L1 Elapsed Cycles          cycle    474426072
    Average L2 Active Cycles         cycle  19289501.69
    Total L2 Elapsed Cycles          cycle    312962768
    Average SM Active Cycles         cycle  19749971.04
    Total SM Elapsed Cycles          cycle    474426072
    Average SMSP Active Cycles       cycle  19751767.79
    Total SMSP Elapsed Cycles        cycle   1897704288
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.57
    Elapsed Cycles                cycle     19782816
    Memory Throughput                 %        94.51
    DRAM Throughput                   %         0.78
    Duration                         ms        12.56
    L1/TEX Cache Throughput           %        94.67
    L2 Cache Throughput               %        16.75
    SM Active Cycles              cycle  19750072.29
    Compute (SM) Throughput           %        94.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.49
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.13
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.49
    Mem Busy                              %        51.14
    Max Bandwidth                         %        94.51
    L1/TEX Hit Rate                       %        84.81
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.85
    Mem Pipes Busy                        %        94.51
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.88
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.49%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.88 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.85
    Warp Cycles Per Executed Instruction           cycle        22.85
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.49%                                                                                           
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.7% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.95
    Issued Instructions                             inst    985471771
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.92
    Achieved Active Warps Per SM           warp        47.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       585016
    Total DRAM Elapsed Cycles        cycle    301203456
    Average L1 Active Cycles         cycle  19750072.29
    Total L1 Elapsed Cycles          cycle    474786368
    Average L2 Active Cycles         cycle  19639262.44
    Total L2 Elapsed Cycles          cycle    316523504
    Average SM Active Cycles         cycle  19750072.29
    Total SM Elapsed Cycles          cycle    474786368
    Average SMSP Active Cycles       cycle  19748109.93
    Total SMSP Elapsed Cycles        cycle   1899145472
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.32
    Elapsed Cycles                cycle     19773271
    Memory Throughput                 %        94.56
    DRAM Throughput                   %         0.65
    Duration                         ms        14.98
    L1/TEX Cache Throughput           %        94.68
    L2 Cache Throughput               %        16.94
    SM Active Cycles              cycle  19748113.88
    Compute (SM) Throughput           %        94.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.30
    Dropped Samples                sample            2
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.25
    Mem Busy                              %        51.17
    Max Bandwidth                         %        94.56
    L1/TEX Hit Rate                       %        84.70
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       101.22
    Mem Pipes Busy                        %        94.56
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.48%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.88
    Eligible Warps Per Scheduler        warp         2.83
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.445%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.88 active warps per scheduler, but only an average of 2.83 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.86
    Warp Cycles Per Executed Instruction           cycle        22.86
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.445%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.8% of the total average of 22.9 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.43
    Issued Instructions                             inst    985471721
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.98
    Achieved Active Warps Per SM           warp        47.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       586300
    Total DRAM Elapsed Cycles        cycle    359215104
    Average L1 Active Cycles         cycle  19748113.88
    Total L1 Elapsed Cycles          cycle    474557888
    Average L2 Active Cycles         cycle     19418868
    Total L2 Elapsed Cycles          cycle    316369728
    Average SM Active Cycles         cycle  19748113.88
    Total SM Elapsed Cycles          cycle    474557888
    Average SMSP Active Cycles       cycle  19747719.74
    Total SMSP Elapsed Cycles        cycle   1898231552
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle     19790250
    Memory Throughput                 %        94.47
    DRAM Throughput                   %         0.79
    Duration                         ms        12.33
    L1/TEX Cache Throughput           %        94.66
    L2 Cache Throughput               %        16.84
    SM Active Cycles              cycle  19751832.75
    Compute (SM) Throughput           %        94.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.01
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.07
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.13
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.52
    Mem Busy                              %        51.12
    Max Bandwidth                         %        94.47
    L1/TEX Hit Rate                       %        84.81
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       101.16
    Mem Pipes Busy                        %        94.47
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.44%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.82
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.525%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.82 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.85
    Warp Cycles Per Executed Instruction           cycle        22.85
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.525%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.4% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.95
    Issued Instructions                             inst    985471771
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.78
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584164
    Total DRAM Elapsed Cycles        cycle    295684096
    Average L1 Active Cycles         cycle  19751832.75
    Total L1 Elapsed Cycles          cycle    474963504
    Average L2 Active Cycles         cycle  19300235.88
    Total L2 Elapsed Cycles          cycle    313681664
    Average SM Active Cycles         cycle  19751832.75
    Total SM Elapsed Cycles          cycle    474963504
    Average SMSP Active Cycles       cycle  19753214.70
    Total SMSP Elapsed Cycles        cycle   1899854016
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.65
    Elapsed Cycles                cycle     19793699
    Memory Throughput                 %        94.50
    DRAM Throughput                   %         0.81
    Duration                         ms        12.00
    L1/TEX Cache Throughput           %        94.67
    L2 Cache Throughput               %        17.54
    SM Active Cycles              cycle     19749461
    Compute (SM) Throughput           %        94.50
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.55
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.56
    Mem Busy                              %        51.15
    Max Bandwidth                         %        94.50
    L1/TEX Hit Rate                       %        84.66
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.07
    Mem Pipes Busy                        %        94.50
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.45%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.80
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.495%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.80 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.85
    Warp Cycles Per Executed Instruction           cycle        22.85
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.495%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.2% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.57
    Issued Instructions                             inst    985471735
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.91
    Achieved Active Warps Per SM           warp        47.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       584648
    Total DRAM Elapsed Cycles        cycle    287698944
    Average L1 Active Cycles         cycle     19749461
    Total L1 Elapsed Cycles          cycle    474811232
    Average L2 Active Cycles         cycle  19078556.19
    Total L2 Elapsed Cycles          cycle    309718240
    Average SM Active Cycles         cycle     19749461
    Total SM Elapsed Cycles          cycle    474811232
    Average SMSP Active Cycles       cycle  19752830.72
    Total SMSP Elapsed Cycles        cycle   1899244928
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (96, 96, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     19761130
    Memory Throughput                 %        94.61
    DRAM Throughput                   %         0.82
    Duration                         ms        11.87
    L1/TEX Cache Throughput           %        94.66
    L2 Cache Throughput               %        17.21
    SM Active Cycles              cycle  19750552.42
    Compute (SM) Throughput           %        94.61
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.16
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.58
    Mem Busy                              %        51.21
    Max Bandwidth                         %        94.61
    L1/TEX Hit Rate                       %        84.76
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.35
    Mem Pipes Busy                        %        94.61
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 41.5%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 18.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.87
    Eligible Warps Per Scheduler        warp         2.81
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.386%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.87 active warps per scheduler, but only an average of 2.81 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.83
    Warp Cycles Per Executed Instruction           cycle        22.83
    Avg. Active Threads Per Warp                                31.83
    Avg. Not Predicated Off Threads Per Warp                    31.82
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.386%                                                                                          
          On average, each warp of this kernel spends 7.6 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.2% of the total average of 22.8 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     10265240
    Executed Instructions                           inst    985463040
    Avg. Issued Instructions Per Scheduler          inst  10265330.94
    Issued Instructions                             inst    985471770
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   9216
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         2359296
    Uses Green Context                                             0
    Waves Per SM                                                  64
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.94
    Achieved Active Warps Per SM           warp        47.49
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       586604
    Total DRAM Elapsed Cycles        cycle    284610560
    Average L1 Active Cycles         cycle  19750552.42
    Total L1 Elapsed Cycles          cycle    474266128
    Average L2 Active Cycles         cycle  19310010.38
    Total L2 Elapsed Cycles          cycle    310479328
    Average SM Active Cycles         cycle  19750552.42
    Total SM Elapsed Cycles          cycle    474266128
    Average SMSP Active Cycles       cycle  19750762.85
    Total SMSP Elapsed Cycles        cycle   1897064512
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     29562624
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.51
    Elapsed Cycles                cycle     31365599
    Memory Throughput                 %        94.57
    DRAM Throughput                   %         0.73
    Duration                         ms        20.70
    L1/TEX Cache Throughput           %        94.74
    L2 Cache Throughput               %        19.96
    SM Active Cycles              cycle  31309599.42
    Compute (SM) Throughput           %        94.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.52
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.16
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.41
    Mem Busy                              %        55.97
    Max Bandwidth                         %        94.57
    L1/TEX Hit Rate                       %        82.31
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.68
    Mem Pipes Busy                        %        94.57
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.89%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.00
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.00
    Active Warps Per Scheduler          warp        11.80
    Eligible Warps Per Scheduler        warp         2.76
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.427%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.80 active warps per scheduler, but only an average of 2.76 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.69
    Warp Cycles Per Executed Instruction           cycle        22.69
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.427%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.1% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.56
    Issued Instructions                             inst   1562670678
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.27
    Achieved Active Warps Per SM           warp        47.17
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       911304
    Total DRAM Elapsed Cycles        cycle    496470016
    Average L1 Active Cycles         cycle  31309599.42
    Total L1 Elapsed Cycles          cycle    752770112
    Average L2 Active Cycles         cycle  31477281.56
    Total L2 Elapsed Cycles          cycle    506180848
    Average SM Active Cycles         cycle  31309599.42
    Total SM Elapsed Cycles          cycle    752770112
    Average SMSP Active Cycles       cycle  31301526.58
    Total SMSP Elapsed Cycles        cycle   3011080448
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0176%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     31353565
    Memory Throughput                 %        94.61
    DRAM Throughput                   %         0.81
    Duration                         ms        18.83
    L1/TEX Cache Throughput           %        94.73
    L2 Cache Throughput               %        19.96
    SM Active Cycles              cycle     31314023
    Compute (SM) Throughput           %        94.61
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.50
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.56
    Mem Busy                              %        55.92
    Max Bandwidth                         %        94.61
    L1/TEX Hit Rate                       %        82.71
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.81
    Mem Pipes Busy                        %        94.61
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.9%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.391%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.82 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.74
    Warp Cycles Per Executed Instruction           cycle        22.74
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.391%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.8% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.75
    Issued Instructions                             inst   1562670696
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.43
    Achieved Active Warps Per SM           warp        47.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       918316
    Total DRAM Elapsed Cycles        cycle    451567616
    Average L1 Active Cycles         cycle     31314023
    Total L1 Elapsed Cycles          cycle    752484568
    Average L2 Active Cycles         cycle  30680971.31
    Total L2 Elapsed Cycles          cycle    492616384
    Average SM Active Cycles         cycle     31314023
    Total SM Elapsed Cycles          cycle    752484568
    Average SMSP Active Cycles       cycle  31310983.09
    Total SMSP Elapsed Cycles        cycle   3009938272
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01763%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.61
    Elapsed Cycles                cycle     31390688
    Memory Throughput                 %        94.63
    DRAM Throughput                   %         0.79
    Duration                         ms        19.43
    L1/TEX Cache Throughput           %        94.73
    L2 Cache Throughput               %        20.31
    SM Active Cycles              cycle  31315117.21
    Compute (SM) Throughput           %        94.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte         8.32
    Dropped Samples                sample            1
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.52
    Mem Busy                              %        55.97
    Max Bandwidth                         %        94.63
    L1/TEX Hit Rate                       %        82.25
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.70
    Mem Pipes Busy                        %        94.63
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.92%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.91%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.366%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.71
    Warp Cycles Per Executed Instruction           cycle        22.71
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.366%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.9% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.62
    Issued Instructions                             inst   1562670684
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.36
    Achieved Active Warps Per SM           warp        47.21
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       922000
    Total DRAM Elapsed Cycles        cycle    465989632
    Average L1 Active Cycles         cycle  31315117.21
    Total L1 Elapsed Cycles          cycle    752287520
    Average L2 Active Cycles         cycle  30938503.62
    Total L2 Elapsed Cycles          cycle    495856080
    Average SM Active Cycles         cycle  31315117.21
    Total SM Elapsed Cycles          cycle    752287520
    Average SMSP Active Cycles       cycle  31307284.09
    Total SMSP Elapsed Cycles        cycle   3009150080
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01766%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     31358115
    Memory Throughput                 %        94.60
    DRAM Throughput                   %         0.81
    Duration                         ms        18.83
    L1/TEX Cache Throughput           %        94.73
    L2 Cache Throughput               %        20.17
    SM Active Cycles              cycle  31314303.92
    Compute (SM) Throughput           %        94.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.79
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.56
    Mem Busy                              %        55.95
    Max Bandwidth                         %        94.60
    L1/TEX Hit Rate                       %        82.58
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.80
    Mem Pipes Busy                        %        94.60
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.9%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.9%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.79
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.404%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.79 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.67
    Warp Cycles Per Executed Instruction           cycle        22.67
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.404%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.0% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.90
    Issued Instructions                             inst   1562670710
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.50
    Achieved Active Warps Per SM           warp        47.28
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       918316
    Total DRAM Elapsed Cycles        cycle    451638784
    Average L1 Active Cycles         cycle  31314303.92
    Total L1 Elapsed Cycles          cycle    752591640
    Average L2 Active Cycles         cycle  30864182.88
    Total L2 Elapsed Cycles          cycle    492287936
    Average SM Active Cycles         cycle  31314303.92
    Total SM Elapsed Cycles          cycle    752591640
    Average SMSP Active Cycles       cycle  31312555.67
    Total SMSP Elapsed Cycles        cycle   3010366560
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01775%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     31354475
    Memory Throughput                 %        94.61
    DRAM Throughput                   %         0.81
    Duration                         ms        18.83
    L1/TEX Cache Throughput           %        94.73
    L2 Cache Throughput               %        20.03
    SM Active Cycles              cycle  31314512.54
    Compute (SM) Throughput           %        94.61
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.11
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.56
    Mem Busy                              %        55.94
    Max Bandwidth                         %        94.61
    L1/TEX Hit Rate                       %        82.53
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.48
    Mem Pipes Busy                        %        94.61
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.91%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.9%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.98
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.02
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.394%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.72
    Warp Cycles Per Executed Instruction           cycle        22.72
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.394%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.8% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.35
    Issued Instructions                             inst   1562670658
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.35
    Achieved Active Warps Per SM           warp        47.21
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       917712
    Total DRAM Elapsed Cycles        cycle    451581952
    Average L1 Active Cycles         cycle  31314512.54
    Total L1 Elapsed Cycles          cycle    752506736
    Average L2 Active Cycles         cycle  30746472.81
    Total L2 Elapsed Cycles          cycle    492630320
    Average SM Active Cycles         cycle  31314512.54
    Total SM Elapsed Cycles          cycle    752506736
    Average SMSP Active Cycles       cycle  31315006.46
    Total SMSP Elapsed Cycles        cycle   3010026944
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01767%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle     31375521
    Memory Throughput                 %        94.54
    DRAM Throughput                   %         0.78
    Duration                         ms        19.55
    L1/TEX Cache Throughput           %        94.74
    L2 Cache Throughput               %        20.17
    SM Active Cycles              cycle  31310717.29
    Compute (SM) Throughput           %        94.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.16
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.49
    Mem Busy                              %        55.89
    Max Bandwidth                         %        94.54
    L1/TEX Hit Rate                       %        82.69
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.61
    Mem Pipes Busy                        %        94.54
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.87%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.88%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.457%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.82 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.73
    Warp Cycles Per Executed Instruction           cycle        22.73
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.457%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.9% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.90
    Issued Instructions                             inst   1562670710
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.46
    Achieved Active Warps Per SM           warp        47.26
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       908392
    Total DRAM Elapsed Cycles        cycle    468779008
    Average L1 Active Cycles         cycle  31310717.29
    Total L1 Elapsed Cycles          cycle    753010016
    Average L2 Active Cycles         cycle  30822744.94
    Total L2 Elapsed Cycles          cycle    497312720
    Average SM Active Cycles         cycle  31310717.29
    Total SM Elapsed Cycles          cycle    753010016
    Average SMSP Active Cycles       cycle  31310770.75
    Total SMSP Elapsed Cycles        cycle   3012040064
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01755%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.63
    Elapsed Cycles                cycle     31358962
    Memory Throughput                 %        94.59
    DRAM Throughput                   %         0.79
    Duration                         ms        19.18
    L1/TEX Cache Throughput           %        94.73
    L2 Cache Throughput               %        19.97
    SM Active Cycles              cycle  31312283.46
    Compute (SM) Throughput           %        94.59
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        15.60
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.16
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.52
    Mem Busy                              %        55.96
    Max Bandwidth                         %        94.59
    L1/TEX Hit Rate                       %        82.61
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        99.10
    Mem Pipes Busy                        %        94.59
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.9%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.9%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.00
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.00
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.407%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.71
    Warp Cycles Per Executed Instruction           cycle        22.71
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.407%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.1% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.57
    Issued Instructions                             inst   1562670679
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.46
    Achieved Active Warps Per SM           warp        47.26
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       910760
    Total DRAM Elapsed Cycles        cycle    459933696
    Average L1 Active Cycles         cycle  31312283.46
    Total L1 Elapsed Cycles          cycle    752614472
    Average L2 Active Cycles         cycle  30942206.81
    Total L2 Elapsed Cycles          cycle    497139072
    Average SM Active Cycles         cycle  31312283.46
    Total SM Elapsed Cycles          cycle    752614472
    Average SMSP Active Cycles       cycle  31306251.85
    Total SMSP Elapsed Cycles        cycle   3010457888
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01762%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.60
    Elapsed Cycles                cycle     31370903
    Memory Throughput                 %        94.56
    DRAM Throughput                   %         0.79
    Duration                         ms        19.55
    L1/TEX Cache Throughput           %        94.73
    L2 Cache Throughput               %        20.14
    SM Active Cycles              cycle  31313116.08
    Compute (SM) Throughput           %        94.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        12.26
    Dropped Samples                sample            5
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.51
    Mem Busy                              %        55.92
    Max Bandwidth                         %        94.56
    L1/TEX Hit Rate                       %        82.85
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.65
    Mem Pipes Busy                        %        94.56
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.88%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.81
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.443%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.81 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.72
    Warp Cycles Per Executed Instruction           cycle        22.72
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.443%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.0% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.83
    Issued Instructions                             inst   1562670704
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.43
    Achieved Active Warps Per SM           warp        47.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       923224
    Total DRAM Elapsed Cycles        cycle    468710400
    Average L1 Active Cycles         cycle  31313116.08
    Total L1 Elapsed Cycles          cycle    752899632
    Average L2 Active Cycles         cycle     30669406
    Total L2 Elapsed Cycles          cycle    497240432
    Average SM Active Cycles         cycle  31313116.08
    Total SM Elapsed Cycles          cycle    752899632
    Average SMSP Active Cycles       cycle  31306528.36
    Total SMSP Elapsed Cycles        cycle   3011598528
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01746%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.68
    Elapsed Cycles                cycle     31361881
    Memory Throughput                 %        94.58
    DRAM Throughput                   %         0.83
    Duration                         ms        18.67
    L1/TEX Cache Throughput           %        94.72
    L2 Cache Throughput               %        20.09
    SM Active Cycles              cycle  31315976.62
    Compute (SM) Throughput           %        94.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.24
    Dropped Samples                sample            0
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.60
    Mem Busy                              %        55.92
    Max Bandwidth                         %        94.58
    L1/TEX Hit Rate                       %        81.91
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.63
    Mem Pipes Busy                        %        94.58
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.9%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.89%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.00
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.00
    Active Warps Per Scheduler          warp        11.79
    Eligible Warps Per Scheduler        warp         2.74
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.416%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.79 active warps per scheduler, but only an average of 2.74 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.68
    Warp Cycles Per Executed Instruction           cycle        22.68
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.416%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 34.0% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.21
    Issued Instructions                             inst   1562670644
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.37
    Achieved Active Warps Per SM           warp        47.22
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       931580
    Total DRAM Elapsed Cycles        cycle    447655936
    Average L1 Active Cycles         cycle  31315976.62
    Total L1 Elapsed Cycles          cycle    752683648
    Average L2 Active Cycles         cycle  30952374.75
    Total L2 Elapsed Cycles          cycle    497300304
    Average SM Active Cycles         cycle  31315976.62
    Total SM Elapsed Cycles          cycle    752683648
    Average SMSP Active Cycles       cycle  31301422.56
    Total SMSP Elapsed Cycles        cycle   3010734592
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01762%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (112, 112, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.62
    Elapsed Cycles                cycle     31375822
    Memory Throughput                 %        94.54
    DRAM Throughput                   %         0.79
    Duration                         ms        19.37
    L1/TEX Cache Throughput           %        94.73
    L2 Cache Throughput               %        20.05
    SM Active Cycles              cycle  31314115.71
    Compute (SM) Throughput           %        94.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.37
    Dropped Samples                sample            5
    Maximum Sampling Interval          us          512
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.98
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.52
    Mem Busy                              %        55.89
    Max Bandwidth                         %        94.54
    L1/TEX Hit Rate                       %        82.56
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       101.08
    Mem Pipes Busy                        %        94.54
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 49.87%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 15.1 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 25.88%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 23.2 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.82
    Eligible Warps Per Scheduler        warp         2.75
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.457%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.82 active warps per scheduler, but only an average of 2.75 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.74
    Warp Cycles Per Executed Instruction           cycle        22.74
    Avg. Active Threads Per Warp                                31.82
    Avg. Not Predicated Off Threads Per Warp                    31.81
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.457%                                                                                          
          On average, each warp of this kernel spends 7.7 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.8% of the total average of 22.7 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  16277728.83
    Executed Instructions                           inst   1562661968
    Avg. Issued Instructions Per Scheduler          inst  16277819.90
    Issued Instructions                             inst   1562670710
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  12544
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         3211264
    Uses Green Context                                             0
    Waves Per SM                                               87.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.42
    Achieved Active Warps Per SM           warp        47.24
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       920612
    Total DRAM Elapsed Cycles        cycle    464451584
    Average L1 Active Cycles         cycle  31314115.71
    Total L1 Elapsed Cycles          cycle    753007552
    Average L2 Active Cycles         cycle  30661544.38
    Total L2 Elapsed Cycles          cycle    495541440
    Average SM Active Cycles         cycle  31314115.71
    Total SM Elapsed Cycles          cycle    753007552
    Average SMSP Active Cycles       cycle  31309565.27
    Total SMSP Elapsed Cycles        cycle   3012030208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     46510352
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.01752%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 149187 excessive sectors (0% of the total 
          843201804 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source            
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.70
    Elapsed Cycles                cycle     46742201
    Memory Throughput                 %        94.67
    DRAM Throughput                   %         0.82
    Duration                         ms        27.48
    L1/TEX Cache Throughput           %        94.74
    L2 Cache Throughput               %        22.70
    SM Active Cycles              cycle  46704821.83
    Compute (SM) Throughput           %        94.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.80
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.09
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.58
    Mem Busy                              %        69.14
    Max Bandwidth                         %        94.67
    L1/TEX Hit Rate                       %        79.42
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.62
    Mem Pipes Busy                        %        94.67
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.02%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.76
    Eligible Warps Per Scheduler        warp         2.62
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.33%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.76 active warps per scheduler, but only an average of 2.62 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.63
    Warp Cycles Per Executed Instruction           cycle        22.63
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.33%                                                                                           
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.3% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270840.33
    Issued Instructions                             inst   2330000672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.93
    Achieved Active Warps Per SM           warp        47.00
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1356480
    Total DRAM Elapsed Cycles        cycle    658950144
    Average L1 Active Cycles         cycle  46704821.83
    Total L1 Elapsed Cycles          cycle   1121774696
    Average L2 Active Cycles         cycle  45250720.69
    Total L2 Elapsed Cycles          cycle    732340304
    Average SM Active Cycles         cycle  46704821.83
    Total SM Elapsed Cycles          cycle   1121774696
    Average SMSP Active Cycles       cycle  46704101.59
    Total SMSP Elapsed Cycles        cycle   4487098784
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1828%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.66
    Elapsed Cycles                cycle     46734392
    Memory Throughput                 %        94.68
    DRAM Throughput                   %         0.81
    Duration                         ms        28.07
    L1/TEX Cache Throughput           %        94.74
    L2 Cache Throughput               %        22.15
    SM Active Cycles              cycle  46705642.04
    Compute (SM) Throughput           %        94.68
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        10.94
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.55
    Mem Busy                              %        69.15
    Max Bandwidth                         %        94.68
    L1/TEX Hit Rate                       %        78.98
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       102.45
    Mem Pipes Busy                        %        94.68
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.03%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.99
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.01
    Active Warps Per Scheduler          warp        11.74
    Eligible Warps Per Scheduler        warp         2.64
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.318%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.74 active warps per scheduler, but only an average of 2.64 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.58
    Warp Cycles Per Executed Instruction           cycle        22.58
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.318%                                                                                          
          On average, each warp of this kernel spends 7.3 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.5% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270840.33
    Issued Instructions                             inst   2330000672
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.92
    Achieved Active Warps Per SM           warp        47.00
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1363884
    Total DRAM Elapsed Cycles        cycle    673090560
    Average L1 Active Cycles         cycle  46705642.04
    Total L1 Elapsed Cycles          cycle   1121624136
    Average L2 Active Cycles         cycle  45750530.12
    Total L2 Elapsed Cycles          cycle    734275392
    Average SM Active Cycles         cycle  46705642.04
    Total SM Elapsed Cycles          cycle   1121624136
    Average SMSP Active Cycles       cycle  46685118.32
    Total SMSP Elapsed Cycles        cycle   4486496544
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1843%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.46
    Elapsed Cycles                cycle     46742547
    Memory Throughput                 %        94.67
    DRAM Throughput                   %         0.71
    Duration                         ms        32.12
    L1/TEX Cache Throughput           %        94.79
    L2 Cache Throughput               %        23.19
    SM Active Cycles              cycle  46681856.21
    Compute (SM) Throughput           %        94.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.53
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.13
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.36
    Mem Busy                              %        69.26
    Max Bandwidth                         %        94.67
    L1/TEX Hit Rate                       %        78.27
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        96.83
    Mem Pipes Busy                        %        94.67
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.02%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.01
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        47.99
    Active Warps Per Scheduler          warp        11.73
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.331%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.73 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.55
    Warp Cycles Per Executed Instruction           cycle        22.55
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.331%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.1% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270840.10
    Issued Instructions                             inst   2330000650
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.70
    Achieved Active Warps Per SM           warp        46.89
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1362036
    Total DRAM Elapsed Cycles        cycle    770291712
    Average L1 Active Cycles         cycle  46681856.21
    Total L1 Elapsed Cycles          cycle   1121782376
    Average L2 Active Cycles         cycle  45780084.06
    Total L2 Elapsed Cycles          cycle    755449792
    Average SM Active Cycles         cycle  46681856.21
    Total SM Elapsed Cycles          cycle   1121782376
    Average SMSP Active Cycles       cycle  46663215.80
    Total SMSP Elapsed Cycles        cycle   4487129504
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1793%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.71
    Elapsed Cycles                cycle     46740350
    Memory Throughput                 %        94.67
    DRAM Throughput                   %         0.83
    Duration                         ms        27.33
    L1/TEX Cache Throughput           %        94.75
    L2 Cache Throughput               %        22.46
    SM Active Cycles              cycle     46703297
    Compute (SM) Throughput           %        94.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        11.34
    Dropped Samples                sample            8
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.97
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.10
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.59
    Mem Busy                              %        69.18
    Max Bandwidth                         %        94.67
    L1/TEX Hit Rate                       %        79.89
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       101.91
    Mem Pipes Busy                        %        94.67
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.46%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.02%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        51.97
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.03
    Active Warps Per Scheduler          warp        11.74
    Eligible Warps Per Scheduler        warp         2.61
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.329%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.74 active warps per scheduler, but only an average of 2.61 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.59
    Warp Cycles Per Executed Instruction           cycle        22.59
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.329%                                                                                          
          On average, each warp of this kernel spends 7.2 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.1% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270840.35
    Issued Instructions                             inst   2330000674
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.95
    Achieved Active Warps Per SM           warp        47.02
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1360792
    Total DRAM Elapsed Cycles        cycle    655474688
    Average L1 Active Cycles         cycle     46703297
    Total L1 Elapsed Cycles          cycle   1121762392
    Average L2 Active Cycles         cycle  45458965.12
    Total L2 Elapsed Cycles          cycle    733976064
    Average SM Active Cycles         cycle     46703297
    Total SM Elapsed Cycles          cycle   1121762392
    Average SMSP Active Cycles       cycle  46703723.67
    Total SMSP Elapsed Cycles        cycle   4487049568
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1832%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.41
    Elapsed Cycles                cycle     46731934
    Memory Throughput                 %        94.69
    DRAM Throughput                   %         0.69
    Duration                         ms        33.14
    L1/TEX Cache Throughput           %        94.79
    L2 Cache Throughput               %        23.10
    SM Active Cycles              cycle  46681022.04
    Compute (SM) Throughput           %        94.69
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.02
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.33
    Mem Busy                              %        69.22
    Max Bandwidth                         %        94.69
    L1/TEX Hit Rate                       %        78.13
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.76
    Mem Pipes Busy                        %        94.69
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.47%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.03%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.00
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        48.00
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.312%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.54
    Warp Cycles Per Executed Instruction           cycle        22.54
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.312%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.9% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270839.85
    Issued Instructions                             inst   2330000626
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.60
    Achieved Active Warps Per SM           warp        46.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1373040
    Total DRAM Elapsed Cycles        cycle    794779648
    Average L1 Active Cycles         cycle  46681022.04
    Total L1 Elapsed Cycles          cycle   1121563560
    Average L2 Active Cycles         cycle  47373145.44
    Total L2 Elapsed Cycles          cycle    747324256
    Average SM Active Cycles         cycle  46681022.04
    Total SM Elapsed Cycles          cycle   1121563560
    Average SMSP Active Cycles       cycle  46671044.27
    Total SMSP Elapsed Cycles        cycle   4486254240
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1875%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.39
    Elapsed Cycles                cycle     46727524
    Memory Throughput                 %        94.70
    DRAM Throughput                   %         0.68
    Duration                         ms        33.50
    L1/TEX Cache Throughput           %        94.79
    L2 Cache Throughput               %        23.06
    SM Active Cycles              cycle  46680073.17
    Compute (SM) Throughput           %        94.70
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.16
    Dropped Samples                sample            1
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.30
    Mem Busy                              %        69.20
    Max Bandwidth                         %        94.70
    L1/TEX Hit Rate                       %        77.87
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       100.35
    Mem Pipes Busy                        %        94.70
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.47%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.03%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.01
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        47.99
    Active Warps Per Scheduler          warp        11.73
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.304%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.73 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.55
    Warp Cycles Per Executed Instruction           cycle        22.55
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.304%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.0% of the total average of 22.6 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270839.90
    Issued Instructions                             inst   2330000630
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.54
    Achieved Active Warps Per SM           warp        46.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1356688
    Total DRAM Elapsed Cycles        cycle    803248128
    Average L1 Active Cycles         cycle  46680073.17
    Total L1 Elapsed Cycles          cycle   1121459416
    Average L2 Active Cycles         cycle  46905341.44
    Total L2 Elapsed Cycles          cycle    755230288
    Average SM Active Cycles         cycle  46680073.17
    Total SM Elapsed Cycles          cycle   1121459416
    Average SMSP Active Cycles       cycle  46662650.96
    Total SMSP Elapsed Cycles        cycle   4485837664
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1837%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.42
    Elapsed Cycles                cycle     46704332
    Memory Throughput                 %        94.74
    DRAM Throughput                   %         0.69
    Duration                         ms        32.78
    L1/TEX Cache Throughput           %        94.79
    L2 Cache Throughput               %        23.08
    SM Active Cycles              cycle  46679958.67
    Compute (SM) Throughput           %        94.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        14.09
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.33
    Mem Busy                              %        69.22
    Max Bandwidth                         %        94.74
    L1/TEX Hit Rate                       %        78.16
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.02
    Mem Pipes Busy                        %        94.74
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.5%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.04%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.01
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        47.99
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.257%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.54
    Warp Cycles Per Executed Instruction           cycle        22.54
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.257%                                                                                          
          On average, each warp of this kernel spends 7.5 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.1% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270840.01
    Issued Instructions                             inst   2330000641
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.70
    Achieved Active Warps Per SM           warp        46.89
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1365376
    Total DRAM Elapsed Cycles        cycle    785944576
    Average L1 Active Cycles         cycle  46679958.67
    Total L1 Elapsed Cycles          cycle   1120903104
    Average L2 Active Cycles         cycle  46730435.31
    Total L2 Elapsed Cycles          cycle    762997232
    Average SM Active Cycles         cycle  46679958.67
    Total SM Elapsed Cycles          cycle   1120903104
    Average SMSP Active Cycles       cycle  46666878.48
    Total SMSP Elapsed Cycles        cycle   4483612416
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1812%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.29
    Elapsed Cycles                cycle     46708675
    Memory Throughput                 %        94.73
    DRAM Throughput                   %         0.63
    Duration                         ms        36.21
    L1/TEX Cache Throughput           %        94.80
    L2 Cache Throughput               %        23.51
    SM Active Cycles              cycle  46677088.62
    Compute (SM) Throughput           %        94.73
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.76
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        52.00
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.20
    Mem Busy                              %        69.26
    Max Bandwidth                         %        94.73
    L1/TEX Hit Rate                       %        78.06
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %       103.96
    Mem Pipes Busy                        %        94.73
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.49%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.04%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.01
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        47.99
    Active Warps Per Scheduler          warp        11.71
    Eligible Warps Per Scheduler        warp         2.65
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.265%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.71 active warps per scheduler, but only an average of 2.65 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.51
    Warp Cycles Per Executed Instruction           cycle        22.51
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.265%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 33.0% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270840.23
    Issued Instructions                             inst   2330000662
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.64
    Achieved Active Warps Per SM           warp        46.87
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1361912
    Total DRAM Elapsed Cycles        cycle    868276224
    Average L1 Active Cycles         cycle  46677088.62
    Total L1 Elapsed Cycles          cycle   1121006504
    Average L2 Active Cycles         cycle  46604638.31
    Total L2 Elapsed Cycles          cycle    738496016
    Average SM Active Cycles         cycle  46677088.62
    Total SM Elapsed Cycles          cycle   1121006504
    Average SMSP Active Cycles       cycle  46663375.03
    Total SMSP Elapsed Cycles        cycle   4484026016
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1867%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.43
    Elapsed Cycles                cycle     46723468
    Memory Throughput                 %        94.70
    DRAM Throughput                   %         0.70
    Duration                         ms        32.61
    L1/TEX Cache Throughput           %        94.79
    L2 Cache Throughput               %        23.08
    SM Active Cycles              cycle  46683152.38
    Compute (SM) Throughput           %        94.70
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        16.06
    Dropped Samples                sample            0
    Maximum Sampling Interval          ms         1.02
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.15
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.34
    Mem Busy                              %        69.20
    Max Bandwidth                         %        94.70
    L1/TEX Hit Rate                       %        77.69
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        98.77
    Mem Pipes Busy                        %        94.70
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.48%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.03%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.01
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        47.99
    Active Warps Per Scheduler          warp        11.72
    Eligible Warps Per Scheduler        warp         2.65
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.295%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.72 active warps per scheduler, but only an average of 2.65 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.53
    Warp Cycles Per Executed Instruction           cycle        22.53
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.295%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.8% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst     24270840
    Issued Instructions                             inst   2330000640
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.71
    Achieved Active Warps Per SM           warp        46.90
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1362288
    Total DRAM Elapsed Cycles        cycle    781930496
    Average L1 Active Cycles         cycle  46683152.38
    Total L1 Elapsed Cycles          cycle   1121361912
    Average L2 Active Cycles         cycle  46884328.12
    Total L2 Elapsed Cycles          cycle    758545488
    Average SM Active Cycles         cycle  46683152.38
    Total SM Elapsed Cycles          cycle   1121361912
    Average SMSP Active Cycles       cycle  46664307.35
    Total SMSP Elapsed Cycles        cycle   4485447648
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1829%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  matrixMulKernel(const float *, const float *, float *, int, int, int) (128, 128, 1)x(16, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         5.99
    SM Frequency                    Ghz         1.36
    Elapsed Cycles                cycle     46727445
    Memory Throughput                 %        94.70
    DRAM Throughput                   %         0.66
    Duration                         ms        34.23
    L1/TEX Cache Throughput           %        94.79
    L2 Cache Throughput               %        23.74
    SM Active Cycles              cycle  46681868.46
    Compute (SM) Throughput           %        94.70
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 6% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        13.57
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            8
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.08
    Executed Ipc Elapsed  inst/cycle         2.08
    Issue Slots Busy               %        51.99
    Issued Ipc Active     inst/cycle         2.08
    SM Busy                        %        60.14
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (35.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    --------------------------- ----------- ------------
    Metric Name                 Metric Unit Metric Value
    --------------------------- ----------- ------------
    Memory Throughput               Gbyte/s         1.27
    Mem Busy                              %        69.20
    Max Bandwidth                         %        94.70
    L1/TEX Hit Rate                       %        78.11
    L2 Compression Success Rate           %            0
    L2 Compression Ratio                               0
    L2 Hit Rate                           %        97.17
    Mem Pipes Busy                        %        94.70
    --------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 47.47%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.03%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 25.6 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        52.01
    Issued Warp Per Scheduler                        0.52
    No Eligible                            %        47.99
    Active Warps Per Scheduler          warp        11.71
    Eligible Warps Per Scheduler        warp         2.66
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 5.304%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of   
          11.71 active warps per scheduler, but only an average of 2.66 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        22.51
    Warp Cycles Per Executed Instruction           cycle        22.51
    Avg. Active Threads Per Warp                                31.81
    Avg. Not Predicated Off Threads Per Warp                    31.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.304%                                                                                          
          On average, each warp of this kernel spends 7.4 cycles being stalled waiting for the L1 instruction queue for 
          local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing      
          local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to      
          avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if  
          the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple      
          lower-width memory operations into fewer wider memory operations and try interleaving memory operations and   
          math instructions. This stall type represents about 32.9% of the total average of 22.5 cycles between         
          issuing two instructions.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst  24270749.33
    Executed Instructions                           inst   2329991936
    Avg. Issued Instructions Per Scheduler          inst  24270840.25
    Issued Instructions                             inst   2330000664
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  16384
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              24
    Threads                                   thread         4194304
    Uses Green Context                                             0
    Waves Per SM                                              113.78
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.70
    Achieved Active Warps Per SM           warp        46.90
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1353576
    Total DRAM Elapsed Cycles        cycle    820897792
    Average L1 Active Cycles         cycle  46681868.46
    Total L1 Elapsed Cycles          cycle   1121457392
    Average L2 Active Cycles         cycle  46799373.12
    Total L2 Elapsed Cycles          cycle    755818512
    Average SM Active Cycles         cycle  46681868.46
    Total SM Elapsed Cycles          cycle   1121457392
    Average SMSP Active Cycles       cycle  46667236.26
    Total SMSP Elapsed Cycles        cycle   4485829568
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.03
    Branch Instructions              inst     69071104
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1832%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2202952 excessive sectors (0% of the      
          total 1191384742 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source     
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

